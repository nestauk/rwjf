{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Innovation Analysis Using MeSH Terms as Document Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '86444f44-f1fc-4405-90ec-f21cdbeb8e4d' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '86444f44-f1fc-4405-90ec-f21cdbeb8e4d' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"86444f44-f1fc-4405-90ec-f21cdbeb8e4d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Using the NLM Batch MTI Tool\n",
    "\n",
    "As part of their Indexing Initiative, NIH and NLM have developed an algorithm that will suggest MeSh terms as labels for a document. A public tool that makes use of their technology is the [MeSH on Demand](https://meshb.nlm.nih.gov/MeSHonDemand), which can process a single text at a time. However, with an account, users can access a series of [batch processing tools](https://ii.nlm.nih.gov/Batch/index.shtml), which can perform the same kind of labelling on a corpus of texts, along with a greater degree of control over parameters such as term filtering and outputs.\n",
    "\n",
    "The tools require uploaded texts to be formated in one of a set of specific styles, which can be found [here](https://ii.nlm.nih.gov/Help/index.shtml). Here we import the descriptions that we want to be labelled from the _Grant Database_, and write them to a text file in the \"Single Line Delimited Input w/ ID\" format, with ASCII characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grichardson/miniconda3/envs/rwjf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,5,7,10,11,12,13,14,18,19,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gh_user_id</th>\n",
       "      <th>gdb_dataset_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>status</th>\n",
       "      <th>gh_user_creation_date</th>\n",
       "      <th>description</th>\n",
       "      <th>funding</th>\n",
       "      <th>row_id</th>\n",
       "      <th>currency</th>\n",
       "      <th>...</th>\n",
       "      <th>cb_category_list</th>\n",
       "      <th>cb_role</th>\n",
       "      <th>end_date</th>\n",
       "      <th>gh_valid_description</th>\n",
       "      <th>name</th>\n",
       "      <th>administrative_area_level_1</th>\n",
       "      <th>administrative_area_level_2</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>gtr</td>\n",
       "      <td>GDB</td>\n",
       "      <td>2009-04-01 00:00:00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The past decade has seen a renaissance in acce...</td>\n",
       "      <td>2313330.0</td>\n",
       "      <td>http://gtr.rcuk.ac.uk:80/gtr/api/projects/00C9...</td>\n",
       "      <td>GBP</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University College London</td>\n",
       "      <td>England</td>\n",
       "      <td>Greater London</td>\n",
       "      <td>-0.132718</td>\n",
       "      <td>51.524469</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>gtr</td>\n",
       "      <td>GDB</td>\n",
       "      <td>2009-10-01 00:00:00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Humans and animals are made up of millions of ...</td>\n",
       "      <td>1520860.0</td>\n",
       "      <td>http://gtr.rcuk.ac.uk:80/gtr/api/projects/00E4...</td>\n",
       "      <td>GBP</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University College London</td>\n",
       "      <td>England</td>\n",
       "      <td>Greater London</td>\n",
       "      <td>-0.132718</td>\n",
       "      <td>51.524469</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gh_user_id gdb_dataset_id source_id           start_date  status  \\\n",
       "0         NaN            gtr       GDB  2009-04-01 00:00:00  Closed   \n",
       "1         NaN            gtr       GDB  2009-10-01 00:00:00  Closed   \n",
       "\n",
       "  gh_user_creation_date                                        description  \\\n",
       "0                   NaN  The past decade has seen a renaissance in acce...   \n",
       "1                   NaN  Humans and animals are made up of millions of ...   \n",
       "\n",
       "     funding                                             row_id currency  \\\n",
       "0  2313330.0  http://gtr.rcuk.ac.uk:80/gtr/api/projects/00C9...      GBP   \n",
       "1  1520860.0  http://gtr.rcuk.ac.uk:80/gtr/api/projects/00E4...      GBP   \n",
       "\n",
       "        ...       cb_category_list cb_role end_date gh_valid_description  \\\n",
       "0       ...                    NaN     NaN      NaN                  NaN   \n",
       "1       ...                    NaN     NaN      NaN                  NaN   \n",
       "\n",
       "                        name administrative_area_level_1  \\\n",
       "0  University College London                     England   \n",
       "1  University College London                     England   \n",
       "\n",
       "   administrative_area_level_2       lng        lat         country  \n",
       "0               Greater London -0.132718  51.524469  United Kingdom  \n",
       "1               Greater London -0.132718  51.524469  United Kingdom  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdb_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = gdb_df['description']\n",
    "\n",
    "index_pad_length = len(str(max(descriptions.index)))\n",
    "\n",
    "with open(os.path.join(proc_data, 'mti', 'gdb_descriptions_for_mti.txt'), 'w') as f:\n",
    "    for i, t in zip(descriptions.index, descriptions.values):\n",
    "        if pd.isnull(t):\n",
    "            t = 'None'\n",
    "        f.write('{}|{}\\n'.format(str(i).rjust(index_pad_length, '0'),\n",
    "                                 t.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text file was uploaded to the [Generic Batch with Validation](https://ii.nlm.nih.gov/Batch/UTS_Required/genericWithValidation.shtml) tool using \"Single Line Delimited Input w/ ID\" as the only _Batch Specific_ option, and the execution arguments (recommended by an NLM employee):\n",
    "\n",
    "```\n",
    "MTI.Linux -MoD_PP -trackPositional -E\n",
    "```\n",
    "\n",
    "Upon completion of the job (which was run overnight), a link to the results are emailed to the user. From here we can download the file `text.out`, which contains the results for every document in the text uploaded. For each document, the results take the following form:\n",
    "\n",
    "```\n",
    "000000|PRC|17768168;17211657;24593272;25377753;22960242;17691196;8705186;20643683;25030295;9682210|\n",
    "000000|Protons|D011522|C0033727|52674|1195^7^0\n",
    "000000|Radioactive Waste|D011850|C0034552|46935|1413^17^0\n",
    "000000|Universities|D014495|C0041740|27039|1830^12^0\n",
    "000000|Diamond|D018130|C0057717|9522|1022^7^0\n",
    "000000|Nuclear Energy|D009678|C0028572|29955|1354^14^0\n",
    "000000|Neutrons|D009502|C0027946|22098|953^7^0\n",
    "000000|Ions|D007477|C0022023|20340|1213^4^0\n",
    "000000|Physics|D010825|C0031837|7722|531^7^0;251^7^0\n",
    "000000|Students|D013334|C0038492|2466|1675^17^0;1684^8^0\n",
    "000000|Neoplasms|D009369|C0027651|999|1227^6^0\n",
    "000000|United Kingdom|D006113|C0041700|1000|86^6^0\n",
    "000000|Cell Proliferation|D049109|C0596290|1000|1321^13^0\n",
    "```\n",
    "\n",
    "The number on the left is the ID we assigned to each document in the input file, and the first line of each document's results contains the PubMed unique IDs for the top 10 articles which are similar to the one uploaded. More information on interpreting outputs from the MTI tools can be found [here](https://ii.nlm.nih.gov/resource/MTI_output_help_info.html). The following rows contain the MeSH terms that were matched from the text. These are found using a combination of keyword matching, fuzzy matching, and similarity of the document to other documents in the PubMed archive. Each row contains the MeSH term, its CUI, a score, and the position of the word of phrase in the original document that gave rise to the label. This positional information consists of three components start position (zero offset), length, and a flag if the text is split (e.g. \"community care\" in \"community of care\"). Behind the scenes, the text input is reformatted, and as a result, the start and end indicies are off by 13 plus the length of your input index values.\n",
    "\n",
    "For reference, here is line from the original text input file that gave the results above:\n",
    "\n",
    "_000000|The past decade has seen a renaissance in accelerator R&amp;D in the UK, building upon the existing expertise at the Rutherford Appleton and Daresbury Laboratories and fuelled by the need to prepare for the generation of particle physics facilities after the LHC. Both the University of Oxford and Royal Holloway University of London have made very significant contributions to this renewed programme. In 2004, the John Adams Institute for Accelerator Science was created, jointly hosted by the Departments of Physics of the University of Oxford and Royal Holloway University of London, with support from PPARC and CCLRC, now merged into STFC. The initial programme was focussed on R&amp;D for the Linear Collider and the Neutrino Factory, but has broadened considerably since its inception to include developments of advanced and novel light sources, work on the upgrades for the Large Hadron Collider at CERN, the ISIS spallation neutron source at the Rutherford Appleton Laboratory, and of the new Diamond Light Source on the Harwell Science and Innovation Campus, and the development of novel accelerators for a variety of applications from medicine (for example, using protons and light ions to treat cancer therapy) to energy (where accelerator-driven sub-critical reactors could contribute to proliferation-safe generation of nuclear energy and help reduce the volume of highly active radioactive waste). A key part of the strategy is the training of a new generation of accelerator scientists able to design, build and operate the new facilities that would be required in the future and we have made excellent progress on this, with more than 20 graduate students and 15 PDRAs being trained by the JAI alone. We propose to continue this programme, as part of a broadly-based collaboration between the universities, the accelerator science institutes (John Adams Institute, the Cockcroft Institute and the STFC Accelerator Science and Technology Centre), and the national laboratories. A key objective is to encourage the development of a domestic industry able to support this work._\n",
    "\n",
    "## Importing Batch Processed MeSH Labels\n",
    "\n",
    "After downloading and renameing our `text.out` results file, we can import it and match it up to the original dataset. The following function is slightly lengthy, but it essentially parses the results and aggregates all of the labels associated with each document. It also sorts them in the original order that they were found in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(proc_data, 'mti', 'gdb_mesh_on_demand_labels.out'), 'r') as f:\n",
    "    mesh_on_demand_labels = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mti_batch_output(batch_output, descriptions, padding=21, log_every=10000):\n",
    "    \"\"\"parse_mti_batch_output\n",
    "    Parses the MTI Batch processor output text files to generate a structured output.\n",
    "    \n",
    "    Args:\n",
    "    batch_output (str): Raw text output from MTI Batch.\n",
    "    descriptions (list:str): Original texts.\n",
    "    padding (int): Accounts for variations in label indexing. 21 for gdb and 19 for \n",
    "        pioneers and globals. Defaults to 21.\n",
    "    log_every (int): Number of iterations to print progress.\n",
    "        \n",
    "    Returns:\n",
    "    mti_output_dict (dict):\n",
    "    doc_vocab_to_mesh_map (dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    mti_output_dict = defaultdict(dict)\n",
    "    mti_output_dict['pub_med_uids'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_duis'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_cuis'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_labels'] = defaultdict(list)\n",
    "    mti_output_dict['scores'] = defaultdict(list)\n",
    "    mti_output_dict['original_phrases'] = defaultdict(list)\n",
    "    mti_output_dict['term_counts'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_term_token_start_idx'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_term_token_end_idx'] = defaultdict(list)\n",
    "    \n",
    "    doc_vocab_to_mesh_map = defaultdict(list)\n",
    "    \n",
    "    batch_output_list = batch_output.copy()\n",
    "    \n",
    "    # loop through documents and get only the mesh label outputs for that doc\n",
    "    for doc_id in range(len(descriptions)):\n",
    "        \n",
    "        doc_results = []\n",
    "        \n",
    "        original_phrases = []\n",
    "        mesh_labels = []\n",
    "        cui_codes = []\n",
    "        starts = []\n",
    "        ends = []\n",
    "        term_counts = []\n",
    "        \n",
    "        if not doc_id % log_every:\n",
    "            print(\"Processed:\", doc_id)\n",
    "\n",
    "        for row_id, result_row in enumerate(batch_output_list):\n",
    "            row_doc_id = int(result_row.split('|')[0]) \n",
    "            if row_doc_id == doc_id:\n",
    "                doc_results.append(result_row)\n",
    "            else:\n",
    "                break\n",
    "            cutoff = row_id + 1\n",
    "        # drop the labels for the current document from the original labels list\n",
    "        # about 60x faster than re-indexing\n",
    "        del batch_output_list[:cutoff]\n",
    "        \n",
    "        # cycle through labels in doc and add them to dicts\n",
    "        for doc_row in doc_results:\n",
    "            elements = doc_row.split('|')\n",
    "            \n",
    "            index_padding = len(elements[0])\n",
    "            i = int(elements[0])\n",
    "            \n",
    "            # deal with the first line that only contains PubMed IDs for the doc\n",
    "            if elements[1] == 'PRC':\n",
    "                pub_med_uids = elements[-2].split(';')\n",
    "                mti_output_dict['pub_med_uids'][i] = pub_med_uids\n",
    "            # deal with the actual MeSH labels\n",
    "            else:\n",
    "                ml = elements[1]\n",
    "                mti_output_dict['mesh_labels'][i].append(ml)\n",
    "                mti_output_dict['mesh_duis'][i].append(elements[2])\n",
    "                mti_output_dict['mesh_cuis'][i].append(elements[3])\n",
    "                mti_output_dict['scores'][i].append(elements[4])\n",
    "            \n",
    "                positions = elements[-1].split('^')\n",
    "\n",
    "                if len(positions) > 3:\n",
    "                    positions = flatten([p.split(';') for p in positions])\n",
    "                positions = list(grouper(3, positions))\n",
    "\n",
    "#                 starts.append([int(p[0]) for p in positions])\n",
    "                term_counts.append(len(positions))\n",
    "                mesh_labels.append([ml])\n",
    "                \n",
    "                original_phrases_row = []\n",
    "                for position in positions:\n",
    "                    if isinstance(position, str):\n",
    "                        continue\n",
    "                    elif None in position:\n",
    "                        continue\n",
    "                    else:\n",
    "                        start = int(position[0]) - (padding - index_padding) - len(elements[0])\n",
    "                        starts.append(start)\n",
    "                        end = start + int(position[1])\n",
    "                        ends.append(end)\n",
    "                        original_phrases_row.append(descriptions[i][start:end])\n",
    "                \n",
    "                # map phrases from text to their MeSH labels in separate dict\n",
    "                for opr in original_phrases_row:\n",
    "                    original_phrases.append(opr)\n",
    "                    if ml not in doc_vocab_to_mesh_map[opr]:\n",
    "                        doc_vocab_to_mesh_map[opr].append(ml)\n",
    "\n",
    "        mti_output_dict['original_phrases'][i] = original_phrases\n",
    "        \n",
    "        # get the phrases from the texts in their original order\n",
    "#         starts = flatten(starts)\n",
    "        original_phrases_ordered = [op for (s, op) in sorted(zip(starts, original_phrases), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['original_phrases'][i] = original_phrases_ordered\n",
    "        # and the mesh labels\n",
    "        mesh_labels_ordered = flatten([[ml] * tc for ml, tc in zip(mesh_labels, term_counts)])\n",
    "        mesh_labels_ordered = flatten([ml for (s, ml) in sorted(zip(starts, mesh_labels_ordered), key=lambda pair: pair[0])])\n",
    "        mti_output_dict['mesh_labels'][i] = mesh_labels_ordered\n",
    "        \n",
    "        cui_ordered = flatten([[cui] * tc for cui, tc in zip(mti_output_dict['mesh_cuis'][i], term_counts)])\n",
    "        cui_ordered = [cui for (s, cui) in sorted(zip(starts, cui_ordered), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['mesh_cuis'][i] = cui_ordered\n",
    "        \n",
    "        dui_ordered = flatten([[dui] * tc for dui, tc in zip(mti_output_dict['mesh_duis'][i], term_counts)])\n",
    "        dui_ordered = [dui for (s, dui) in sorted(zip(starts, dui_ordered), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['mesh_duis'][i] = dui_ordered\n",
    "        \n",
    "        mti_output_dict['mesh_term_token_start_idx'][i] = sorted(starts)\n",
    "        mti_output_dict['mesh_term_token_end_idx'][i] = sorted(ends)\n",
    "        mti_output_dict['term_counts'][i] = term_counts\n",
    "\n",
    "    return mti_output_dict, doc_vocab_to_mesh_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n",
      "Processed: 10000\n",
      "Processed: 20000\n",
      "Processed: 30000\n",
      "Processed: 40000\n",
      "Processed: 50000\n",
      "Processed: 60000\n",
      "Processed: 70000\n",
      "Processed: 80000\n",
      "Processed: 90000\n",
      "Processed: 100000\n",
      "Processed: 110000\n",
      "Processed: 120000\n",
      "Processed: 130000\n",
      "Processed: 140000\n"
     ]
    }
   ],
   "source": [
    "# takes a little while to run\n",
    "mti_output_dict, vocab_mesh_mapping = parse_mti_batch_output(mesh_on_demand_labels, descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step here is to merge the data with the original GDB doc ids and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_doc_id_df = pd.read_csv(os.path.join(inter_data, 'gdb_doc_ids.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['original_phrases', 'mesh_labels', 'pub_med_uids', 'mesh_labels', 'mesh_cuis',\n",
    "          'mesh_duis', 'mesh_term_token_start_idx', 'mesh_term_token_end_idx']:\n",
    "    gdb_doc_id_df[c] = gdb_doc_id_df['doc_id'].map(mti_output_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_doc_id_df.to_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pioneer and Global Projects\n",
    "\n",
    "We also need to repeat this process for the RWJF Pioneer and Global projects.\n",
    "\n",
    "Prepare for the MTI batch processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))\n",
    "rwjf_descriptions = rwjf_df['description']\n",
    "source = rwjf_df['source_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pad_length = len(str(max(descriptions.index)))\n",
    "\n",
    "with open(os.path.join(proc_data, 'mti', 'rwjf_pioneers_and_globals_for_mti.txt'), 'w') as f:\n",
    "    for i, t, s in zip(descriptions.index, descriptions.values, source.values):\n",
    "        s = s[0]\n",
    "        if pd.isnull(t):\n",
    "            t = 'None'\n",
    "        f.write('{}{}|{}\\n'.format(s, str(i).rjust(index_pad_length, '0'),\n",
    "                                 t.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, open and parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(proc_data, 'mti', 'mti_labelled_rwfj_pioneers_and_globals.out')) as f:\n",
    "    mesh_od_pio_global_labels = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_od_pio_global_labels = [m[1:] for m in mesh_od_pio_global_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n"
     ]
    }
   ],
   "source": [
    "mti_pio_global_output_dict, vocab_pio_global_mesh_mapping = parse_mti_batch_output(mesh_od_pio_global_labels,\n",
    "                                                                                   rwjf_descriptions,\n",
    "                                                                                   padding=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have doc ids for the RWJF projects yet. Let's add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_doc_ids = rwjf_df.index.values + len(gdb_df)\n",
    "rwjf_doc_id_df = pd.DataFrame({'doc_id': rwjf_doc_ids})\n",
    "rwjf_doc_id_df.to_csv(os.path.join(inter_data, 'rwjf_doc_ids.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['original_phrases', 'mesh_labels', 'pub_med_uids', 'mesh_labels', 'mesh_cuis',\n",
    "          'mesh_duis', 'mesh_term_token_start_idx', 'mesh_term_token_end_idx']:\n",
    "    rwjf_doc_id_df[c] = pd.Series(rwjf_doc_id_df.index.values).map(mti_pio_global_output_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_doc_id_df.to_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some features of the labels look like for the processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(proc_data + 'gdb_5_25_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_labels = column_to_list(gdb_df, 'mesh_labels')\n",
    "mesh_label_counts = Counter(flatten(mesh_labels))\n",
    "\n",
    "original_phrases = column_to_list(gdb_df, 'original_phrases')\n",
    "original_phrases_counts = Counter(flatten(original_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label and Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common MeSH Labels and original terms:\\n')\n",
    "print('{}\\t{:30}\\t{}\\t{:20}\\t{}\\n'.format('Rank', 'MeSH Label', 'Count', 'Original Term', 'Count'))\n",
    "\n",
    "for i, (ml, op) in enumerate(zip(mesh_label_counts.most_common(20), original_phrases_counts.most_common(20))):\n",
    "    print('{}\\t{:30}\\t{}\\t{:20}\\t{}'.format(i, ml[0], ml[1], op[0], op[1]))\n",
    "\n",
    "print('\\nNumber of unique MeSH Labels:', len(set(flatten(mesh_labels))))\n",
    "print('Number of unique original terms:', len(set(flatten(original_phrases))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common MeSH label is _Humans_, however this is not the case for the original document terms. This is because the MeSH on Demand labelling parameters works on inference as well as exact matching. There can also be multiple MeSH labels for each term identified in the document. Overall however, there are a smaller set of MeSH labels compared to the original terms that generated them. There are a lot of shared terms, revealing some of the largest topics and actors to be covered in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "mesh_label_counts_low = Counter({k: v for k, v in mesh_label_counts.items() if v < n})\n",
    "original_phrases_counts_low = Counter({k: v for k, v in original_phrases_counts.items() if v < n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common MeSH Labels and original terms:\\n')\n",
    "print('{}\\t{:40}\\t{}\\t{:40}\\t{}\\n'.format('Rank', 'MeSH Label', 'Count', 'Original Term', 'Count'))\n",
    "\n",
    "for i, (ml, op) in enumerate(zip(mesh_label_counts_low.most_common(30), original_phrases_counts_low.most_common(30))):\n",
    "    print('{}\\t{:40}\\t{}\\t{:40}\\t{}'.format(i, ml[0], ml[1], op[0], op[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some of the labels and terms with low counts, we can see that there is much more variety, and some very specific concepts have been captured, from chemicals, to ailments, to body parts and some treatments.\n",
    "\n",
    "### Matching MeSH Labels to Groups\n",
    "\n",
    "A feature of the MeSH terms is that they are organised in a tree structure, with each term being nested underneath a larger grouping. Some of the tree branches are up to 13 nodes deep. We can take the MeSH ontology (pre-processed from the available XML file), and check how this matches up to the labelled terms in the documents. We do this using the Descriptor Unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mesh_codes_processed_DUI_5_21_2018.json', 'r') as f:\n",
    "    mesh_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:20}\\t{:20}\\t{}\\t{}\\t{}\\t{}\\n'.format('MeSH Label', 'Original', 'DUI', 'Start', 'End', 'MeSH Group'))\n",
    "for label, code, op, start, end in zip(mti_output_dict['mesh_labels'][0],\n",
    "                                       mti_output_dict['mesh_duis'][0],\n",
    "                                       mti_output_dict['original_phrases'][0],\n",
    "                                       mti_output_dict['mesh_term_token_start_idx'][0],\n",
    "                                       mti_output_dict['mesh_term_token_end_idx'][0]):\n",
    "    mesh_group = mesh_codes[code]['tree_DescriptorStringProcessed_1']\n",
    "    print('{:20}\\t{:20}\\t{}\\t{}\\t{}\\t{}'.format(label, op, code, start, end, mesh_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms above are printed from those labelled in the very first document in the database (shown in full above). The groups chosen are the 1st level, so not the most high level, and they seem to separate the terms into mostly sensible categories. There are mistakes due to the parsing method we have used for the MeSH XML file, however conersation is ongoing with NLM about how best to fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Topic Modelling\n",
    "\n",
    "To demonstrate a potential use of the MeSH labels, we will have a quick look at topic modelling.\n",
    "\n",
    "One issue with topic modelling on the entire dataset, is that only very coarse themes that are common across many documents are picked up. These often include very common health issues, or administrative jargon. Using the MeSH labels, we can create a subset of documents, and perform topic modelling on them instead, in a bid to find the topics that exist within any particular concept. We can imagine this as potentially useful as an innovation discovery tool for researchers wanting to find out more about a particular health domain.\n",
    "\n",
    "As an example, we can use the MeSH label \"Climate\" to generate our data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term = \"'Climate'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_key_term_df = gdb_df[gdb_df['mesh_labels'].str.contains(key_term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdb_key_term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data subset consists of about 1700 documents.\n",
    "\n",
    "First we parse the text, remove stop words and other non-useful tokens, and create ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/external/en_ranknl_long.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "stop_words = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop in stop_words:\n",
    "    nlp_sm.vocab[stop].is_stop = True\n",
    "    nlp_sm.vocab[stop.title()].is_stop = True\n",
    "    nlp_sm.vocab[stop.upper()].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_descriptions = [nlp_sm(d) for d in gdb_key_term_df['Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_processed_lemmas = []\n",
    "for ktd in key_term_descriptions:\n",
    "    lemmas = []\n",
    "    for t in ktd:\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        if t.is_bracket:\n",
    "            continue\n",
    "        if t.is_quote:\n",
    "            continue\n",
    "        if t.like_num:\n",
    "            continue\n",
    "        if t.is_digit:\n",
    "            continue\n",
    "        if t.like_url:\n",
    "            continue\n",
    "        if t.like_email:\n",
    "            continue\n",
    "        if t.is_space:\n",
    "            continue\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.text.title() == key_term:\n",
    "            continue\n",
    "            #         if t.lower_ in stop_words:\n",
    "#             continue\n",
    "        lemmas.append(t.lower_ + '/' + t.pos_)\n",
    "    key_term_description_processed_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_tools import GensimNGrammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer = GensimNGrammer(n=2, **{'min_count': 30, 'threshold': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_processed_lemma_trigrams = ngrammer.fit_transform(key_term_description_processed_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common term frequencies, we can see that even these contain fairly contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flatten(key_term_description_processed_lemma_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the topic model. As we don't have a large number of documents, the number of topics is kept fairly low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(key_term_description_processed_lemma_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow = [dictionary.doc2bow(kt) for kt in key_term_description_processed_lemma_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=doc_bow, id2word=dictionary, num_topics=100, chunksize=200, passes=10, minimum_probability=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda, doc_bow, dictionary)\n",
    "pyLDAvis.save_html(lda_vis, fig_path + '/gdb_climate_{date}_lda_viz.html'.format(date=today_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem to be fairly representative of groups of intuitively connected concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs = []\n",
    "for db in doc_bow:\n",
    "    lda_vecs.append(gensim.matutils.sparse2full(lda[db], length=100))\n",
    "\n",
    "lda_vecs = np.array(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne = tsne.fit_transform(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne_df = pd.DataFrame(lda_tsne)\n",
    "lda_tsne_df.rename(columns={0: 'tsne_0', 1: 'tsne_1'}, inplace=True)\n",
    "labels = [', '.join(list(set(ast.literal_eval(f)))) for f in gdb_key_term_df['mesh_labels'].values]\n",
    "lda_tsne_df['labels'] = labels\n",
    "lda_tsne_df['doc_id'] = gdb_key_term_df.index.values\n",
    "lda_tsne_cds = ColumnDataSource(lda_tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(p):\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # turn off y-axis tick labels\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool(tooltips=[\n",
    "    (\"ID\", \"@doc_id\"),\n",
    "    (\"Labels\", \"@labels\"),\n",
    "])\n",
    "\n",
    "p = figure(width=800, height=600,\n",
    "          x_axis_label='tsne_0', y_axis_label='tsne_1',\n",
    "           title='Documents containing the MeSH label \"Climate\" transformed by LDA and t-SNE')\n",
    "p.circle(x='tsne_0', y='tsne_1', source= lda_tsne_cds, size=7, alpha=0.5)\n",
    "p.add_tools(hover)\n",
    "p = clean_plot(p)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing one of the tight clusters at random, we can get a handful of document IDs, and fetch their details from the original GDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df.iloc[[9035, 13419, 21148, 11818, 15795]][['Source ID', 'Description', 'country', 'Start Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the topics and the clusterings that we have may be due to duplicate entries in the dataset. This is possibly dominating the topic models, and also the TSNE model. \n",
    "\n",
    "Let's try to do this again on a de-duplicated \"Climate\" data subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_key_term_dedupe_df = gdb_key_term_df.drop_duplicates(subset='Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdb_key_term_dedupe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_dedupe_descriptions = [nlp_sm(d) for d in gdb_key_term_dedupe_df['Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_dedupe_description_processed_lemmas = []\n",
    "for ktd in key_term_dedupe_descriptions:\n",
    "    lemmas = []\n",
    "    for t in ktd:\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        if t.is_bracket:\n",
    "            continue\n",
    "        if t.is_quote:\n",
    "            continue\n",
    "        if t.like_num:\n",
    "            continue\n",
    "        if t.is_digit:\n",
    "            continue\n",
    "        if t.like_url:\n",
    "            continue\n",
    "        if t.like_email:\n",
    "            continue\n",
    "        if t.is_space:\n",
    "            continue\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.text.title() == key_term:\n",
    "            continue\n",
    "            #         if t.lower_ in stop_words:\n",
    "#             continue\n",
    "        lemmas.append(t.lower_ + '/' + t.pos_)\n",
    "    key_term_dedupe_description_processed_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer = GensimNGrammer(n=2, **{'min_count': 15, 'threshold': 5})\n",
    "key_term_description_dedupe_processed_lemma_trigrams = ngrammer.fit_transform(key_term_dedupe_description_processed_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common term frequencies again, we can see a similar distribution, which is reassuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flatten(key_term_description_dedupe_processed_lemma_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_term_counter(counter, n, low_pass=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the topic model. As we don't have a large number of documents, the number of topics is kept fairly low. However, we should first remove some of the most commonly occuring words, as they will skew the topics significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_dedupe_processed_lemma_trigrams_low = [[t for t in terms if counts[t] < 1000] for terms in key_term_description_dedupe_processed_lemma_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dedupe = Dictionary(key_term_description_dedupe_processed_lemma_trigrams_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow_dedupe = [dictionary_dedupe.doc2bow(kt) for kt in key_term_description_dedupe_processed_lemma_trigrams_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dedupe = LdaModel(corpus=doc_bow_dedupe, id2word=dictionary_dedupe, num_topics=50, chunksize=1000, passes=10, minimum_probability=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem to be fairly representative of groups of intuitively connected concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dedupe.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs_dedupe = []\n",
    "for db in doc_bow_dedupe:\n",
    "    lda_vecs_dedupe.append(gensim.matutils.sparse2full(lda[db], length=100))\n",
    "\n",
    "lda_vecs_dedupe = np.array(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dedupe = TSNE(\n",
    "            random_state=42,\n",
    "            perplexity=30,\n",
    ")\n",
    "lda_tsne_dedupe = tsne.fit_transform(lda_vecs_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne_dedupe_df = pd.DataFrame(lda_tsne_dedupe)\n",
    "lda_tsne_dedupe_df.rename(columns={0: 'tsne_0', 1: 'tsne_1'}, inplace=True)\n",
    "labels_dedupe = [', '.join(list(set(ast.literal_eval(f)))) for f in gdb_key_term_dedupe_df['mesh_labels'].values]\n",
    "lda_tsne_dedupe_df['labels'] = labels\n",
    "lda_tsne_dedupe_df['doc_id'] = gdb_key_term_dedupe_df.index.values\n",
    "lda_tsne_dedupe_cds = ColumnDataSource(lda_tsne_dedupe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool(tooltips=[\n",
    "    (\"ID\", \"@doc_id\"),\n",
    "    (\"Labels\", \"@labels\"),\n",
    "])\n",
    "\n",
    "p = figure(width=800, height=600,\n",
    "           x_axis_label='tsne_0', y_axis_label='tsne_1',\n",
    "           title='De-duplicated documents containing the MeSH label \"Climate\" transformed by LDA and t-SNE')\n",
    "p.circle(x='tsne_0', y='tsne_1', source=lda_tsne_dedupe_cds, size=7, alpha=0.5)\n",
    "p.add_tools(hover)\n",
    "p = clean_plot(p)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda_dedupe, doc_bow_dedupe, dictionary_dedupe)\n",
    "pyLDAvis.save_html(lda_vis, fig_path + '/gdb_climate_dedupe_{date}_lda_viz.html'.format(date=today_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the very dense clusters have disappeared.\n",
    "\n",
    "From topic modelling within an individual topic, we can see that it is possible to uncover the more nuanced topics within the documents covered by a broad term such as \"Climate\". This could be useful for helping users to discover new areas to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
