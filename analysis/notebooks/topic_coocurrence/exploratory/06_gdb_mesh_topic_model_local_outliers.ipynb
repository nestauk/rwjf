{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Outlier Projects\n",
    "======================\n",
    "\n",
    "This notebook explores the possibility of using local outlier detection on project descriptions to identify those that are outliers within their own topic group. The descriptions are vectorised using various methods, before local outlier detection is run. A quantitative assessment is carried out alongside a qualitative comparison of the outlying projects against their nearest neighbours.\n",
    "\n",
    "Authors: George Richardson (george.richardson@nesta.org.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are going to load both the GDB and the RWJF Pioneer and Global projects, and join them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove projects from GitHub as they don't play nicely with MeSH terms, and Crunchbase as they're very short. There are also some projects with null descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the two sets of projects and extract their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)\n",
    "gdb_df.set_index('doc_id', inplace=True)\n",
    "gdb_df = gdb_df.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MeSH Label Exploration\n",
    "\n",
    "Before carrying out the analysis, we should have a brief look at some properties of the corpus as expressed in terms of the MeSH labels that have been assigned to the projects.\n",
    "\n",
    "To begin with, we will just have a look at the frequencies of the MeSH labels in the corpus. We will do this for the corpus containting all of the labels and then also the _deduplicated_ corpus - one where each project description is reduced to its deduplicated set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')\n",
    "description_mesh_labels_deduped = [list(set(ml)) for ml in description_mesh_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how long our corpus of MeSH labels is\n",
    "n_mesh_labels = len(flatten(description_mesh_labels))\n",
    "_description_mesh_labels_deduped_flat = flatten(description_mesh_labels_deduped)\n",
    "n_mesh_labels_deduped = len(_description_mesh_labels_deduped_flat)\n",
    "n_mesh_labels_unique = len(list(set(_description_mesh_labels_deduped_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of labels: {}'.format(n_mesh_labels))\n",
    "print('Number of labels in deduplicated descriptions: {}'.format(n_mesh_labels_deduped))\n",
    "print('Number of unique labels: {}'.format(n_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have just over 21,000 labels used to describe the projects. These labels are used over 2.4 million times in the original label representations, but only 1.3 million times in the deduplicated sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))\n",
    "mesh_label_deduped_counts = Counter(flatten(description_mesh_labels_deduped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.src.data.data_utilities import print_counter_extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MeSH label counts with duplicates:\\n')\n",
    "print_counter_extremes(mesh_label_counts, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MeSH label counts without duplicates:\\n')\n",
    "print_counter_extremes(mesh_label_deduped_counts, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top labels in our corpus include named groups, research terms, and a few health realated terms. They are all words that could concievably be used in a vast array of contexts. On the other hand, a random display of words with low counts shows many highly specialised labels, including chemical names and particular organisms or anatomical parts.\n",
    "\n",
    "Next, we'll look at the distribution of the frequencies themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_deduped_frequency_counts = Counter(mesh_label_deduped_counts.values())\n",
    "x_mesh_label_deduped_frequencies = []\n",
    "y_mesh_label_deduped_frequency_counts = []\n",
    "\n",
    "for k, v in mesh_label_deduped_frequency_counts.items():\n",
    "    x_mesh_label_deduped_frequencies.append(k)\n",
    "    y_mesh_label_deduped_frequency_counts.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_0 = plt.subplots(1, figsize=(6, 4))\n",
    "ax_0.scatter(x_mesh_label_deduped_frequencies, y_mesh_label_deduped_frequency_counts,\n",
    "           alpha=0.2, edgecolors='none', s=50)\n",
    "ax_0.set_xscale('log')\n",
    "ax_0.set_xlabel('Term Frequency')\n",
    "ax_0.set_ylabel('N Terms with Frequency')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of term frequencies follows a power law. There are almost 5000 terms which occur only once, which accounts for over 25% of the labels used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Outlier Detection\n",
    "\n",
    "Local outlier detection measures the isolation of a sample with respect to the cluster of its nearest k neighbours. From the [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html) website:\n",
    "\n",
    "> The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.\n",
    "\n",
    "Here we will vectorise project descriptions into multi-dimensional space so that we can perform this algorithm upon them. This is essentially topic modelling followed by the outlier detection. Our hypothesis is that projects that fall within a topic, but mention terms that are not usually associated with other projects clustered in that topic area, will be picked up as local outliers. The result should be that innovations in which ideas are newly brought to an established context will be identified.\n",
    "\n",
    "To assess how well this has worked, we will take three approaches:\n",
    "\n",
    "- Compare the results from different vectorisation methods\n",
    "- Analyse the outlier factors of the RWJF Pioneer projects in comparison to the other projects\n",
    "- Manually compare projects with high local outlier factors to other projects in their nearest neighbour cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Document Preparation\n",
    "\n",
    "For this analysis, we will use the deduplicated label representations of the projects. This is because we care whether a document mentions a concept at all, as opposed to \"how much\" a document is about a given subject.\n",
    "\n",
    "To prepare the documents, we will\n",
    "\n",
    "- eliminate labels that only appear once in the corpus\n",
    "- manually eliminate some of the most frequently occuring labels\n",
    "- convert to a format suitable for topic modelling\n",
    "- (normalise the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most infrequent\n",
    "description_mesh_labels_deduped_lof = [[l for l in dmld if mesh_label_deduped_counts[l] > 1]\n",
    "                                       for dmld in description_mesh_labels_deduped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing a the top 3 words chosen manually\n",
    "# 'Humans' is disproportionately represented\n",
    "# 'Goals' and 'Animals' offer little semantic information in our context\n",
    "\n",
    "removes = ['Humans', 'Goals', 'Students', 'Animals']\n",
    "\n",
    "description_mesh_labels_deduped_lof = [[l for l in dmldl if l not in removes]\n",
    "                                       for dmldl in description_mesh_labels_deduped_lof]\n",
    "\n",
    "# removing docs that contain less than 5 labels\n",
    "# get document lengths\n",
    "description_mesh_labels_deduped_lengths = [len(l) for l in description_mesh_labels_deduped_lof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['description_mesh_labels_deduped_lengths'] = description_mesh_labels_deduped_lengths\n",
    "gdb_df['description_mesh_labels_deduped_lof'] = description_mesh_labels_deduped_lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df = gdb_df[gdb_df['description_mesh_labels_deduped_lengths'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_deduped_dictionary = Dictionary(lof_df['description_mesh_labels_deduped_lof'].values)\n",
    "description_mesh_label_deduped_corpus = [description_mesh_label_deduped_dictionary.doc2bow(d)\n",
    "                                         for d in lof_df['description_mesh_labels_deduped_lof'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LSI\n",
    "\n",
    "The first method we are going to try for vectorisation in LSI due to its interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try again:**\n",
    "\n",
    "Try with docs with min 5 terms.\n",
    "\n",
    "Else try with the full documents (pre-processed with n-grams and/or noun chunks).\n",
    "Remove any documents that are too short?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus=description_mesh_label_deduped_corpus,\n",
    "                   id2word=description_mesh_label_deduped_dictionary)\n",
    "corpus_tfidf = tfidf[description_mesh_label_deduped_corpus]\n",
    "\n",
    "lsi_model = LsiModel(corpus_tfidf,\n",
    "                     id2word=description_mesh_label_deduped_dictionary,\n",
    "                     num_topics=300)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lsi = corpus2dense(corpus_lsi, num_terms=300)\n",
    "matrix_lsi = matrix_lsi.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_labels = lof.fit_predict(matrix_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_values = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lof, '../../../models/lof_lsi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_lsi = pca.fit_transform(matrix_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_lsi = tsne.fit_transform(pca_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df['tsne_0'] = tsne_lsi[:, 0]\n",
    "lof_df['tsne_1'] = tsne_lsi[:, 1]\n",
    "lof_df['outlier_colour'] = ['red'  if l == -1 else 'blue' for l in lof_labels]\n",
    "lof_df['outlier_values'] = lof_values\n",
    "lof_df['outlier_values_log'] = np.log10(np.abs(lof_values))\n",
    "lof_df['outlier_label'] = lof_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_df = lof_df[lof_df['outlier_values'] > -10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = ColumnDataSource(lof_df[['tsne_0', 'tsne_1', 'outlier_colour', 'outlier_values', 'outlier_values_log']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BoxZoomTool, ResetTool, WheelZoomTool, LinearColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(palette='Viridis256', low=tsne_df['outlier_values_log'].min(), high=tsne_df['outlier_values_log'].max())\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Description\", \"@descriptions\"),\n",
    "])\n",
    "                                \n",
    "box = BoxZoomTool()\n",
    "reset = ResetTool()\n",
    "zoom = WheelZoomTool()\n",
    "# color_mapper = CategoricalColorMapper(factors=list(w2v_df['label'].values.unique()))\n",
    "\n",
    "terms_tsne_scatter = figure(width=700, height=600, tools=[hover, box, reset, zoom],\n",
    "                     title='TSNE Plot of Projects')\n",
    "terms_tsne_scatter.circle(x='tsne_0', y='tsne_1', source=cds, alpha=0.2,\n",
    "                          color='outlier_colour',\n",
    "#                           color={'field': 'outlier_values_log', 'transform': color_mapper},\n",
    "                          radius=0.2\n",
    "                         )\n",
    "terms_tsne_scatter.xgrid.visible = False\n",
    "terms_tsne_scatter.ygrid.visible = False\n",
    "\n",
    "\n",
    "show(terms_tsne_scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df['outlier_label'][lof_df['source_id'] == 'pioneers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_kdtree = scipy.spatial.KDTree(matrix_lsi, leafsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a random outlier and it's nearest neighbours in the topic model space, we can see that they often share one single topic. This highlights the very high term dispersity in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random outlier\n",
    "outlier = lof_df[lof_df['outlier_label'] == -1].sample(1)\n",
    "# get it's topic model vector\n",
    "outlier_lsi_vector = matrix_lsi[outlier.index.values[0], :]\n",
    "# find nearest neighbours\n",
    "neighbour_closeness, neighbour_indices = lof_kdtree.query(outlier_lsi_vector, 20)\n",
    "\n",
    "neighbours = lof_df.iloc[neighbour_indices[1:]]\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4))\n",
    "ax.scatter(outlier['tsne_0'], outlier['tsne_1'], c=outlier['outlier_colour'], s=300)\n",
    "ax.scatter(neighbours['tsne_0'], neighbours['tsne_1'], c=neighbours['outlier_colour'])\n",
    "plt.show()\n",
    "\n",
    "print(sorted(outlier['description_mesh_labels_deduped_lof'].values[0]), '\\n')\n",
    "print(outlier['description'].values[0], '\\n')\n",
    "\n",
    "for c, d, l in zip(neighbour_closeness, neighbours['description'].values, neighbours['description_mesh_labels_deduped_lof'].values):\n",
    "    print('===============')\n",
    "    print(c, '\\n')\n",
    "    print(sorted(l), '\\n')\n",
    "    print(d[:1000])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
