{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks and Word Vectors with MeSH Labels\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))\n",
    "rwjf_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Doc IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdb_df['doc_id'] = pd.read_csv(os.path.join(inter_data, 'gdb_doc_ids.csv'))['doc_id']\n",
    "# rwjf_df['doc_id'] = pd.read_csv(os.path.join(inter_data, 'rwjf_doc_ids.csv'))['doc_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeSH Label Exploration\n",
    "\n",
    "In this section, we explore some properties of the corpus in terms of the MeSH labels that have been assigned to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')\n",
    "description_mesh_labels_unique = [list(set(ml)) for ml in description_mesh_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))\n",
    "mesh_label_unique_counts = Counter(flatten(description_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_counter_extremes(counter, n=20, low_pass=5, order_low_counts=False):\n",
    "    \"\"\"print_counter_extremes\n",
    "    Prints the top most common elements and a the least common elements\n",
    "    from a Counter object.\n",
    "    \n",
    "    Args:\n",
    "    counter (collections.Counter):\n",
    "    n (int): The number of elements to print. Will print n most common and\n",
    "        n least common. Defaults to 20.\n",
    "    low_pass (int): Threshold value that determines the highest possible count\n",
    "        for elements that fall into the \"least common\" group. Defaults to 5.\n",
    "    order_low_counts (bool): If True, will sort the elements with low counts\n",
    "        and print the 20 most common of those. Defaults to False.\n",
    "    \"\"\"\n",
    "    most_common = counter.most_common(n)\n",
    "    low_counts = {k: v for k, v in counter.items() if v <= low_pass}\n",
    "    low_count_keys_n = list(itertools.islice(low_counts, n))\n",
    "    low_counts_n = [(k, low_counts[k]) for k in low_count_keys_n]\n",
    "        \n",
    "    max_len_high = 0\n",
    "    max_len_low = 0\n",
    "    for high, low in zip(most_common, low_counts_n):\n",
    "        if len(high[0]) > max_len_high:\n",
    "            max_len_high = len(high[0])\n",
    "        if len(low[0]) > max_len_low:\n",
    "            max_len_low = len(low[0])\n",
    "        \n",
    "    print('{:<{max_len_high}}\\t{}\\t{:{max_len_low}}\\t{}\\n'.format('Label (Common)', 'Count', 'Label (Uncommon)', 'Count',\n",
    "                                                                  max_len_high=max_len_high + 2,\n",
    "                                                                  max_len_low=max_len_low + 2))\n",
    "    for high, low in zip(most_common, low_counts_n):\n",
    "        print('{:<{max_len_high}}\\t{}\\t{:{max_len_low}}\\t{}'.format(high[0], high[1], low[0], low[1],\n",
    "                                                         max_len_high=max_len_high + 2,\n",
    "                                                         max_len_low=max_len_low + 2))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MeSH label counts with duplicates:\\n')\n",
    "print_counter_extremes(mesh_label_counts, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MeSH label counts without duplicates:\\n')\n",
    "print_counter_extremes(mesh_label_unique_counts, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top labels in our corpus include named groups, research terms, and a few health realated terms. They are all words that could concievably be used in a vast array of contexts. On the other hand, a random display of words with low counts shows many highly specialised labels, including chemical names and particular organisms or anatomical parts.\n",
    "\n",
    "The next part of this notebook labels each term with its corresponding higher 0th and 1st level group from the MeSH term structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_duis = eval_column(gdb_df, 'mesh_duis')\n",
    "description_mesh_duis_unique = [list(set(ml)) for ml in description_mesh_duis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from the MeSH labels the their unique IDs\n",
    "mesh_label_dui_map = {}\n",
    "\n",
    "for labels, duis in zip(description_mesh_labels_unique, description_mesh_duis_unique):\n",
    "    for label, dui in zip(labels, duis):\n",
    "        if label not in mesh_label_dui_map:\n",
    "            mesh_label_dui_map[label] = dui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(inter_data, 'mesh_ontology', 'mesh_descriptions.json'), 'r') as f:\n",
    "    mesh_label_ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from the MeSH label unique IDs to their MeSH Tree number\n",
    "dui_tree_number_map = {}\n",
    "\n",
    "for descriptor in mesh_label_ontology['DescriptorRecordSet']['DescriptorRecord']:\n",
    "    tree_number = descriptor.get('TreeNumberList')\n",
    "    if tree_number is not None:\n",
    "        tree_number = tree_number.get('TreeNumber')\n",
    "    if isinstance(tree_number, list):\n",
    "        dui_tree_number_map[descriptor['DescriptorUI']] = tree_number\n",
    "    else:\n",
    "        dui_tree_number_map[descriptor['DescriptorUI']] = [tree_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of 0th level MeSH Tree codes to their semantic representations\n",
    "tree_level_0_map = {\n",
    "    'A': 'anatomy',\n",
    "    'B': 'organisms',\n",
    "    'C': 'diseases',\n",
    "    'D': 'chemicals and drugs',\n",
    "    'E': 'analytical, diagnostic, and therapeutic techniques, and equipment',\n",
    "    'F': 'psychiatry and psychology',\n",
    "    'G': 'phenomena and processes',\n",
    "    'H': 'disciplines and occupations',\n",
    "    'I': 'anthropology, education, sociology, and social phenomena',\n",
    "    'J': 'technology, industry, and agriculture',\n",
    "    'K': 'humanities',\n",
    "    'L': 'information science',\n",
    "    'M': 'named groups',\n",
    "    'N': 'health care',\n",
    "    'V': 'publication characteristics',\n",
    "    'Z': 'geographicals'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of 1st level MeSH Tree codes to their semantic representations\n",
    "tree_level_1_map = {}\n",
    "\n",
    "for descriptor in mesh_label_ontology['DescriptorRecordSet']['DescriptorRecord']:\n",
    "    tree_number = descriptor.get('TreeNumberList')\n",
    "    descriptor_name = descriptor.get('DescriptorName')\n",
    "    descriptor_name = descriptor_name['String']\n",
    "    if tree_number is not None:\n",
    "        tree_number = tree_number.get('TreeNumber')\n",
    "        if isinstance(tree_number, str):\n",
    "            if len(tree_number.split('.')) == 1:\n",
    "                tree_level_1_map[tree_number] = descriptor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_nodes = list(mesh_label_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how long our corpus of MeSH labels is\n",
    "n_mesh_labels = len(flatten(description_mesh_labels))\n",
    "n_mesh_labels_unique = len(flatten(description_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of MeSH labels in project descriptions: {}'.format(n_mesh_labels))\n",
    "print('Number of unique MeSH labels in project descriptions: {}'.format(n_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tree_group(label, label_dui_map, dui_tree_number_map, tree_map, level=1):\n",
    "    dui = label_dui_map.get(label)\n",
    "    groups = []\n",
    "    if dui is not None:\n",
    "        tree_numbers = dui_tree_number_map.get(dui)\n",
    "        if tree_numbers is not None:\n",
    "            for tn in tree_numbers:\n",
    "                if tn is not None:\n",
    "                    tn = tn.split('.')\n",
    "                    if level == 0:\n",
    "                        tn = tn[0][0]\n",
    "                    elif level == 1:\n",
    "                        tn = tn[0]\n",
    "                    else:\n",
    "                        tn = '.'.join(tn[:level - 1])\n",
    "                    group = tree_map.get(tn)\n",
    "                    if group is not None:\n",
    "                        groups.append(group)\n",
    "    if len(groups) > 0:\n",
    "        return list(set(groups))\n",
    "    else:\n",
    "        return ['Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(term_0, term_1, n):\n",
    "    return n / (mesh_label_unique_counts[term_0] + mesh_label_unique_counts[term_1] - n)\n",
    "\n",
    "def edge_association_strength(term_0, term_1, n):\n",
    "    return (2 * n_edges * n) / (mesh_label_edge_counts[term_0] * mesh_label_edge_counts[term_1])\n",
    "\n",
    "def coocurrence_association_strength(term_0, term_1, n):\n",
    "    return (2 * n_coocurrences * n) / (mesh_label_coocurrence_counts[term_0] * mesh_label_coocurrence_counts[term_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Documents\n",
    "\n",
    "To understand the nature of the terms in the network and their relationships with the documents, we need to extract quantitative information aobut the documents. Here we parse the corpus, using _spaCy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(raw_data, 'en_ranknl_long.txt'), 'r') as f:\n",
    "    text = f.read()\n",
    "stop_words = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop in stop_words:\n",
    "    nlp.vocab[stop].is_stop = True\n",
    "    nlp.vocab[stop.title()].is_stop = True\n",
    "    nlp.vocab[stop.upper()].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy allows us to build a corpus where each document has attributes, such as the document ID. This is useful if we want to match up documents with other fields in the DataFrame later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = gdb_df[['doc_id', 'mesh_labels', 'country', 'year']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions_tokenized = [tokenizer(d) for d in descriptions]\n",
    "%time descriptions_tokenized = textacy.Corpus(lang=nlp, texts=descriptions, metadatas=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Graph\n",
    "\n",
    "We now have a series of documents described in terms of their MeSH labels. From this we can build a network to explore the connections between concepts, and try to understand how we might be able to find \"innovative\" combinations or terms in an unsupervised manner. For this network, nodes will be the terms themselves, while an edge being drawn between two nodes will represent that term pair appearing in at least one document together - a coocurrence.\n",
    "\n",
    "- Node attributes:\n",
    "    - count: total number of times the term appeared in the corpus (int)\n",
    "    - count_normalised: count, normalised by the total number of terms in the corpus (float)\n",
    "    - mesh_tree_codes: array of MeSH Tree Numbers (list)\n",
    "    - edge_count: number of other nodes that this node is connected to (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mesh_labels = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mln in mesh_label_nodes:\n",
    "    count = mesh_label_unique_counts[mln]\n",
    "    graph_mesh_labels.add_node(\n",
    "        mln,\n",
    "        count=count,\n",
    "        count_normalised=count / n_mesh_labels_unique,\n",
    "        group_0=map_tree_group(\n",
    "            mln, \n",
    "            mesh_label_dui_map, \n",
    "            dui_tree_number_map, \n",
    "            tree_level_0_map,\n",
    "            level=0\n",
    "        ),\n",
    "        group_1=map_tree_group(\n",
    "            mln, \n",
    "            mesh_label_dui_map, \n",
    "            dui_tree_number_map, \n",
    "            tree_level_1_map\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node is connected to another node by an edge when they coocurr in at least one document. However, this provides no information about how often different labels coocurr with each other. To account for this, we will provide a choice of weights for the edges. The first is the raw number of coocurrences across the corpus. We then also calculate the Jaccard index and the association strength as defined in Waltman _et al_.  2009 and Noyons _et al_. 2010 respectively.\n",
    "\n",
    "- Edge attributes\n",
    "    - weight_absolute: number of documents in which terms coocur (int)\n",
    "    - jaccard_similarity: number between 0 and 1 representing the Jaccard Index of terms (float)\n",
    "    - association_strength: association strength score of terms (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_combinations = []\n",
    "for ml in description_mesh_labels_unique:\n",
    "    mesh_label_doc = list(itertools.combinations(set(ml), r=2))\n",
    "    mesh_label_doc = [tuple(sorted(mld)) for mld in mesh_label_doc]\n",
    "    description_mesh_label_combinations.append(mesh_label_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_coocurrence_counts = Counter(flatten(description_mesh_label_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_coocurrence_count_map = defaultdict(dict)\n",
    "\n",
    "for ml, c in edge_coocurrence_counts.items():\n",
    "    n0 = ml[0]\n",
    "    n1 = ml[1]\n",
    "    mesh_label_coocurrence_count_map[n0][n1] = c\n",
    "    mesh_label_coocurrence_count_map[n1][n0] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coocurrences = len(flatten(description_mesh_label_combinations))\n",
    "n_edges = len(edge_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Co-occurrences:', n_coocurrences)\n",
    "print('Number of Edges:', n_edges)\n",
    "print('Mean Co-occurences per Edge:', n_coocurrences / n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_coocurrence_counts = {}\n",
    "mesh_label_edge_counts = {}\n",
    "\n",
    "for ml, co in mesh_label_coocurrence_count_map.items():\n",
    "    coocurrence_count = sum([n for n in co.values()])\n",
    "    edge_count = len(co)\n",
    "    mesh_label_edge_counts[ml] = edge_count\n",
    "    mesh_label_coocurrence_counts[ml] = coocurrence_count\n",
    "\n",
    "mesh_label_coocurrence_counts = Counter(mesh_label_coocurrence_counts)\n",
    "mesh_label_edge_counts = Counter(mesh_label_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_counter_extremes(mesh_label_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_counter_extremes(mesh_label_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    node_0 = edge[0]\n",
    "    node_1 = edge[1]\n",
    "    graph_mesh_labels.add_edge(node_0, node_1,\n",
    "                               coocurrences_absolute=count,\n",
    "                               jaccard_similarity=jaccard_similarity(node_0,\n",
    "                                                                     node_1,\n",
    "                                                                     count),\n",
    "                               association_strength=coocurrence_association_strength(node_0,\n",
    "                                                                                     node_1,\n",
    "                                                                                     count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mesh_term_raw_graph.json', 'w') as f:\n",
    "    json.dump(nx.node_link_data(graph_mesh_labels), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the edge attributes for the most common and some of the least common coocurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "\n",
    "print('{:<30}\\t{:<8}\\t{:30}\\t{:<8}\\t{:<15}\\t{}\\t{}\\n'.format(\n",
    "    'Node 0', 'Count 0', 'Node 1', 'Count 1', 'Cooccurences',\n",
    "    'Jaccard Index', 'Association Strength'))\n",
    "\n",
    "for l, n in edge_coocurrence_counts.most_common(top_n):\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    c0 = mesh_label_unique_counts[term_0]\n",
    "    c1 = mesh_label_unique_counts[term_1]\n",
    "    j = jaccard_similarity(term_0, term_1, n)\n",
    "    a = coocurrence_association_strength(term_0, term_1, n)\n",
    "    print('{:<30}\\t{:<8}\\t{:30}\\t{:<8}\\t{:<15}\\t{:.2f}\\t{:12.2f}'.format(l[0], c0, l[1], c1, n, j, a))\n",
    "    \n",
    "print('...')\n",
    "\n",
    "low_counts = {k: v for k, v in edge_coocurrence_counts.items() if v <= 3}\n",
    "low_count_keys_n = list(itertools.islice(low_counts, top_n))\n",
    "low_counts_n = [(k, low_counts[k]) for k in low_count_keys_n]\n",
    "\n",
    "for l, n in low_counts_n:\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    c0 = mesh_label_unique_counts[term_0]\n",
    "    c1 = mesh_label_unique_counts[term_1]\n",
    "    j = jaccard_similarity(term_0, term_1, n)\n",
    "    a = coocurrence_association_strength(term_0, term_1, n)\n",
    "    print('{:<30}\\t{:8}\\t{:30}\\t{:<8}\\t{:<15}\\t{:.2f}\\t{:12.2f}'.format(l[0], c0, l[1], c1, n, j, a))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that edges representing high numbers of coocurrences are unsurprisingly drawn between terms that have a high count across the corpus. Interestingly, the terms with the highest number of coocurrences are not the two that have the highest frequency (_Humans_ and _Students_). We can see that for the top terms, the Jaccard similarity hovers at the lower end of the range. We can see how it is modulated by both the number of coocurrences, but also the individual term frequencies. At the lower end of the coocurrence counts, we can see connected terms that exhibit a Jaccard index of only 0.01 or less. The association strength however shows a large degree of variation, in part due to its inherent nature of not being bounded between finite limits. The range of values among the most highly occurring terms and the less frequent terms are more consistent too. However, it can be seen that terms with lower frequencies can exhibit much higher association strengths, as their low counts reduces the chances of them coocurring \"by chance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Exploration\n",
    "\n",
    "In this section, we plot and describe characteristics of the MeSH labels and the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_cdf(ax, x, bins, normed=False, stats=True):\n",
    "    \n",
    "    v = plt.get_cmap('viridis')\n",
    "    pdf_c = v.colors[0]\n",
    "    cdf_c = v.colors[90]\n",
    "    mean_c = v.colors[180]\n",
    "    med_c = v.colors[230]\n",
    "    \n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "\n",
    "    pdf, bin_edges = np.histogram(x, bins)\n",
    "    cdf = np.cumsum(pdf / np.sum(pdf))\n",
    "    if normed:\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.bar(bin_edges[:-1], pdf / np.sum(pdf), np.diff(bin_edges), color=pdf_c)\n",
    "    else:\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.bar(bin_edges[:-1], pdf, np.diff(bin_edges), color=pdf_c)\n",
    "    if stats:\n",
    "        mean = np.mean(x)\n",
    "        median = np.median(x)\n",
    "        ax.axvline(mean, color=mean_c, linestyle='--', linewidth=3, label='Mean: {:.2f}'.format(mean), alpha=0.7)\n",
    "        ax.axvline(median, color=med_c, linestyle='--', linewidth=3, label='Median: {:.2f}'.format(median), alpha=0.7)\n",
    "        ax.legend()\n",
    "    ax.set_xlim((np.min(bin_edges), np.max(bin_edges)))\n",
    "    ax_cdf = ax.twinx()\n",
    "    ax_cdf.plot(bin_edges[:-1], cdf, color=cdf_c, linewidth=4, alpha=0.8)\n",
    "    ax_cdf.set_ylabel('Cumulative')\n",
    "    ax_cdf.set_ylim((0, 1))\n",
    "    ax_cdf.tick_params('y')\n",
    "    return ax, ax_cdf\n",
    "    \n",
    "def pdf_cdf_h(ax, y, bins, normed=False, stats=True):\n",
    "    v = plt.get_cmap('viridis')\n",
    "    pdf_c = v.colors[0]\n",
    "    cdf_c = v.colors[90]\n",
    "    mean_c = v.colors[180]\n",
    "    med_c = v.colors[230]\n",
    "    \n",
    "    xmax = np.max(y)\n",
    "    xmin = np.min(y)\n",
    "\n",
    "    pdf, bin_edges = np.histogram(y, bins)\n",
    "    cdf = np.cumsum(pdf / np.sum(pdf))\n",
    "    if normed:\n",
    "        ax.set_xlabel('Probability')\n",
    "        ax.barh(bin_edges[:-1], pdf / np.sum(pdf), np.diff(bin_edges), color=pdf_c)\n",
    "    else:\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.barh(bin_edges[:-1], pdf, np.diff(bin_edges), color=pdf_c)\n",
    "    if stats:\n",
    "        mean = np.mean(y)\n",
    "        median = np.median(y)\n",
    "        ax.axhline(mean, color=mean_c, linestyle='--', linewidth=3, label='Mean: {:.2f}'.format(mean), alpha=0.7)\n",
    "        ax.axhline(median, color=med_c, linestyle='--', linewidth=3, label='Median: {:.2f}'.format(median), alpha=0.7)\n",
    "        ax.legend()\n",
    "    ax.set_ylim((np.min(bin_edges), np.max(bin_edges)))\n",
    "    ax_cdf = ax.twiny()\n",
    "    ax_cdf.plot(cdf, bin_edges[:-1], color=cdf_c, linewidth=4, alpha=0.8)\n",
    "    ax_cdf.set_xlabel('Cumulative')\n",
    "    ax_cdf.set_xlim((0, 1))\n",
    "    ax_cdf.tick_params('y')\n",
    "    return ax, ax_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexbin_sidegrams(x, y, hb_bins=None, hb_gridsize=50, hb_mincnt=1, hb_xlabel='', \n",
    "                     title='', hb_ylabel='', cb_label='', \n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_hb = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "    cbaxes = plt.axes([0, 0.1, 0.02, 0.65])\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    hb = ax_hb.hexbin(x, y, bins=hb_bins, gridsize=hb_gridsize, cmap='viridis', mincnt=hb_mincnt)\n",
    "    ax_hb.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "    cb = plt.colorbar(hb, cax = cbaxes)\n",
    "    cb.set_label(cb_label)\n",
    "    cb.ax.yaxis.set_ticks_position('left')\n",
    "    cb.ax.yaxis.set_label_position('left')\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x[~np.isinf(x)])\n",
    "    xmin = np.min(x[~np.isinf(x)])\n",
    "    ymax = np.max(y[~np.isinf(y)])\n",
    "    ymin = np.min(y[~np.isinf(y)])\n",
    "\n",
    "    ax_hb.set_xlim((xmin, xmax))\n",
    "    ax_hb.set_ylim((ymin, ymax))\n",
    "    ax_hb.set_xlabel(hb_xlabel)\n",
    "    ax_hb.set_ylabel(hb_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_hb.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_hb.get_ylim())\n",
    "    \n",
    "#     labels = ax_hist_cum_x.get_yticklabels()\n",
    "#     labels[0] = ''\n",
    "#     ax_hist_cum_x.set_yticklabels(labels)\n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_sidegrams(x, y, title='', sc_ylabel='', sc_xlabel='',\n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True,\n",
    "                     **sc_kwargs):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_sc = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    sc = ax_sc.scatter(x, y, **sc_kwargs)\n",
    "    ax_sc.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "    ymax = np.max(y)\n",
    "    ymin = np.min(y)\n",
    "\n",
    "    ax_sc.set_xlim((xmin, xmax))\n",
    "    ax_sc.set_ylim((ymin, ymax))\n",
    "    ax_sc.set_xlabel(sc_xlabel)\n",
    "    ax_sc.set_ylabel(sc_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_sc.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_sc.get_ylim())\n",
    "    \n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_scatter_sidegrams(x, y, c, cmap='viridis', title='', sc_ylabel='', sc_xlabel='',\n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True,\n",
    "                     **sc_kwargs):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_sc = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    sc = ax_sc.scatter(x, y, c, cmap=cmap, **sc_kwargs)\n",
    "    ax_sc.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "    ymax = np.max(y)\n",
    "    ymin = np.min(y)\n",
    "\n",
    "    ax_sc.set_xlim((xmin, xmax))\n",
    "    ax_sc.set_ylim((ymin, ymax))\n",
    "    ax_sc.set_xlabel(sc_xlabel)\n",
    "    ax_sc.set_ylabel(sc_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_sc.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_sc.get_ylim())\n",
    "    \n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df = pd.DataFrame({'terms': list(mesh_label_unique_counts.keys())})\n",
    "\n",
    "terms_df['occurrences'] = terms_df['terms'].map(mesh_label_unique_counts)\n",
    "terms_df['cooccurrences'] = terms_df['terms'].map(mesh_label_coocurrence_counts)\n",
    "terms_df['edges'] = terms_df['terms'].map(mesh_label_edge_counts)\n",
    "terms_df['occurrences_log'] = np.log10(terms_df['occurrences'])\n",
    "terms_df['cooccurrences_log'] = np.log10(terms_df['cooccurrences'])\n",
    "terms_df['edges_log'] = np.log10(terms_df['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df['cooccurrences_per_occurrence'] = terms_df['cooccurrences'] / terms_df['occurrences']\n",
    "terms_df['edges_per_occurrence'] = terms_df['edges'] / terms_df['occurrences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_arrays(df, col_0, col_1):\n",
    "    sub_df = df[[col_0, col_1]]\n",
    "    sub_df = sub_df.replace([np.inf, -np.inf], np.nan)\n",
    "    sub_df = sub_df[((~pd.isnull(sub_df[col_0])) & (~pd.isnull(sub_df[col_1])))]\n",
    "    arr_0 = sub_df[col_0]\n",
    "    arr_1 = sub_df[col_1]\n",
    "    return arr_0, arr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_occurrences_arr, term_cooccurrences_arr = get_valid_arrays(terms_df, 'occurrences_log', 'cooccurrences_log')\n",
    "\n",
    "hexbin_sidegrams(term_occurrences_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Count',\n",
    "                 hb_ylabel='Log10 Term Co-occurrences Count',\n",
    "                 title='Term Occurrences vs. Term Co-ocurrences', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_occurrences_arr, term_edges_arr = get_valid_arrays(terms_df, 'occurrences_log', 'edges_log')\n",
    "\n",
    "hexbin_sidegrams(term_occurrences_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Count',\n",
    "                 hb_ylabel='Log10 Term Edge Count',\n",
    "                 title='Term Occurrences vs. Term Edges', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(term_edges_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Edge Count',\n",
    "                 hb_ylabel='Log10 Term Co-occurrence Count',\n",
    "                 title='Term Occurrences vs. Term Edges', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50,\n",
    "                 th_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mesh_labels_unique = list(set(flatten(description_mesh_labels_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_association_strengths_dict = defaultdict(list)\n",
    "\n",
    "for l, n in edge_coocurrence_counts.items():\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    assoc_str = coocurrence_association_strength(term_0, term_1, n)\n",
    "    term_association_strengths_dict[term_0].append(assoc_str)\n",
    "    term_association_strengths_dict[term_1].append(assoc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mean_assoc_strengths = {}\n",
    "term_std_assoc_strengths = {}\n",
    "term_med_assoc_strengths = {}\n",
    "term_min_assoc_strengths = {}\n",
    "term_max_assoc_strengths = {}\n",
    "\n",
    "for t, a_s in term_association_strengths_dict.items():\n",
    "    term_mean_assoc_strengths[t] = np.mean(a_s)\n",
    "    term_std_assoc_strengths[t] = np.std(a_s)\n",
    "    term_med_assoc_strengths[t] = np.median(a_s)\n",
    "    term_min_assoc_strengths[t] = np.min(a_s)\n",
    "    term_max_assoc_strengths[t] = np.max(a_s)\n",
    "    \n",
    "terms_df['mean_assoc_strength'] = terms_df['terms'].map(term_mean_assoc_strengths)\n",
    "terms_df['median_assoc_strength'] = terms_df['terms'].map(term_med_assoc_strengths)\n",
    "terms_df['std_assoc_strength'] = terms_df['terms'].map(term_std_assoc_strengths)\n",
    "terms_df['min_assoc_strength'] = terms_df['terms'].map(term_min_assoc_strengths)\n",
    "terms_df['max_assoc_strength'] = terms_df['terms'].map(term_max_assoc_strengths)\n",
    "terms_df['mean_assoc_strength_log'] = np.log10(terms_df['mean_assoc_strength'])\n",
    "terms_df['median_assoc_strength_log'] = np.log10(terms_df['median_assoc_strength'])\n",
    "terms_df['std_assoc_strength_log'] = np.log10(terms_df['std_assoc_strength'])\n",
    "terms_df['min_assoc_strength_log'] = np.log(terms_df['min_assoc_strength'])\n",
    "terms_df['max_assoc_strength_log'] = np.log(terms_df['max_assoc_strength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences, mean_assoc_strengths = get_valid_arrays(terms_df, 'cooccurrences', 'mean_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(np.log(cooccurrences), mean_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_assoc_strengths, std_assoc_strengths = get_valid_arrays(terms_df, 'mean_assoc_strength_log', 'std_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(mean_assoc_strengths, std_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_assoc_strengths, med_assoc_strengths = get_valid_arrays(terms_df, 'mean_assoc_strength_log', 'median_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(mean_assoc_strengths, med_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_assoc_strengths, max_assoc_strengths = get_valid_arrays(terms_df, 'min_assoc_strength_log', 'max_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(min_assoc_strengths, max_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_df(df, cols):\n",
    "    sub_df = df[cols]\n",
    "    sub_df = sub_df.replace([np.inf, -np.inf], np.nan)\n",
    "    for col in cols:\n",
    "        sub_df = sub_df[~pd.isnull(sub_df[col])]\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df_tmp = get_valid_df(terms_df, ['terms', 'occurrences_log', 'cooccurrences_log', 'edges_log', 'mean_assoc_strength_log', 'median_assoc_strength_log',\n",
    "                             'std_assoc_strength_log', 'min_assoc_strength_log', 'max_assoc_strength_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scatter_matrix(terms_df_tmp.sample(frac=0.1, random_state=42), alpha=0.1, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold.t_sne import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_tsne = tsne.fit_transform(terms_df_tmp[['occurrences_log', 'cooccurrences_log', 'edges_log', 'mean_assoc_strength_log', 'median_assoc_strength_log',\n",
    "                                              'std_assoc_strength_log', 'min_assoc_strength_log', 'max_assoc_strength_log']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df_tmp['tsne_0'] = terms_tsne[:, 0]\n",
    "terms_df_tmp['tsne_1'] = terms_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = ColumnDataSource(terms_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BoxZoomTool, ResetTool, WheelZoomTool, LinearColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(palette='Magma256', low=min(terms_df_tmp['occurrences_log']), high=max(terms_df_tmp['occurrences_log']))\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Term\", \"@terms\"),\n",
    "])\n",
    "box = BoxZoomTool()\n",
    "reset = ResetTool()\n",
    "zoom = WheelZoomTool()\n",
    "# color_mapper = CategoricalColorMapper(factors=list(w2v_df['label'].values.unique()))\n",
    "\n",
    "terms_tsne_scatter = figure(width=700, height=600, tools=[hover, box, reset, zoom],\n",
    "                     title='TSNE Plot of MeSH Term Network Characteristics')\n",
    "terms_tsne_scatter.circle(x='tsne_0', y='tsne_1', source=cds, alpha=0.2,\n",
    "                          color={'field': 'occurrences_log', 'transform': color_mapper}\n",
    "#                           color=factor_cmap('clusters_k', palette=Category20_20, factors=list(w2v_df['clusters_k'].unique())),\n",
    "#                           size=6\n",
    "                         )\n",
    "terms_tsne_scatter.xgrid.visible = False\n",
    "terms_tsne_scatter.ygrid.visible = False\n",
    "\n",
    "\n",
    "show(terms_tsne_scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df = pd.DataFrame({'edge': list(edge_coocurrence_counts.keys())})\n",
    "term_edges_df['cooccurrence_count'] = term_edges_df['edge'].map(edge_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = term_edges_df['edge'].values\n",
    "edge_terms_0 = [e[0] for e in edge_list]\n",
    "edge_terms_1 = [e[1] for e in edge_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['term_0'] = edge_terms_0\n",
    "term_edges_df['term_1'] = edge_terms_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['term_occurrence_0'] = term_edges_df['term_0'].map(mesh_label_unique_counts)\n",
    "term_edges_df['term_occurrence_1'] = term_edges_df['term_1'].map(mesh_label_unique_counts)\n",
    "term_edges_df['term_cooccurrence_0'] = term_edges_df['term_0'].map(mesh_label_coocurrence_counts)\n",
    "term_edges_df['term_cooccurrence_1'] = term_edges_df['term_1'].map(mesh_label_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['cooccurrence_per_occurrence'] = term_edges_df['cooccurrence_count'] / (term_edges_df['term_occurrence_0'] + term_edges_df['term_occurrence_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['cooccurrence_assoc_strength'] = (2 * n_coocurrences * term_edges_df['cooccurrence_count']) / (term_edges_df['term_cooccurrence_0'] * term_edges_df['term_cooccurrence_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_association_strength(edge):\n",
    "    edge = ast.literal_eval(edge)\n",
    "    return coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge])\n",
    "\n",
    "term_edges_df['assoc_strengths'] = term_edges_df['edge'].apply(lambda x: map_association_strength(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_sample_df = term_edges_df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(np.log(term_edges_df['cooccurrence_count']), np.log(term_edges_df['assoc_strengths']),\n",
    "                 hb_xlabel='Log10 Term Edge Co-occurrence Count',\n",
    "                 hb_ylabel='Log10 Term Edge Association Strength',\n",
    "                 title='Term Edge Co-occurrence Count vs. Association Strengths', \n",
    "                 cb_label='Log Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50,\n",
    "                 hb_bins='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_association_strengths = []\n",
    "term_edge_counts = []\n",
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    term_edge_association_strengths.append(coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge]))\n",
    "    term_edge_counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_association_strengths_log = np.log(term_edge_association_strengths)\n",
    "term_edge_counts_log = np.log(term_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_jaccard_similarities = []\n",
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    term_edge_jaccard_similarities.append(jaccard_similarity(edge[0], edge[1], count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_jaccard_similarities_log = np.log(term_edge_jaccard_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(term_edge_counts_log, term_edge_jaccard_similarities_log,\n",
    "                 hb_bins='log', hb_gridsize=50,\n",
    "                 hb_xlabel='Number of Tokens',\n",
    "                 hb_ylabel='Number of MeSH Labels',\n",
    "                 title='Document Lengths vs. Number of Unique MeSH Labels Identified', \n",
    "                 cb_label='Log Document Count'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no new dataframe here, as we can just use the gdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['n_tokens'] = [len(d) for d in descriptions_tokenized]\n",
    "gdb_df['n_mesh_labels'] = [len(ml) for ml in description_mesh_labels]\n",
    "gdb_df['n_unique_mesh_labels'] = [len(ml) for ml in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(gdb_df['n_tokens'], gdb_df['n_unique_mesh_labels'], \n",
    "                 hb_bins='log', hb_gridsize=50,\n",
    "                 hb_xlabel='Number of Tokens',\n",
    "                 hb_ylabel='Number of Unique MeSH Labels',\n",
    "                 title='Document Lengths vs. Number of Unique MeSH Labels Identified', \n",
    "                 cb_label='Log Document Count',\n",
    "                 th_bins=200,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a large number (around 30%) of documents with less than 50 tokens and less than 10 MeSH labels. There's a roughly linear relationship between the number of tokens and the number of MeSH labels, as would be reasonable to expect, however the spread of points is very large. There vast majority of documents have less than 800 tokens or 40 labels. The table below gives exact values for the summary statistics of these distributions.\n",
    "\n",
    "One other notable feature is that the histogram of document lengths shows a few humps at around 250, 350 and 500 tokens. Perhaps this reflects proposal or abstract word limits imposed by funding bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_percentiles(distributions, percentiles=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]):\n",
    "    percentile_distributions = []\n",
    "    for d in distributions:\n",
    "        if len(d) > 1:\n",
    "            percentile_distributions.append(np.percentile(d, percentiles))\n",
    "        else:\n",
    "            percentile_distributions.append([np.nan] * len(percentiles))\n",
    "    return np.matrix(percentile_distributions)\n",
    "\n",
    "def matrix_2_df(matrix, header_prefix):\n",
    "    headers = ['{}_{}'.format(header_prefix, i * 10) for i in range(0, matrix.shape[1])]\n",
    "    df = pd.DataFrame(matrix, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first document features we will calculate are the **fraction of shared labels**, the **fraction of unique labels**, and the **percentiles of the distribution of label frequencies**.\n",
    "\n",
    "We will put these in a new dataframe, which will contain only the features we plan to use for document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shareds = []\n",
    "n_uniques = []\n",
    "label_freq_norm_dists = []\n",
    "\n",
    "for dmlu in description_mesh_labels_unique:\n",
    "    label_freqs = [mesh_label_unique_counts[label] for label in dmlu]\n",
    "    label_freq_norm_dists.append([lf / n_mesh_labels_unique for lf in label_freqs])\n",
    "    n_shared = len([lf for lf in label_freqs if lf > 1])\n",
    "    n_shareds.append(n_shared)\n",
    "    n_uniques.append(len(dmlu) - n_shared)\n",
    "\n",
    "frequency_normed_percentiles = distribution_percentiles(label_freq_norm_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_label_frequency_norm_percentiles_df = matrix_2_df(frequency_normed_percentiles, 'label_frequency_percentile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = gdb_df[['doc_id', 'n_unique_mesh_labels']]\n",
    "doc_df = doc_df.merge(pd.DataFrame({'n_shared_labels': n_shareds, 'n_unique_labels': n_uniques, 'doc_id': gdb_df['doc_id'].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['n_shared_labels_norm'] = doc_df['n_shared_labels'] / doc_df['n_unique_mesh_labels']\n",
    "doc_df['n_unique_labels_norm'] = doc_df['n_unique_labels'] / doc_df['n_unique_mesh_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, doc_label_frequency_norm_percentiles_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add the **perctentile distributions of the assocation strengths** of the intra-document label co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_n_unique_mesh_labels = gdb_df['n_unique_mesh_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_association_strengths = []\n",
    "\n",
    "for dmlc in description_mesh_label_combinations:\n",
    "    doc_association_strengths.append([coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge]) for edge in dmlc])\n",
    "    \n",
    "doc_assoc_strength_percentiles = distribution_percentiles(doc_association_strengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_doc_association_strength_headers = ['assoc_strength_{}_pctile'.format(i * 10) for i in range(0, 11)]\n",
    "percentile_df = pd.DataFrame(doc_assoc_strength_percentiles, columns=percentile_doc_association_strength_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, percentile_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to know the overlap of the documents with each other so we will calculate the percentile distributions of **document jaccard similarities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(d0, d1):\n",
    "    intersection_cardinality = len(set.intersection(*[set(d0), set(d1)]))\n",
    "    union_cardinality = len(set.union(*[set(d0), set(d1)]))\n",
    "    if union_cardinality > 0:\n",
    "        return intersection_cardinality / float(union_cardinality)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_unique_joined = [' '.join([t.replace('-', '_').replace(' ', '_').replace(',', '').lower() for t in d]) for d in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = cv.fit_transform(description_mesh_label_unique_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow_size = docs_bow.shape[0]\n",
    "# number of random documents to sample from the corpus\n",
    "size = 2000\n",
    "\n",
    "mean_jaccard_indices = []\n",
    "\n",
    "for i in range(docs_bow_size):\n",
    "    d0 = description_mesh_labels_unique[i]\n",
    "    d_ids = np.random.randint(0, high=docs_bow_size, size=(size))\n",
    "    d_ids[~np.in1d(d_ids, i).reshape(d_ids.shape)]\n",
    "    random_docs = itemgetter(*d_ids)(description_mesh_labels_unique)\n",
    "    \n",
    "    mean_jaccard_indices.append([jaccard_index(d0, random_docs[i]) for i in range(size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_jaccard_percentiles = distribution_percentiles(mean_jaccard_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_jaccard_indices_overlap_only = [[d for d in doc if d != 0] for doc in mean_jaccard_indices]\n",
    "doc_jaccard_overlap_percentiles = distribution_percentiles(doc_jaccard_indices_overlap_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_indices_overlap_df = matrix_2_df(doc_jaccard_overlap_percentiles, 'jaccard_overlap_pctile')\n",
    "jaccard_indices_df = matrix_2_df(doc_jaccard_percentiles, 'jaccard_pctile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, jaccard_indices_overlap_df], axis=1, join_axes=[doc_df.index])\n",
    "doc_df = pd.concat([doc_df, jaccard_indices_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv(proc_data + 'gdb_description_network_features_{}.csv'.format(today_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intra-document word embedding similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_ends = column_to_list(gdb_df, 'mesh_term_token_end_idx')\n",
    "description_mesh_starts = column_to_list(gdb_df, 'mesh_term_token_start_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_phrases = column_to_list(gdb_df, 'original_phrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_joined = [[t.replace('-', '_').replace(' ', '_').replace(',', '').lower() for t in d] for d in description_mesh_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_mesh_label_subs = []\n",
    "\n",
    "for description, originals, labels in zip(descriptions, description_mesh_phrases, description_mesh_label_joined):\n",
    "#     description = description.replace('{', ' ').replace('}', ' ')\n",
    "    labels_used = []\n",
    "    originals_used = []\n",
    "    for label, original in zip(labels, originals):\n",
    "#         description = description[:start] + '{}' + description[end:]\n",
    "#         description = description.format(label)\n",
    "        if label not in labels_used:\n",
    "            description = description.replace(original, label.lower())\n",
    "            labels_used.append(label)\n",
    "            originals_used.append(original)\n",
    "            \n",
    "    descriptions_mesh_label_subs.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_labeled_tokenized = [tokenizer(d) for d in descriptions_mesh_label_subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_labeled_tokens = [[t.text for t in d] for d in descriptions_labeled_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(descriptions_labeled_tokens, size=300, window=7, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_word_vecs = w2v_model.wv\n",
    "mesh_label_word_vecs.save('../models/mesh_labels_word_vecs_{}'.format(today_str))\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def doc_word_vecs(doc_combinations, word_vecs):\n",
    "    description_cosine_similarities = []\n",
    "    for combos in doc_combinations:\n",
    "        doc_similarities = []\n",
    "        if len(combos) > 0:\n",
    "            for combo in combos:\n",
    "                term_0 = combo[0].replace('-', '_').replace(' ', '_').replace(',', '').lower()\n",
    "                term_1 = combo[1].replace('-', '_').replace(' ', '_').replace(',', '').lower()\n",
    "                if (term_0 in word_vecs) & (term_1 in word_vecs):\n",
    "                    doc_similarities.append(cosine(word_vecs[term_0], word_vecs[term_1]))\n",
    "        if len(doc_similarities) == 0:\n",
    "            doc_similarities.append(np.array([np.nan]))\n",
    "        description_cosine_similarities.append(doc_similarities)\n",
    "    return description_cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarities = doc_word_vecs(description_mesh_label_combinations, mesh_label_word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarity_percentiles = distribution_percentiles(description_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarity_percentiles_df = matrix_2_df(description_cosine_similarity_percentiles, 'mesh_label_w2v_cosine_pctile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, description_cosine_similarity_percentiles_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv(proc_data + 'gdb_description_network_features_{}.csv'.format(today_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering based on all non-network features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should consider **network features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = doc_df['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df_features = doc_df.drop(columns=['doc_id', 'n_unique_mesh_labels', 'n_shared_labels', 'n_unique_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=5, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sclr = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in doc_df_features.columns:\n",
    "    doc_df_features[c][pd.isnull(doc_df_features[c])] = np.mean(doc_df_features[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df_features = std_sclr.fit_transform(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.fit(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gm.predict(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame({'cluster': labels, 'doc_id': doc_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = label_df[label_df['cluster'] == 3].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_unique_arr = np.array(description_mesh_labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, l in zip(gdb_df.iloc[cluster_0[:20]]['Description'].values, description_mesh_labels_unique_arr[cluster_0[:20]]):\n",
    "    print('=============================================== \\n')\n",
    "    print(l, '\\n')\n",
    "    print(d, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(description_mesh_labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(d) for d in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lsi = corpus2dense(corpus_lsi, num_terms=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lsi = matrix_lsi.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_labels = lof.fit_predict(matrix_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pioneer_doc_ids = gdb_df[gdb_df['Source ID'] == 'pioneers']['doc_id'].index.values\n",
    "global_doc_ids = gdb_df[gdb_df['Source ID'] == 'global']['doc_id'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df = pd.DataFrame({'cluster': lof_labels, 'doc_id': doc_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lofs = lof_df[lof_df['cluster'] == -1]['doc_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['local_outlier_factor'] = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, l in zip(gdb_df.iloc[lofs[:20]]['Description'].values, gdb_df.iloc[lofs[:20]]['mesh_labels'].values):\n",
    "    print('=============================================== \\n')\n",
    "    print(l, '\\n')\n",
    "    print(d, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count = Counter(lof_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_node_counts_x = []\n",
    "edge_node_counts_y = []\n",
    "\n",
    "for k, v in edge_node_counts.items():\n",
    "    edge_node_counts_x.append(k)\n",
    "    edge_node_counts_y.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax0.scatter(np.log(edge_node_counts_x), edge_node_counts_y, color=plt.get_cmap('viridis').colors[0], alpha=0.2)\n",
    "ax0.set_xlabel('Log Number of Nodes')\n",
    "ax0.set_ylabel('Number of Edges')\n",
    "pdf_cdf(ax1, np.log(edge_nodes), 100, normed=False)\n",
    "ax1.set_xlabel('Log Number of Edges')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_generator = community.girvan_newman(graph_mesh_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, count in mesh_label_edges.items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_doc_association_strength = [0 if pd.isnull(a) else a for a in mean_doc_association_strength ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts['Nutrition Policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_policy_neighbours = list(graph_mesh_labels.neighbors('Nutrition Policy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_processes_neighbours = list(graph_mesh_labels.neighbors('Mental Processes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nodes = list(set(nutrition_policy_neighbours + mental_processes_neighbours))\n",
    "sub_nodes = [sn for sn in sub_nodes if mesh_label_counts[sn] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mental_processes_nutrition_policy_graph.json', 'w') as f:\n",
    "    json.dump(nx.node_link_data(sub_graph), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nx.to_pandas_adjacency(sub_graph, weight='association_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df)\n",
    "df_log[np.isinf(df_log)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df_log, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
