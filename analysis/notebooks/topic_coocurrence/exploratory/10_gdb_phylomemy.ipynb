{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks and Word Vectors with MeSH Labels\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.dynamics import PhylomemeticGraph\n",
    "from rhodonite.graphs import SlidingWindowGraph\n",
    "from rhodonite.spectral import association_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.generation import price_network\n",
    "from graph_tool.draw import graph_draw\n",
    "from graph_tool.all import GraphView, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are going to load both the GDB and the RWJF Pioneer and Global projects, and join them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove projects from GitHub as they don't play nicely with MeSH terms, and Crunchbase as they're very short. There are also some projects with null descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the two sets of projects and extract their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)\n",
    "gdb_df.set_index('doc_id', inplace=True)\n",
    "gdb_df = gdb_df.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a MeSH Label Corpus\n",
    "\n",
    "We need to build a corpus of MeSH label transformed documents that is appropriate for the network we want to build. This will require some filtering, however first we should build a vocabulary of all the terms that we have, so that we can reference any of them by a unique ID at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For filtering later, we will calculate the counts of the MeSH labels. We know already that there are some labels which are highly over-represented, and many which occur only once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_filter(docs, high_threshold=None, low_threshold=None, remove=[], counter=None):\n",
    "    \"\"\"freqency_filter\n",
    "    Filters words from a corpus that occur more frequently than high_threshold\n",
    "    and less frequently than low_threshold.\n",
    "    \n",
    "    Args:\n",
    "        docs (:obj:`list` of :obj:`list`): Corupus of tokenised documents.\n",
    "        high_threshold (int): Upper limit for token frequency\n",
    "        low_threshold (int): Lower limit for token frequency\n",
    "        remove (:obj:`list`): List of terms to remove\n",
    "    \n",
    "    Yields:\n",
    "        doc_filtered (:obj:`list`): Document with elements removed based\n",
    "            on frequency\n",
    "    \"\"\"\n",
    "    docs_filtered = []\n",
    "    if counter is None:\n",
    "        counter = Counter(flatten(docs))\n",
    "    for doc in docs:\n",
    "        doc_filtered = []\n",
    "        for t in doc:\n",
    "            if t in remove:\n",
    "                continue\n",
    "            if high_threshold is not None:\n",
    "                if counter[t] > high_threshold:\n",
    "                    continue\n",
    "            if low_threshold is not None:\n",
    "                if counter[t] < low_threshold:\n",
    "                    continue\n",
    "            doc_filtered.append(t)\n",
    "        docs_filtered.append(doc_filtered)\n",
    "    return docs_filtered\n",
    "\n",
    "def filter_description_labels(description_labels, fn):\n",
    "    return [list(filter(fn, dl)) for dl in description_labels]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))\n",
    "mesh_label_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_filtered = frequency_filter(description_mesh_labels, high_threshold=40000,\n",
    "                                                    low_threshold=5,\n",
    "                                                    remove = \n",
    "                                                    ['Students', 'Humans', 'Animals', 'Research','Goals',\n",
    "                                                     'Universities', 'Research Personnel', 'United States', \n",
    "                                                     'United Kingdom', 'Research', 'Awards and Prizes',\n",
    "                                                     'Faculty', 'Mice', 'Mathematics', 'Fellowships and Scholarships',\n",
    "                                                    'Surveys and Questionnaires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases(description_mesh_labels_filtered, min_count=3)\n",
    "bigrammer = Phraser(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_bigrams = [bigrammer[d] for d in description_mesh_labels_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(description_mesh_labels_bigrams)\n",
    "trigrammer = Phraser(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_trigrams = [trigrammer[d] for d in description_mesh_labels_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_final = []\n",
    "for d in description_mesh_labels_trigrams:\n",
    "    corrected_d = []\n",
    "    for t in d:\n",
    "        if len(t.split('_')) > 1:\n",
    "            parts = t.split('_')\n",
    "            corrected_d.append(' '.join(sorted(set(parts))))\n",
    "        else:\n",
    "            corrected_d.append(t)\n",
    "    description_mesh_labels_final.append(corrected_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_mesh_labels = Dictionary(description_mesh_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['coocurrence_labels'] = description_mesh_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df[gdb_df['coocurrence_labels'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Projects by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the most recent 10 years of projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df_co[(gdb_df_co['year'] >= 2006) & (gdb_df_co['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sliding Window Coocurrence Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will want to create a new set of labelled descriptions where the terms with very high counts and little semantic value are removed, and also those that appear very few times in the corpus. We will also need to map the labels to token IDs which can then act as the vertex values in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = range(2006, 2018)\n",
    "co_graphs = [SlidingWindowGraph(gdb_df_co[gdb_df_co['year'] == t]['coocurrence_labels'],\n",
    "                             dictionary=dictionary_mesh_labels, window_size=2)\n",
    "          for t in times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_graphs = [g.prepare() for g in co_graphs]\n",
    "co_graphs = [g.build() for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_strengths = [association_strength(g) for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_period = 0\n",
    "end_period = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PhylomemeticGraph(co_graphs[start_period:end_period], association_strengths[start_period:end_period],\n",
    "                       dictionary_mesh_labels, times[start_period:end_period],\n",
    "                       max_weight=None, min_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time pg = pg.prepare('/Users/grichardson/cfinder/pg_out', '/Users/grichardson/cfinder/CFinder_commandline_mac')\n",
    "%time pg = pg.prepare('/Users/grichardson/cfinder/pg_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cs in pg.clique_sets:\n",
    "    print(Counter([len(c) for c in cs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pg.build(workers=4, min_clique_size=5, delta_0=0.4, parent_limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_antecedents(vertices, limit):\n",
    "    \"\"\"find_antecedents\n",
    "    Find all the antecedents of a particular vertex.\n",
    "    \n",
    "    Args:\n",
    "        vertices (:obj:`iter` of :obj:`Vertex`): A list of vertices\n",
    "            for which the antecedents need to be found.\n",
    "    \n",
    "    Returns:\n",
    "        antes (:obj:`iter` of :obj:`Vertex`): A list of vertices that\n",
    "            are the antecedents of the input vertices.\n",
    "    \"\"\"\n",
    "    l = 0\n",
    "    antes = []\n",
    "    for v in vertices:\n",
    "        if l < limit:\n",
    "            if v.in_degree() > 0:\n",
    "                antes.append(list(v.in_neighbors()))\n",
    "                if len(list(v.in_neighbors())) > 1:\n",
    "                    antes += find_antecedents(v.in_neighbors())\n",
    "                else:\n",
    "                    antes += find_antecedents(list(v.in_neighbors()))\n",
    "                l += 1\n",
    "    return antes\n",
    "\n",
    "def get_age(g, v):\n",
    "    \"\"\"get_age\n",
    "    Finds the age of a vertex in a PhylomemeticGraph.\n",
    "    Very. Slowly.\n",
    "    \n",
    "    Args:\n",
    "        g (:obj:`PhylomemeticGraph`): A phylomemetic graph.\n",
    "        v (:obj:`Vertex` or int): A vertex in g.\n",
    "        \n",
    "    Returns:\n",
    "        age (int): The difference in time between v and its most\n",
    "            distant antecedent.\n",
    "    \"\"\"\n",
    "    antes = find_antecedents([v])\n",
    "    if len(antes) == 0:\n",
    "        return 0\n",
    "    year = g.vp['times'][v]\n",
    "    if type(antes) == list:\n",
    "        antes = flatten(antes)\n",
    "    oldest_ancestor = min([g.vp['times'][a] for a in antes])\n",
    "    age = year - oldest_ancestor\n",
    "    return age\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_thresh = GraphView(pg, efilt=lambda e: pg.ep['jaccard_weights'][e] > 0.5)\n",
    "pg_thresh = GraphView(pg_thresh, vfilt=lambda v: (v.out_degree() > 0) | (v.in_degree() > 0))\n",
    "graph_draw(pg_thresh, vertex_fill_color=pg_thresh.vp['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertex in pg_thresh.vertices():\n",
    "#     if np.random.randint(0, 10) > 5:\n",
    "    if vertex.in_degree() == 1:\n",
    "#             if vertex.out_degree() > 0:\n",
    "        terms_s = pg_thresh.vp['terms'][vertex]\n",
    "        print('-', pg_thresh.vp['times'][vertex], '-')\n",
    "        print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_s])))\n",
    "        print(pg_thresh.vp['density'][vertex])\n",
    "        print('\\n=== Parents ===')\n",
    "\n",
    "        for i, n in enumerate(vertex.in_neighbors()):\n",
    "            terms_n = pg_thresh.vp['terms'][n]\n",
    "            print(pg_thresh.vp['times'][n])\n",
    "            print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "#         print('\\n=== Children ===')\n",
    "\n",
    "#         for i, n in enumerate(n.out_neighbours()):\n",
    "#             terms_n = pg_thresh.vp['terms'][n]\n",
    "#             print(pg_thresh.vp['times'][n])\n",
    "#             print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Computed Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.dynamics import label_emergence, label_special_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full.load(os.path.join(inter_data, 'phylomemetics/pg_2006_2017_cat.xml.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_density(g):\n",
    "    corr = g.new_vertex_property('float')\n",
    "    for v in g.vertices():\n",
    "        card = len(g.vp['terms'][v])\n",
    "        corr[v] = g.vp['density'][v] / card\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = correct_density(pg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_phylomemetic_graph(g, term, dictionary, min_jaccard=0):\n",
    "    \"\"\"filter_phylomemetic_graph\n",
    "    Get a subgraph of vertices that contain a particular term.\n",
    "    \"\"\"\n",
    "    term_id = dictionary.token2id[term]\n",
    "    g_filt = GraphView(\n",
    "        g,\n",
    "        vfilt=lambda v: term_id in g.vp['terms'][v],\n",
    "        efilt=lambda e: g.ep['jaccard_weights'][e] > min_jaccard\n",
    "    )\n",
    "    return g_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vp_values(g, vertex_prop_name):\n",
    "    mask = g.get_vertex_filter()[0]\n",
    "    if mask is not None:\n",
    "        mask = np.where(mask.get_array())\n",
    "        pm = g.vp[vertex_prop_name].get_array()[mask]\n",
    "    else:\n",
    "        pm = g.vp[vertex_prop_name].get_array()\n",
    "    \n",
    "    return pm\n",
    "\n",
    "def get_aggregate_vp(g, vp, vp_grouper, agg=None):\n",
    "    \"\"\"aggregate_property_map\n",
    "    \n",
    "    Args:\n",
    "        g (:obj:`Graph`): A graph.\n",
    "        vp (str): String representing an internal property map\n",
    "            of graph, g.\n",
    "        vp_grouper (str): String representing name of an internal\n",
    "            property map that will be used to group by.\n",
    "        agg (:obj:`function`): Function to aggregate by. For\n",
    "            example, min, max, sum, numpy.mean, etc.\n",
    "    Returns:\n",
    "        (:obj:`iter` of float): Aggregated values from x. \n",
    "    \"\"\"\n",
    "    vp_vals = get_vp_values(g, vp)\n",
    "    vp_agg = get_vp_values(g, vp_grouper)\n",
    "    \n",
    "    sid_x = vp_agg.argsort()\n",
    "    # Get where the sorted version of base changes groups\n",
    "    split_idx = np.flatnonzero(np.diff(vp_agg[sid_x]) > 0) + 1\n",
    "    # OR np.unique(base[sidx],return_index=True)[1][1:]\n",
    "\n",
    "    # Finally sort inp based on the sorted indices and split based on split_idx\n",
    "    vp_vals_grouped = np.split(vp_vals[sid_x], split_idx)\n",
    "    \n",
    "    x = sorted(set(vp_agg))\n",
    "    if agg: \n",
    "        y = [agg(vvg) for vvg in vp_vals_grouped]\n",
    "    else:\n",
    "        y = vp_vals_grouped\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ages(g):\n",
    "    \"\"\"label_ages\n",
    "    Get the ages of each vertex in a graph and put them into\n",
    "    a property map. The age is defined as the number of steps between\n",
    "    a vertex and its most distant antecedent.\n",
    "    \n",
    "    Args:\n",
    "        g (:obj:`Graph`): A graph.\n",
    "        \n",
    "    Returns:\n",
    "        age_vp (:obj:`PropertyMap(`): A property map containing\n",
    "            the age of each vertex.\n",
    "    \"\"\"\n",
    "\n",
    "    # get all vertices with age 0 in dictionary\n",
    "    ages = {}\n",
    "    for v in g.vertices():\n",
    "        if v.in_degree() == 0:\n",
    "            ages[v] = 0\n",
    "\n",
    "    # find youngest in-neighbour of each node\n",
    "    # if its age is in the dict then get the new age by\n",
    "    # adding the difference\n",
    "    # else append back on to the list to try again\n",
    "    vertices = list(g.vertices())\n",
    "    for v in vertices:\n",
    "        if v in ages:\n",
    "            continue\n",
    "        else:\n",
    "            year_v = g.vp['times'][v]\n",
    "\n",
    "            predecessors = list(v.in_neighbors())\n",
    "            years = [g.vp['times'][p] for p in predecessors]\n",
    "            min_i = np.argmin(years)\n",
    "            min_neighbor = predecessors[min_i]\n",
    "            year_neighbor = years[min_i]\n",
    "        if min_neighbor in ages:\n",
    "            year_parent = ages[min_neighbor]\n",
    "            ages[v] = age[min_neighbor] + (year_v - year_neighbor)\n",
    "        else:\n",
    "            vertices.append(v)\n",
    "\n",
    "    age_vp = g.new_vertex_property('int')\n",
    "\n",
    "    for v in g.vertices():\n",
    "        age_vp[v] = ages[v]\n",
    "    \n",
    "    return age_vp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Age, Emergence, and Special Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence = label_emergence(pg_full)\n",
    "pg_full.vp['emergence'] = emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full.vp['density_c'] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching, merging = label_special_events(pg_full)\n",
    "pg_full.vp['merging'] = branching\n",
    "pg_full.vp['branching'] = merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = label_ages(pg_full)\n",
    "pg_full.vp['age'] = ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years, density_anual_mean = get_aggregate_vp(pg_full, 'density', 'times', agg=np.mean)\n",
    "year_density_mean_mapping = {k: v for k, v in zip(years, density_anual_mean)}\n",
    "\n",
    "density_normed = pg_full.new_vertex_property('float')\n",
    "\n",
    "for v in pg_full.vertices():\n",
    "    year = pg_full.vp['times'][v]\n",
    "    density = pg_full.vp['density_c'][v]\n",
    "    d_mean = year_density_mean_mapping[year]\n",
    "    density_normed[v] = density / d_mean\n",
    "    \n",
    "pg_full.vp['density_normed'] = density_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Density and Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full_density, pg_full_emergence = get_aggregate_vp(pg_full, 'density_normed', 'emergence', np.median)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(['Ephemeral', 'Emerging', 'Steady', 'Declining'], pg_full_emergence / np.max(pg_full_emergence),\n",
    "        linewidth=3)\n",
    "\n",
    "ax.set_xlabel('Emergence')\n",
    "ax.set_ylabel('Median Field Density (Normalised)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence_map = {0: 'Ephemeral', 1: 'Emerging', 2: 'Steady', 3: 'Declining'}\n",
    "thresh_pgs = []\n",
    "thresh_pg_densities = []\n",
    "thresh_pg_el = []\n",
    "j_threshes = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "for j_thresh in j_threshes:\n",
    "    thresh_pg = filter_phylomemetic_graph(\n",
    "            pg_full,\n",
    "            'Machine Learning', \n",
    "            dictionary_mesh_labels, \n",
    "            min_jaccard=j_thresh\n",
    "        )\n",
    "    emergence = label_emergence(thresh_pg)\n",
    "    thresh_pg.vp['emergence'] = emergence\n",
    "    thresh_pgs.append(thresh_pg)\n",
    "    emergence_label, density = get_aggregate_vp(thresh_pg, 'density_normed', 'emergence', np.median)\n",
    "    thresh_pg_densities.append(density)\n",
    "    thresh_pg_el.append([emergence_map[el] for el in emergence_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for el, d, j in zip(thresh_pg_el[:-1], thresh_pg_densities[:-1], j_threshes):\n",
    "    plt.plot(el, d, linewidth=3, label=r'$\\delta_0: {}$'.format(j))\n",
    "    plt.scatter(el, d, linewidth=3)\n",
    "\n",
    "ax.set_xlabel('Emergence')\n",
    "ax.set_ylabel('Median Field Density (Normalised)')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emergence and Special Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5 = GraphView(pg_full,\n",
    "                 efilt=lambda e: pg_full.ep['jaccard_weights'][e] > 0.45)\n",
    "emergence = label_emergence(pg_5)\n",
    "pg_5.vp['emergence'] = emergence\n",
    "branching, merging = label_special_events(pg_5)\n",
    "pg_5.vp['branching'] = branching\n",
    "pg_5.vp['merging'] = merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {}\n",
    "for k in pg_5.vertex_properties.keys():\n",
    "    if k != 'terms':\n",
    "        props[k] = get_vp_values(pg_5, k)\n",
    "    \n",
    "pg_5_df = pd.DataFrame(props)\n",
    "pg_5_df.drop(columns=['color', 'density', 'density_c'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2006]\n",
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2007]\n",
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5_groupby_year = pg_5_df.groupby('times')\n",
    "years = pg_5_groupby_year.count().index.values\n",
    "\n",
    "merging_frac_year = pg_5_groupby_year.sum()['merging'] / pg_5_groupby_year.count()['merging']\n",
    "branching_frac_year = pg_5_groupby_year.sum()['branching'] / pg_5_groupby_year.count()['branching']\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plt.plot(years, merging_frac_year, label='% Merging', linewidth=3)\n",
    "plt.scatter(years, merging_frac_year, label=None)\n",
    "plt.plot(years, branching_frac_year, label='% Branching', linewidth=3)\n",
    "plt.scatter(years, branching_frac_year, label=None)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence_fractions = defaultdict(list)\n",
    "\n",
    "for i, g in pg_5_groupby_year:\n",
    "    n = len(g)\n",
    "    counts = g['emergence'].value_counts()\n",
    "    for j in range(4):\n",
    "        emergence_fractions[emergence_map[j].lower()].append(counts.loc[j] / n)\n",
    "\n",
    "emergence_fractions_df = pd.DataFrame(emergence_fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 6))\n",
    "\n",
    "for ax, c in zip(axs[0], emergence_fractions_df.columns):\n",
    "    ax.scatter(emergence_fractions_df[c], merging_frac_year)\n",
    "    ax.set_title('% {}'.format(c.title()))\n",
    "for ax, c in zip(axs[1], emergence_fractions_df.columns):\n",
    "    ax.scatter(emergence_fractions_df[c], branching_frac_year, color='#ff7f0e')\n",
    "\n",
    "axs[0][0].set_ylabel('% Merging', fontsize=12)\n",
    "axs[1][0].set_ylabel('% Branching', fontsize=12)\n",
    "for ax_x in axs:\n",
    "    for ax_y in ax_x:\n",
    "        ax_y.tick_params(\n",
    "            axis='x', which='both', bottom=False, top=False, labelbottom=False) \n",
    "        ax_y.tick_params(\n",
    "            axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full_density_age_mean, pg_full_age = get_aggregate_vp(pg_5, 'density_normed', 'age', agg=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pg_full_density_age_mean[:10], pg_full_age[:10], linewidth=3)\n",
    "ax.scatter(pg_full_density_age_mean[:10], pg_full_age[:10])\n",
    "\n",
    "ax.set_xlabel('Age (years)')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surrounding Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfilt = GraphView(\n",
    "        pg_full, \n",
    "        efilt=lambda e: pg_full.ep['jaccard_weights'][e] > 0.8,\n",
    "        vfilt=lambda v: pg_full.vp['times'][v] not in [2006, 2017, 2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dna, y_dna = get_vp_x_y(pg_dna, 'times', 'density', np.median)\n",
    "x_ml, y_ml = get_vp_x_y(pg_ml, 'times', 'density', np.median)\n",
    "x_preg, y_preg = get_vp_x_y(pg_preg, 'times', 'density', np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_dna[1:], y_dna[1:])\n",
    "plt.plot(x_ml[1:], y_ml[1:])\n",
    "plt.plot(x_preg[1:], y_preg[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankey(g):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
