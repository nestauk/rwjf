{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks and Word Vectors with MeSH Labels\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, Range1d\n",
    "from bokeh.palettes import viridis\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.dynamics import PhylomemeticGraph\n",
    "from rhodonite.graphs import SlidingWindowGraph\n",
    "from rhodonite.spectral import association_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.generation import price_network\n",
    "from graph_tool.draw import graph_draw\n",
    "from graph_tool.all import GraphView, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are going to load both the GDB and the RWJF Pioneer and Global projects, and join them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove projects from GitHub as they don't play nicely with MeSH terms, and Crunchbase as they're very short. There are also some projects with null descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the two sets of projects and extract their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)\n",
    "gdb_df.set_index('doc_id', inplace=True)\n",
    "gdb_df = gdb_df.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a MeSH Label Corpus\n",
    "\n",
    "We need to build a corpus of MeSH label transformed documents that is appropriate for the network we want to build. This will require some filtering, however first we should build a vocabulary of all the terms that we have, so that we can reference any of them by a unique ID at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For filtering later, we will calculate the counts of the MeSH labels. We know already that there are some labels which are highly over-represented, and many which occur only once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_filter(docs, high_threshold=None, low_threshold=None, remove=[], counter=None):\n",
    "    \"\"\"freqency_filter\n",
    "    Filters words from a corpus that occur more frequently than high_threshold\n",
    "    and less frequently than low_threshold.\n",
    "    \n",
    "    Args:\n",
    "        docs (:obj:`list` of :obj:`list`): Corupus of tokenised documents.\n",
    "        high_threshold (int): Upper limit for token frequency\n",
    "        low_threshold (int): Lower limit for token frequency\n",
    "        remove (:obj:`list`): List of terms to remove\n",
    "    \n",
    "    Yields:\n",
    "        doc_filtered (:obj:`list`): Document with elements removed based\n",
    "            on frequency\n",
    "    \"\"\"\n",
    "    docs_filtered = []\n",
    "    if counter is None:\n",
    "        counter = Counter(flatten(docs))\n",
    "    for doc in docs:\n",
    "        doc_filtered = []\n",
    "        for t in doc:\n",
    "            if t in remove:\n",
    "                continue\n",
    "            if high_threshold is not None:\n",
    "                if counter[t] > high_threshold:\n",
    "                    continue\n",
    "            if low_threshold is not None:\n",
    "                if counter[t] < low_threshold:\n",
    "                    continue\n",
    "            doc_filtered.append(t)\n",
    "        docs_filtered.append(doc_filtered)\n",
    "    return docs_filtered\n",
    "\n",
    "def filter_description_labels(description_labels, fn):\n",
    "    return [list(filter(fn, dl)) for dl in description_labels]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))\n",
    "mesh_label_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_filtered = frequency_filter(description_mesh_labels, high_threshold=40000,\n",
    "                                                    low_threshold=5,\n",
    "                                                    remove = \n",
    "                                                    ['Students', 'Humans', 'Animals', 'Research','Goals',\n",
    "                                                     'Universities', 'Research Personnel', 'United States', \n",
    "                                                     'United Kingdom', 'Research', 'Awards and Prizes',\n",
    "                                                     'Faculty', 'Mice', 'Mathematics', 'Fellowships and Scholarships',\n",
    "                                                    'Surveys and Questionnaires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases(description_mesh_labels_filtered, min_count=3)\n",
    "bigrammer = Phraser(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_bigrams = [bigrammer[d] for d in description_mesh_labels_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(description_mesh_labels_bigrams)\n",
    "trigrammer = Phraser(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_trigrams = [trigrammer[d] for d in description_mesh_labels_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_final = []\n",
    "for d in description_mesh_labels_trigrams:\n",
    "    corrected_d = []\n",
    "    for t in d:\n",
    "        if len(t.split('_')) > 1:\n",
    "            parts = t.split('_')\n",
    "            corrected_d.append(' '.join(sorted(set(parts))))\n",
    "        else:\n",
    "            corrected_d.append(t)\n",
    "    description_mesh_labels_final.append(corrected_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_mesh_labels = Dictionary(description_mesh_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['coocurrence_labels'] = description_mesh_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df[gdb_df['coocurrence_labels'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Projects by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the most recent 10 years of projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df_co[(gdb_df_co['year'] >= 2006) & (gdb_df_co['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sliding Window Coocurrence Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will want to create a new set of labelled descriptions where the terms with very high counts and little semantic value are removed, and also those that appear very few times in the corpus. We will also need to map the labels to token IDs which can then act as the vertex values in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = range(2006, 2018)\n",
    "co_graphs = [SlidingWindowGraph(gdb_df_co[gdb_df_co['year'] == t]['coocurrence_labels'],\n",
    "                             dictionary=dictionary_mesh_labels, window_size=2)\n",
    "          for t in times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_graphs = [g.prepare() for g in co_graphs]\n",
    "co_graphs = [g.build() for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_strengths = [association_strength(g) for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_period = 0\n",
    "end_period = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PhylomemeticGraph(co_graphs[start_period:end_period], association_strengths[start_period:end_period],\n",
    "                       dictionary_mesh_labels, times[start_period:end_period],\n",
    "                       max_weight=None, min_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time pg = pg.prepare('/Users/grichardson/cfinder/pg_out', '/Users/grichardson/cfinder/CFinder_commandline_mac')\n",
    "%time pg = pg.prepare('/Users/grichardson/cfinder/pg_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cs in pg.clique_sets:\n",
    "    print(Counter([len(c) for c in cs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pg.build(workers=4, min_clique_size=5, delta_0=0.4, parent_limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_antecedents(vertices, limit):\n",
    "    \"\"\"find_antecedents\n",
    "    Find all the antecedents of a particular vertex.\n",
    "    \n",
    "    Args:\n",
    "        vertices (:obj:`iter` of :obj:`Vertex`): A list of vertices\n",
    "            for which the antecedents need to be found.\n",
    "    \n",
    "    Returns:\n",
    "        antes (:obj:`iter` of :obj:`Vertex`): A list of vertices that\n",
    "            are the antecedents of the input vertices.\n",
    "    \"\"\"\n",
    "    l = 0\n",
    "    antes = []\n",
    "    for v in vertices:\n",
    "        if l < limit:\n",
    "            if v.in_degree() > 0:\n",
    "                antes.append(list(v.in_neighbors()))\n",
    "                if len(list(v.in_neighbors())) > 1:\n",
    "                    antes += find_antecedents(v.in_neighbors())\n",
    "                else:\n",
    "                    antes += find_antecedents(list(v.in_neighbors()))\n",
    "                l += 1\n",
    "    return antes\n",
    "\n",
    "def find_descendents(vertices, limit):\n",
    "    \"\"\"find_descendents\n",
    "    Find all the descendents of a particular vertex.\n",
    "    \n",
    "    Args:\n",
    "        vertices (:obj:`iter` of :obj:`Vertex`): A list of vertices\n",
    "            for which the descendents need to be found.\n",
    "    \n",
    "    Returns:\n",
    "        desc (:obj:`iter` of :obj:`Vertex`): A list of vertices that\n",
    "            are the descendents of the input vertices.\n",
    "    \"\"\"\n",
    "    l = 0\n",
    "    desc = []\n",
    "    for v in vertices:\n",
    "        if l < limit:\n",
    "            if v.out_degree() > 0:\n",
    "                desc.append(list(v.out_neighbors()))\n",
    "                if len(list(v.out_neighbors())) > 1:\n",
    "                    desc += find_descendents(v.out_neighbors(), limit - l - 1)\n",
    "                else:\n",
    "                    desc += find_descendents(list(v.out_neighbors()), limit - l - 1)\n",
    "                l += 1\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_thresh = GraphView(pg, efilt=lambda e: pg.ep['jaccard_weights'][e] > 0.5)\n",
    "pg_thresh = GraphView(pg_thresh, vfilt=lambda v: (v.out_degree() > 0) | (v.in_degree() > 0))\n",
    "graph_draw(pg_thresh, vertex_fill_color=pg_thresh.vp['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertex in pg_thresh.vertices():\n",
    "#     if np.random.randint(0, 10) > 5:\n",
    "    if vertex.in_degree() == 1:\n",
    "#             if vertex.out_degree() > 0:\n",
    "        terms_s = pg_thresh.vp['terms'][vertex]\n",
    "        print('-', pg_thresh.vp['times'][vertex], '-')\n",
    "        print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_s])))\n",
    "        print(pg_thresh.vp['density'][vertex])\n",
    "        print('\\n=== Parents ===')\n",
    "\n",
    "        for i, n in enumerate(vertex.in_neighbors()):\n",
    "            terms_n = pg_thresh.vp['terms'][n]\n",
    "            print(pg_thresh.vp['times'][n])\n",
    "            print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "#         print('\\n=== Children ===')\n",
    "\n",
    "#         for i, n in enumerate(n.out_neighbours()):\n",
    "#             terms_n = pg_thresh.vp['terms'][n]\n",
    "#             print(pg_thresh.vp['times'][n])\n",
    "#             print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Computed Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.dynamics import label_emergence, label_special_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full.load(os.path.join(inter_data, 'phylomemetics/pg_2006_2017_cat.xml.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_density(g):\n",
    "    corr = g.new_vertex_property('float')\n",
    "    for v in g.vertices():\n",
    "        card = len(g.vp['terms'][v])\n",
    "        corr[v] = g.vp['density'][v] / card\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = correct_density(pg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_phylomemetic_graph(g, term, dictionary, min_jaccard=0):\n",
    "    \"\"\"filter_phylomemetic_graph\n",
    "    Get a subgraph of vertices that contain a particular term.\n",
    "    \"\"\"\n",
    "    term_id = dictionary.token2id[term]\n",
    "    g_filt = GraphView(\n",
    "        g,\n",
    "        vfilt=lambda v: term_id in g.vp['terms'][v],\n",
    "        efilt=lambda e: g.ep['jaccard_weights'][e] > min_jaccard\n",
    "    )\n",
    "    return g_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_vp(g, vp, vp_grouper, agg=None):\n",
    "    \"\"\"aggregate_property_map\n",
    "    \n",
    "    Args:\n",
    "        g (:obj:`Graph`): A graph.\n",
    "        vp (str): String representing an internal property map\n",
    "            of graph, g.\n",
    "        vp_grouper (str): String representing name of an internal\n",
    "            property map that will be used to group by.\n",
    "        agg (:obj:`function`): Function to aggregate by. For\n",
    "            example, min, max, sum, numpy.mean, etc.\n",
    "    Returns:\n",
    "        (:obj:`iter` of float): Aggregated values from x. \n",
    "    \"\"\"\n",
    "    vp_vals = get_vp_values(g, vp)\n",
    "    vp_agg = get_vp_values(g, vp_grouper)\n",
    "    \n",
    "    sid_x = vp_agg.argsort()\n",
    "    # Get where the sorted version of base changes groups\n",
    "    split_idx = np.flatnonzero(np.diff(vp_agg[sid_x]) > 0) + 1\n",
    "    # OR np.unique(base[sidx],return_index=True)[1][1:]\n",
    "\n",
    "    # Finally sort inp based on the sorted indices and split based on split_idx\n",
    "    vp_vals_grouped = np.split(vp_vals[sid_x], split_idx)\n",
    "    \n",
    "    x = sorted(set(vp_agg))\n",
    "    if agg: \n",
    "        y = [agg(vvg) for vvg in vp_vals_grouped]\n",
    "    else:\n",
    "        y = vp_vals_grouped\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Age, Emergence, and Special Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence = label_emergence(pg_full)\n",
    "pg_full.vp['emergence'] = emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full.vp['density_c'] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching, merging = label_special_events(pg_full)\n",
    "pg_full.vp['merging'] = branching\n",
    "pg_full.vp['branching'] = merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = label_ages(pg_full)\n",
    "pg_full.vp['age'] = ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years, density_anual_mean = get_aggregate_vp(pg_full, 'density', 'times', agg=np.mean)\n",
    "year_density_mean_mapping = {k: v for k, v in zip(years, density_anual_mean)}\n",
    "\n",
    "density_normed = pg_full.new_vertex_property('float')\n",
    "\n",
    "for v in pg_full.vertices():\n",
    "    year = pg_full.vp['times'][v]\n",
    "    density = pg_full.vp['density'][v]\n",
    "    d_mean = year_density_mean_mapping[year]\n",
    "    density_normed[v] = density / d_mean\n",
    "    \n",
    "pg_full.vp['density_normed'] = density_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Density and Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_full_density, pg_full_emergence = get_aggregate_vp(pg_full, 'density_normed', 'emergence', np.median)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(['Ephemeral', 'Emerging', 'Steady', 'Declining'], pg_full_emergence / np.max(pg_full_emergence),\n",
    "        linewidth=3)\n",
    "\n",
    "ax.set_xlabel('Emergence')\n",
    "ax.set_ylabel('Median Field Density (Normalised)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence_map = {0: 'Ephemeral', 1: 'Emerging', 2: 'Steady', 3: 'Declining'}\n",
    "thresh_pgs = []\n",
    "thresh_dfs = []\n",
    "j_threshes = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "for j_thresh in j_threshes:\n",
    "    thresh_pg = GraphView(\n",
    "        pg_full,\n",
    "        efilt=lambda e: pg_full.ep['jaccard_weights'][e] > j_thresh\n",
    "    )\n",
    "    emergence = label_emergence(thresh_pg)\n",
    "    branching, merging = label_special_events(thresh_pg)\n",
    "    ages = label_ages(thresh_pg)\n",
    "    df = pd.DataFrame({\n",
    "                 'branching': branching.get_array(),\n",
    "                 'merging': merging.get_array(),\n",
    "                 'emergence': emergence.get_array(),\n",
    "                 'age': ages.get_array()}\n",
    "                )\n",
    "    df['emergence'].map(emergence_map)\n",
    "    df.columns = [c + '_{}'.format(j_thresh) for c in df.columns]\n",
    "    thresh_dfs.append(df)\n",
    "    thresh_pgs.append(thresh_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_df = pd.concat(thresh_dfs, axis=1)\n",
    "years = get_vp_values(thresh_pg, 'times')\n",
    "density = get_vp_values(thresh_pg, 'density_normed')\n",
    "pg_df['year'] = years\n",
    "pg_df['density'] = density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in pg_df.columns:\n",
    "    if 'emergence' in c:\n",
    "        pg_df[c] = pg_df[c].map(emergence_map)\n",
    "    if ('density_' in c) | ('year_' in c):\n",
    "        pg_df.drop(c, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for j in j_threshes[:-1]:\n",
    "    cols = [c for c in pg_df.columns if str(j) in c]\n",
    "    cols.append('density')\n",
    "    df_temp = pg_df[cols]\n",
    "    group = df_temp.groupby('emergence_{}'.format(j)).mean()\n",
    "    group = group.loc[['Ephemeral', 'Emerging', 'Steady', 'Declining']]\n",
    "    plt.plot(group.index.values, group['density'], linewidth=3, label=r'$\\delta_0: {}$'.format(j))\n",
    "    plt.scatter(group.index.values, group['density'], linewidth=3, label=None)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emergence and Special Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5 = GraphView(pg_full,\n",
    "                 efilt=lambda e: pg_full.ep['jaccard_weights'][e] > 0.45)\n",
    "emergence = label_emergence(pg_5)\n",
    "pg_5.vp['emergence'] = emergence\n",
    "branching, merging = label_special_events(pg_5)\n",
    "pg_5.vp['branching'] = branching\n",
    "pg_5.vp['merging'] = merging\n",
    "ages = label_ages(pg_5)\n",
    "pg_5.vp['age'] = ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {}\n",
    "for k in pg_5.vertex_properties.keys():\n",
    "    if k != 'terms':\n",
    "        props[k] = get_vp_values(pg_5, k)\n",
    "    \n",
    "pg_5_df = pd.DataFrame(props)\n",
    "pg_5_df.drop(columns=['color', 'density', 'density_normed'], inplace=True)\n",
    "pg_5_df['terms'] = [pg_5.vp['terms'][v] for v in pg_5.vertices()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2006]\n",
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2007]\n",
    "pg_5_df = pg_5_df[pg_5_df['times'] != 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5_groupby_year = pg_5_df.groupby('times')\n",
    "years = pg_5_groupby_year.count().index.values\n",
    "\n",
    "merging_frac_year = pg_5_groupby_year.sum()['merging'] / pg_5_groupby_year.count()['merging'] * 100\n",
    "branching_frac_year = pg_5_groupby_year.sum()['branching'] / pg_5_groupby_year.count()['branching']  * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7, 5))\n",
    "ax.plot(years, merging_frac_year, label='% Merging', linewidth=3)\n",
    "ax.scatter(years, merging_frac_year, label=None)\n",
    "ax.plot(years, branching_frac_year, label='% Branching', linewidth=3)\n",
    "ax.scatter(years, branching_frac_year, label=None)\n",
    "ax.set_ylabel('% of Fields')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times = []\n",
    "# dens = []\n",
    "# terms = []\n",
    "# branching = []\n",
    "# mergning = []\n",
    "# vertex = []\n",
    "# for v in pg_5.vertices():\n",
    "#     if (pg_5.vp['emergence'][v] == 1) | (pg_5.vp['emergence'][v] == 2) | (pg_5.vp['emergence'][v] == 3):\n",
    "#         terms.append(pg_5.vp['terms'][v])\n",
    "#         dens.append(pg_5.vp['density_normed'][v])\n",
    "#         times.append(pg_5.vp['times'][v])\n",
    "#         branching.append(pg_5.vp['branching'][v])\n",
    "#         branching.append(pg_5.vp['merging'][v])\n",
    "#         vertex.append(int(v))\n",
    "# df = pd.DataFrame({'year': times, 'density': dens, 'terms': terms, 'vertex': vertex,\n",
    "#                    'merging': merging, 'branching': branching})\n",
    "# df = df[df['year'] < 2017]\n",
    "\n",
    "query_terms = ['Software']\n",
    "term_id = [dictionary_mesh_labels.token2id[t] for t in query_terms]\n",
    "\n",
    "df_terms = pg_5_df[pg_5_df['terms'].apply(lambda x: True if all([t in x for t in term_id]) else False)]\n",
    "\n",
    "term_groupby_year = df_terms.groupby('times')\n",
    "years = term_groupby_year.count().index.values\n",
    "\n",
    "merging_frac_year = term_groupby_year.sum()['merging'] / term_groupby_year.count()['merging'] * 100\n",
    "branching_frac_year = term_groupby_year.sum()['branching'] / term_groupby_year.count()['branching']  * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7, 5))\n",
    "ax.plot(years, merging_frac_year, label='% Merging', linewidth=3)\n",
    "ax.scatter(years, merging_frac_year, label=None)\n",
    "ax.plot(years, branching_frac_year, label='% Branching', linewidth=3)\n",
    "ax.scatter(years, branching_frac_year, label=None)\n",
    "ax.set_ylabel('% of Fields')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergence_fractions = defaultdict(list)\n",
    "\n",
    "for i, g in pg_5_groupby_year:\n",
    "    n = len(g)\n",
    "    counts = g['emergence'].value_counts()\n",
    "    for j in range(4):\n",
    "        try:\n",
    "            emergence_fractions[emergence_map[j].lower()].append(counts.loc[j] / n)\n",
    "        except:\n",
    "            emergence_fractions[emergence_map[j].lower()].append(np.nan)\n",
    "\n",
    "emergence_fractions_df = pd.DataFrame(emergence_fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 6))\n",
    "\n",
    "for ax, c in zip(axs[0], emergence_fractions_df.columns):\n",
    "    ax.scatter(emergence_fractions_df[c] * 100, merging_frac_year * 100)\n",
    "    ax.set_title('% {}'.format(c.title()))\n",
    "for ax, c in zip(axs[1], emergence_fractions_df.columns):\n",
    "    ax.scatter(emergence_fractions_df[c] * 100, branching_frac_year * 100, color='#ff7f0e')\n",
    "\n",
    "axs[0][0].set_ylabel('% Merging', fontsize=12)\n",
    "axs[1][0].set_ylabel('% Branching', fontsize=12)\n",
    "for ax_x in axs:\n",
    "    for ax_y in ax_x:\n",
    "        ax_y.tick_params(\n",
    "            axis='x', which='both', bottom=False, top=False, labelbottom=False) \n",
    "        ax_y.tick_params(\n",
    "            axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_5_density_age_mean, pg_5_age = get_aggregate_vp(pg_5, 'density_normed', 'age', agg=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pg_5_density_age_mean[:8], pg_5_age[:8], linewidth=3)\n",
    "ax.scatter(pg_5_density_age_mean[:8], pg_5_age[:8])\n",
    "\n",
    "ax.set_xlabel('Branch Age (years)')\n",
    "ax.set_ylabel('Median Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surrounding Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density and Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_density_vs_time(g, dictionary, term_sets=[]):\n",
    "    term_df = []\n",
    "    for terms in term_sets:\n",
    "        dens = []\n",
    "        times = []\n",
    "        term_ids = [dictionary.token2id[t] for t in terms]\n",
    "        for v in g.vertices():\n",
    "            terms_v = g.vp['terms'][v]\n",
    "            if set(term_ids).issubset(terms_v):\n",
    "                dens.append(g.vp['density_normed'][v])\n",
    "                times.append(g.vp['times'][v])\n",
    "        df = pd.DataFrame({' '.join(terms): dens, 'year': times})\n",
    "        term_df.append(df.groupby('year').mean())\n",
    "    df = pd.concat(term_df, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pregnancy_df = get_term_density_vs_time(\n",
    "    pg_5,\n",
    "    dictionary_mesh_labels,\n",
    "    [['Machine Learning'],\n",
    "     ['Algorithms'],\n",
    "     ['Pregnancy'],\n",
    "     ['Neoplasms'],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.plot(female_pregnancy_df.loc[2007:2016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Network Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=dictionary_mesh_labels.keys(), sparse_output=True)\n",
    "pca = PCA(n_components=1)\n",
    "svd = TruncatedSVD(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "dens = []\n",
    "terms = []\n",
    "vertex = []\n",
    "for v in pg_5.vertices():\n",
    "    if (pg_5.vp['emergence'][v] == 1) | (pg_5.vp['emergence'][v] == 2) | (pg_5.vp['emergence'][v] == 3):\n",
    "        terms.append(pg_5.vp['terms'][v])\n",
    "        dens.append(pg_5.vp['density_normed'][v])\n",
    "        times.append(pg_5.vp['times'][v])\n",
    "        vertex.append(int(v))\n",
    "df = pd.DataFrame({'year': times, 'density': dens, 'terms': terms, 'vertex': vertex})\n",
    "df = df[df['year'] < 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.fit(df['terms'])\n",
    "svd.fit(mlb.transform(df['terms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = mlb.transform(df['terms'])\n",
    "svd_vals = svd.transform(matrix)\n",
    "\n",
    "df['y_pos'] = svd_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['density_log'] = np.log10(df['density'])\n",
    "df.set_index('vertex', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [(np.int(e.source()), np.int(e.target())) for e in pg_5.edges()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in dictionary_mesh_labels.token2id.keys() if 'Robot' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms = ['Machine Learning', 'Brain']\n",
    "term_id = [dictionary_mesh_labels.token2id[t] for t in query_terms]\n",
    "\n",
    "df_term = df[df['terms'].apply(lambda x: True if all([t in x for t in term_id]) else False)]\n",
    "\n",
    "edges_term = [e for e in edges if (e[0] in df_term.index.values) & (e[1] in df_term.index.values)]\n",
    "\n",
    "edges_x0 = [df.loc[e[0]]['year'].astype(np.int32) for e in edges_term]\n",
    "edges_x1 = [df.loc[e[1]]['year'].astype(np.int32) for e in edges_term]\n",
    "edges_y0 = [df.loc[e[0]]['y_pos'].astype(np.float32) for e in edges_term]\n",
    "edges_y1 = [df.loc[e[1]]['y_pos'].astype(np.float32) for e in edges_term]\n",
    "\n",
    "\n",
    "terms = [', '.join([dictionary_mesh_labels[t] for t in tokens]) for tokens in df_term['terms'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = ColumnDataSource({'year': df_term['year'].astype(np.int32),\n",
    "                        'y_pos': df_term['y_pos'].astype(np.float32),\n",
    "                        'terms': terms,\n",
    "                        'density': df_term['density_log'].astype(np.float32)})\n",
    "cds_edges = ColumnDataSource({'x0': np.array(edges_x0).astype(np.float32),\n",
    "                              'x1': np.array(edges_x1).astype(np.float32),\n",
    "                              'y0': np.array(edges_y0).astype(np.float32),\n",
    "                              'y1': np.array(edges_y1).astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(width=900, height=400)\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        ('Terms', '@terms'),\n",
    "    ],\n",
    "names=[\"vertices\"])\n",
    "\n",
    "color_mapper = LinearColorMapper(\n",
    "    palette='Magma256', low=min(df_term['density_log']), high=max(df_term['density_log']))\n",
    "\n",
    "p.segment(source=cds_edges, x0='x0', y0='y0', x1='x1', y1='y1', alpha=0.2, line_width=2, color='gray', name='edges')\n",
    "p.circle(source=cds, x='year', y='y_pos', size=7, fill_alpha=0.6, line_alpha=0, name='vertices',\n",
    "        color={'field': 'density', 'transform': color_mapper})\n",
    "\n",
    "p.add_tools(hover)\n",
    "p.x_range = Range1d(2006, 2017)\n",
    "p.xaxis.ticker = list(range(2007, 2017))\n",
    "p.yaxis.ticker = []\n",
    "p.xaxis.major_label_text_font_size = \"12pt\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.topology import label_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = label_components(pg_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "ax.plot(comps[0].get_array()[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plt.scatter( df_term['year'], df_term['y_pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association_strength_vs_time(term_0, term_1):\n",
    "    a_s = []\n",
    "    co_s = []\n",
    "    for a, c in zip(association_strengths, co_graphs):\n",
    "        if (term_0 in c.token2vertex) & (term_1 in c.token2vertex):\n",
    "            t_0 = c.token2vertex[term_0]\n",
    "            t_1 = c.token2vertex[term_1]\n",
    "            edge = tuple(sorted([t_0, t_1]))\n",
    "            try:\n",
    "                a_s.append(a[edge] / np.mean(a.get_array()))\n",
    "                co_s.append(c.ep.cooccurrences[edge])\n",
    "            except:\n",
    "                a_s.append(0)\n",
    "                co_s.append(0)\n",
    "        else:\n",
    "            a_s.append(0)\n",
    "            co_s.append(0)\n",
    "    return a_s, co_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s, co_s = get_association_strength_vs_time('Machine Learning', 'Bacteria')\n",
    "plt.plot(a_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "\n",
    "for v in pg_5.vertices():\n",
    "    desc = find_descendents([v], 2)\n",
    "    preds[v] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfilt = GraphView(\n",
    "        pg_full, \n",
    "        efilt=lambda e: pg_full.ep['jaccard_weights'][e] > 0.8,\n",
    "        vfilt=lambda v: pg_full.vp['times'][v] not in [2006, 2017, 2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dna, y_dna = get_vp_x_y(pg_dna, 'times', 'density', np.median)\n",
    "x_ml, y_ml = get_vp_x_y(pg_ml, 'times', 'density', np.median)\n",
    "x_preg, y_preg = get_vp_x_y(pg_preg, 'times', 'density', np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_dna[1:], y_dna[1:])\n",
    "plt.plot(x_ml[1:], y_ml[1:])\n",
    "plt.plot(x_preg[1:], y_preg[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankey(g):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
