{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks and Word Vectors with MeSH Labels\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.dynamics import PhylomemeticGraph\n",
    "from rhodonite.graphs import SlidingWindowGraph\n",
    "from rhodonite.spectral import association_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.generation import price_network\n",
    "from graph_tool.draw import graph_draw\n",
    "from graph_tool.all import GraphView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are going to load both the GDB and the RWJF Pioneer and Global projects, and join them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove projects from GitHub as they don't play nicely with MeSH terms, and Crunchbase as they're very short. There are also some projects with null descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the two sets of projects and extract their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)\n",
    "gdb_df.set_index('doc_id', inplace=True)\n",
    "gdb_df = gdb_df.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a MeSH Label Corpus\n",
    "\n",
    "We need to build a corpus of MeSH label transformed documents that is appropriate for the network we want to build. This will require some filtering, however first we should build a vocabulary of all the terms that we have, so that we can reference any of them by a unique ID at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For filtering later, we will calculate the counts of the MeSH labels. We know already that there are some labels which are highly over-represented, and many which occur only once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_filter(docs, high_threshold=None, low_threshold=None, remove=[], counter=None):\n",
    "    \"\"\"freqency_filter\n",
    "    Filters words from a corpus that occur more frequently than high_threshold\n",
    "    and less frequently than low_threshold.\n",
    "    \n",
    "    Args:\n",
    "        docs (:obj:`list` of :obj:`list`): Corupus of tokenised documents.\n",
    "        high_threshold (int): Upper limit for token frequency\n",
    "        low_threshold (int): Lower limit for token frequency\n",
    "        remove (:obj:`list`): List of terms to remove\n",
    "    \n",
    "    Yields:\n",
    "        doc_filtered (:obj:`list`): Document with elements removed based\n",
    "            on frequency\n",
    "    \"\"\"\n",
    "    docs_filtered = []\n",
    "    if counter is None:\n",
    "        counter = Counter(flatten(docs))\n",
    "    for doc in docs:\n",
    "        doc_filtered = []\n",
    "        for t in doc:\n",
    "            if t in remove:\n",
    "                continue\n",
    "            if high_threshold is not None:\n",
    "                if counter[t] > high_threshold:\n",
    "                    continue\n",
    "            if low_threshold is not None:\n",
    "                if counter[t] < low_threshold:\n",
    "                    continue\n",
    "            doc_filtered.append(t)\n",
    "        docs_filtered.append(doc_filtered)\n",
    "    return docs_filtered\n",
    "\n",
    "def filter_description_labels(description_labels, fn):\n",
    "    return [list(filter(fn, dl)) for dl in description_labels]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))\n",
    "mesh_label_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_filtered = frequency_filter(description_mesh_labels, high_threshold=40000,\n",
    "                                                    low_threshold=5,\n",
    "                                                    remove = \n",
    "                                                    ['Students', 'Humans', 'Animals', 'Research','Goals',\n",
    "                                                     'Universities', 'Research Personnel', 'United States', \n",
    "                                                     'United Kingdom', 'Research', 'Awards and Prizes',\n",
    "                                                     'Faculty', 'Mice', 'Mathematics', 'Fellowships and Scholarships',\n",
    "                                                    'Surveys and Questionnaires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases(description_mesh_labels_filtered, min_count=3)\n",
    "bigrammer = Phraser(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_bigrams = [bigrammer[d] for d in description_mesh_labels_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(description_mesh_labels_bigrams)\n",
    "trigrammer = Phraser(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_trigrams = [trigrammer[d] for d in description_mesh_labels_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_final = []\n",
    "for d in description_mesh_labels_trigrams:\n",
    "    corrected_d = []\n",
    "    for t in d:\n",
    "        if len(t.split('_')) > 1:\n",
    "            parts = t.split('_')\n",
    "            corrected_d.append(' '.join(sorted(set(parts))))\n",
    "        else:\n",
    "            corrected_d.append(t)\n",
    "    description_mesh_labels_final.append(corrected_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_mesh_labels = Dictionary(description_mesh_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['coocurrence_labels'] = description_mesh_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df[gdb_df['coocurrence_labels'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Projects by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the most recent 10 years of projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co = gdb_df_co[(gdb_df_co['year'] >= 2006) & (gdb_df_co['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df_co['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sliding Window Coocurrence Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will want to create a new set of labelled descriptions where the terms with very high counts and little semantic value are removed, and also those that appear very few times in the corpus. We will also need to map the labels to token IDs which can then act as the vertex values in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = range(2006, 2018)\n",
    "co_graphs = [SlidingWindowGraph(gdb_df_co[gdb_df_co['year'] == t]['coocurrence_labels'],\n",
    "                             dictionary=dictionary_mesh_labels, window_size=2)\n",
    "          for t in times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_graphs = [g.prepare() for g in co_graphs]\n",
    "co_graphs = [g.build() for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_strengths = [association_strength(g) for g in co_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_period = 0\n",
    "end_period = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PhylomemeticGraph(co_graphs[start_period:end_period], association_strengths[start_period:end_period],\n",
    "                       dictionary_mesh_labels, times[start_period:end_period],\n",
    "                       max_weight=None, min_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pg = pg.prepare('/Users/grichardson/cfinder/pg_out', '/Users/grichardson/cfinder/CFinder_commandline_mac')\n",
    "# %time pg = pg.prepare('/Users/grichardson/cfinder/pg_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.delta_0 = 0.5\n",
    "pg.delta_1 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cs in pg.clique_sets:\n",
    "    print(Counter([len(c) for c in cs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pg.build(workers=4, min_clique_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_thresh = GraphView(pg, vfilt=lambda v: (v.out_degree() > 0) | (v.in_degree() > 0))\n",
    "graph_draw(pg_thresh, vertex_fill_color=pg_thresh.vp['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertex in pg_thresh.vertices():\n",
    "    if np.random.randint(0, 10) > 5:\n",
    "        if vertex.in_degree() > 0:\n",
    "#             if vertex.out_degree() > 0:\n",
    "            terms_s = pg_thresh.vp['terms'][vertex]\n",
    "            print(pg_thresh.vp['times'][vertex], '-',\n",
    "                  ' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_s])))\n",
    "            print('\\n=== Parents ===')\n",
    "\n",
    "            for i, n in enumerate(vertex.in_neighbors()):\n",
    "                terms_n = pg_thresh.vp['terms'][n]\n",
    "                print(pg_thresh.vp['times'][n])\n",
    "                print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "            print('\\n=== Children ===')\n",
    "\n",
    "            for i, n in enumerate(n.out_neighbours()):\n",
    "                terms_n = pg_thresh.vp['terms'][n]\n",
    "                print(pg_thresh.vp['times'][n])\n",
    "                print(' + '.join(sorted([dictionary_mesh_labels[t] for t in terms_n])))\n",
    "\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
