{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Innovation Analysis Using MeSH Terms as Document Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#NB Open a standard set of directories\n",
    "sep = os.sep\n",
    "#Paths\n",
    "#Get the top path\n",
    "top_path = os.path.dirname(os.getcwd())\n",
    "#Create the path for external data\n",
    "ext_data = os.path.join(top_path,'data{s}external{s}'.format(s=sep))\n",
    "#Raw path (for html downloads)\n",
    "raw_data = os.path.join(top_path,'data{s}raw{s}'.format(s=sep))\n",
    "#And external data\n",
    "proc_data = os.path.join(top_path,'data{s}processed{s}'.format(s=sep))\n",
    "#And path for figures\n",
    "fig_path = os.path.join(top_path,'reports{s}figures{s}'.format(s=sep))\n",
    "\n",
    "#Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "today_str = \"_\".join([str(x) for x in [today.month, today.day, today.year]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_to_list(df, c):\n",
    "    return [ast.literal_eval(i) for i in df[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Using the NLM Batch MTI Tool\n",
    "\n",
    "As part of their Indexing Initiative, NIH and NLM have developed an algorithm that will suggest MeSh terms as labels for a document. A public tool that makes use of their technology is the [MeSH on Demand](https://meshb.nlm.nih.gov/MeSHonDemand), which can process a single text at a time. However, with an account, users can access a series of [batch processing tools](https://ii.nlm.nih.gov/Batch/index.shtml), which can perform the same kind of labelling on a corpus of texts, along with a greater degree of control over parameters such as term filtering and outputs.\n",
    "\n",
    "The tools require uploaded texts to be formated in one of a set of specific styles, which can be found [here](https://ii.nlm.nih.gov/Help/index.shtml). Here we import the descriptions that we want to be labelled from the _Grant Database_, and write them to a text file in the \"Single Line Delimited Input w/ ID\" format, with ASCII characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(raw_data + 'health_cb_gh_gdb.csv').drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = gdb_df['Description']\n",
    "\n",
    "index_pad_length = len(str(max(descriptions.index)))\n",
    "\n",
    "with open(proc_data + 'gdb_descriptions_for_mti.txt', 'w') as f:\n",
    "    for i, t in zip(descriptions.index, descriptions.values):\n",
    "        if pd.isnull(t):\n",
    "            t = 'None'\n",
    "        f.write('{}|{}\\n'.format(str(i).rjust(index_pad_length, '0'),\n",
    "                                 t.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text file was uploaded to the [Generic Batch with Validation](https://ii.nlm.nih.gov/Batch/UTS_Required/genericWithValidation.shtml) tool using \"Single Line Delimited Input w/ ID\" as the only _Batch Specific_ option, and the execution arguments (recommended by an NLM employee):\n",
    "\n",
    "```\n",
    "MTI.Linux -MoD_PP -trackPositional -E\n",
    "```\n",
    "\n",
    "Upon completion of the job (which was run overnight), a link to the results are emailed to the user. From here we can download the file `text.out`, which contains the results for every document in the text uploaded. For each document, the results take the following form:\n",
    "\n",
    "```\n",
    "000000|PRC|17768168;17211657;24593272;25377753;22960242;17691196;8705186;20643683;25030295;9682210|\n",
    "000000|Protons|D011522|C0033727|52674|1195^7^0\n",
    "000000|Radioactive Waste|D011850|C0034552|46935|1413^17^0\n",
    "000000|Universities|D014495|C0041740|27039|1830^12^0\n",
    "000000|Diamond|D018130|C0057717|9522|1022^7^0\n",
    "000000|Nuclear Energy|D009678|C0028572|29955|1354^14^0\n",
    "000000|Neutrons|D009502|C0027946|22098|953^7^0\n",
    "000000|Ions|D007477|C0022023|20340|1213^4^0\n",
    "000000|Physics|D010825|C0031837|7722|531^7^0;251^7^0\n",
    "000000|Students|D013334|C0038492|2466|1675^17^0;1684^8^0\n",
    "000000|Neoplasms|D009369|C0027651|999|1227^6^0\n",
    "000000|United Kingdom|D006113|C0041700|1000|86^6^0\n",
    "000000|Cell Proliferation|D049109|C0596290|1000|1321^13^0\n",
    "```\n",
    "\n",
    "The number on the left is the ID we assigned to each document in the input file, and the first line of each document's results contains the PubMed unique IDs for the top 10 articles which are similar to the one uploaded. More information on interpreting outputs from the MTI tools can be found [here](https://ii.nlm.nih.gov/resource/MTI_output_help_info.html). The following rows contain the MeSH terms that were matched from the text. These are found using a combination of keyword matching, fuzzy matching, and similarity of the document to other documents in the PubMed archive. Each row contains the MeSH term, its CUI, a score, and the position of the word of phrase in the original document that gave rise to the label. This positional information consists of three components start position (zero offset), length, and a flag if the text is split (e.g. \"community care\" in \"community of care\"). Behind the scenes, the text input is reformatted, and as a result, the start and end indicies are off by 13 plus the length of your input index values.\n",
    "\n",
    "For reference, here is line from the original text input file that gave the results above:\n",
    "\n",
    "_000000|The past decade has seen a renaissance in accelerator R&amp;D in the UK, building upon the existing expertise at the Rutherford Appleton and Daresbury Laboratories and fuelled by the need to prepare for the generation of particle physics facilities after the LHC. Both the University of Oxford and Royal Holloway University of London have made very significant contributions to this renewed programme. In 2004, the John Adams Institute for Accelerator Science was created, jointly hosted by the Departments of Physics of the University of Oxford and Royal Holloway University of London, with support from PPARC and CCLRC, now merged into STFC. The initial programme was focussed on R&amp;D for the Linear Collider and the Neutrino Factory, but has broadened considerably since its inception to include developments of advanced and novel light sources, work on the upgrades for the Large Hadron Collider at CERN, the ISIS spallation neutron source at the Rutherford Appleton Laboratory, and of the new Diamond Light Source on the Harwell Science and Innovation Campus, and the development of novel accelerators for a variety of applications from medicine (for example, using protons and light ions to treat cancer therapy) to energy (where accelerator-driven sub-critical reactors could contribute to proliferation-safe generation of nuclear energy and help reduce the volume of highly active radioactive waste). A key part of the strategy is the training of a new generation of accelerator scientists able to design, build and operate the new facilities that would be required in the future and we have made excellent progress on this, with more than 20 graduate students and 15 PDRAs being trained by the JAI alone. We propose to continue this programme, as part of a broadly-based collaboration between the universities, the accelerator science institutes (John Adams Institute, the Cockcroft Institute and the STFC Accelerator Science and Technology Centre), and the national laboratories. A key objective is to encourage the development of a domestic industry able to support this work._\n",
    "\n",
    "## Importing Batch Processed MeSH Labels\n",
    "\n",
    "After downloading and renameing our `text.out` results file, we can import it and match it up to the original dataset. The following function is slightly lengthy, but it essentially parses the results and aggregates all of the labels associated with each document. It also sorts them in the original order that they were found in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_data + 'gdb_mesh_on_demand_labels.out', 'r') as f:\n",
    "    mesh_on_demand_labels = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mti_batch_output(batch_output, descriptions, log_every=10000):\n",
    "    mti_output_dict = defaultdict(dict)\n",
    "    mti_output_dict['pub_med_uids'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_duis'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_cuis'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_labels'] = defaultdict(list)\n",
    "    mti_output_dict['scores'] = defaultdict(list)\n",
    "    mti_output_dict['original_phrases'] = defaultdict(list)\n",
    "    mti_output_dict['term_counts'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_term_token_start_idx'] = defaultdict(list)\n",
    "    mti_output_dict['mesh_term_token_end_idx'] = defaultdict(list)\n",
    "    \n",
    "    doc_vocab_to_mesh_map = defaultdict(list)\n",
    "    \n",
    "    batch_output_list = batch_output.copy()\n",
    "    \n",
    "    # loop through documents and get only the mesh label outputs for that doc\n",
    "    for doc_id in range(len(descriptions)):\n",
    "        \n",
    "        doc_results = []\n",
    "        \n",
    "        original_phrases = []\n",
    "        mesh_labels = []\n",
    "        cui_codes = []\n",
    "        starts = []\n",
    "        ends = []\n",
    "        term_counts = []\n",
    "        \n",
    "        if not doc_id % log_every:\n",
    "            print(\"Processed:\", doc_id)\n",
    "\n",
    "        for row_id, result_row in enumerate(batch_output_list):\n",
    "            row_doc_id = int(result_row.split('|')[0]) \n",
    "            if row_doc_id == doc_id:\n",
    "                doc_results.append(result_row)\n",
    "            else:\n",
    "                break\n",
    "            cutoff = row_id + 1\n",
    "        # drop the labels for the current document from the original labels list\n",
    "        # about 60x faster than re-indexing\n",
    "        del batch_output_list[:cutoff]\n",
    "        \n",
    "        # cycle through labels in doc and add them to dicts\n",
    "        for doc_row in doc_results:\n",
    "            elements = doc_row.split('|')\n",
    "            \n",
    "            index_padding = len(elements[0])\n",
    "            i = int(elements[0])\n",
    "            \n",
    "            # deal with the first line that only contains PubMed IDs for the doc\n",
    "            if elements[1] == 'PRC':\n",
    "                pub_med_uids = elements[-2].split(';')\n",
    "                mti_output_dict['pub_med_uids'][i] = pub_med_uids\n",
    "            # deal with the actual MeSH labels\n",
    "            else:\n",
    "                ml = elements[1]\n",
    "                mti_output_dict['mesh_labels'][i].append(ml)\n",
    "                mti_output_dict['mesh_duis'][i].append(elements[2])\n",
    "                mti_output_dict['mesh_cuis'][i].append(elements[3])\n",
    "                mti_output_dict['scores'][i].append(elements[4])\n",
    "            \n",
    "                positions = elements[-1].split('^')\n",
    "\n",
    "                if len(positions) > 3:\n",
    "                    positions = flatten([p.split(';') for p in positions])\n",
    "                positions = list(grouper(3, positions))\n",
    "\n",
    "#                 starts.append([int(p[0]) for p in positions])\n",
    "                term_counts.append(len(positions))\n",
    "                mesh_labels.append([ml])\n",
    "                \n",
    "                original_phrases_row = []\n",
    "                for position in positions:\n",
    "                    if isinstance(position, str):\n",
    "                        continue\n",
    "                    elif None in position:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 21 for gdb\n",
    "                        # 15 for pioneers and globals\n",
    "                        start = int(position[0]) - (21 - index_padding) - len(elements[0])\n",
    "                        starts.append(start)\n",
    "                        end = start + int(position[1])\n",
    "                        ends.append(end)\n",
    "                        original_phrases_row.append(descriptions[i][start:end])\n",
    "                \n",
    "                # map phrases from text to their MeSH labels in separate dict\n",
    "                for opr in original_phrases_row:\n",
    "                    original_phrases.append(opr)\n",
    "                    if ml not in doc_vocab_to_mesh_map[opr]:\n",
    "                        doc_vocab_to_mesh_map[opr].append(ml)\n",
    "\n",
    "        mti_output_dict['original_phrases'][i] = original_phrases\n",
    "        \n",
    "        # get the phrases from the texts in their original order\n",
    "#         starts = flatten(starts)\n",
    "        original_phrases_ordered = [op for (s, op) in sorted(zip(starts, original_phrases), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['original_phrases'][i] = original_phrases_ordered\n",
    "        # and the mesh labels\n",
    "        mesh_labels_ordered = flatten([[ml] * tc for ml, tc in zip(mesh_labels, term_counts)])\n",
    "        mesh_labels_ordered = flatten([ml for (s, ml) in sorted(zip(starts, mesh_labels_ordered), key=lambda pair: pair[0])])\n",
    "        mti_output_dict['mesh_labels'][i] = mesh_labels_ordered\n",
    "        \n",
    "        cui_ordered = flatten([[cui] * tc for cui, tc in zip(mti_output_dict['mesh_cuis'][i], term_counts)])\n",
    "        cui_ordered = [cui for (s, cui) in sorted(zip(starts, cui_ordered), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['mesh_cuis'][i] = cui_ordered\n",
    "        \n",
    "        dui_ordered = flatten([[dui] * tc for dui, tc in zip(mti_output_dict['mesh_duis'][i], term_counts)])\n",
    "        dui_ordered = [dui for (s, dui) in sorted(zip(starts, dui_ordered), key=lambda pair: pair[0])]\n",
    "        mti_output_dict['mesh_duis'][i] = dui_ordered\n",
    "        \n",
    "        mti_output_dict['mesh_term_token_start_idx'][i] = sorted(starts)\n",
    "        mti_output_dict['mesh_term_token_end_idx'][i] = sorted(ends)\n",
    "        mti_output_dict['term_counts'][i] = term_counts\n",
    "\n",
    "    return mti_output_dict, doc_vocab_to_mesh_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a little while to run\n",
    "mti_output_dict, vocab_mesh_mapping = parse_mti_batch_output(mesh_on_demand_labels, descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step here is to merge the data with the original GDB and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['doc_id'] = gdb_df.index.values\n",
    "for c in ['original_phrases', 'mesh_labels', 'pub_med_uids', 'mesh_labels', 'mesh_cuis', 'mesh_duis', 'mesh_term_token_start_idx', 'mesh_term_token_end_idx']:\n",
    "    gdb_df[c] = gdb_df['doc_id'].map(mti_output_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df.to_csv(proc_data + 'gdb_{}.csv'.format(today_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pioneer and Global Projects\n",
    "\n",
    "We also need to repeat this process for the RWJF Pioneer and Global projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_meta(project):\n",
    "    '''\n",
    "    This function takes a project and returns the name and the id (if they are available, this is not always the case)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if 'ID'  in project:\n",
    "        #Split on the ID string to get the name\n",
    "        name = project.split('ID')[0].strip()\n",
    "        \n",
    "        #Split on the ID string again to get what we want\n",
    "        grant_id = re.sub('[#:]','',project.split('ID')[1].split('\\n')[0].strip()).strip()\n",
    "\n",
    "    \n",
    "    else:\n",
    "        #If there is no ID we split on line breaks\n",
    "        name = project.strip().split('\\n')[0].strip()\n",
    "        grant_id = np.nan\n",
    "     \n",
    "    #description = project.split('\\n*')[1]\n",
    "    return([name,grant_id])\n",
    "\n",
    "#def clean_up_text(project):\n",
    "    \n",
    "\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Turns a nested list into a flat list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    flat = [x for el in my_list for x in el]\n",
    "    \n",
    "    return(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rwjf_data(file):\n",
    "    '''\n",
    "    This function reads project lists from the RWJF and tidies it up, and returns\n",
    "    a list where each element has the project name, grant id and description\n",
    "    \n",
    "    '''\n",
    "    #Load the data\n",
    "    with open(ext_data+'/'+file, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    \n",
    "    #Split it based on the project separator and leave out the links at the top\n",
    "    projects = data.split('\\n________________\\n')[1:]\n",
    "    \n",
    "    #Extract metadata\n",
    "    project_meta = [get_project_meta(x) for x in projects]\n",
    "    \n",
    "    #project_descriptions = [x[2] for x in project_meta]\n",
    "    \n",
    "    #Clean up the project info\n",
    "    projects_clean = [re.sub('\\* ','',re.sub('\\n','',project)).lower() for project in projects]\n",
    "    \n",
    "    return([[x,y,z] for x,y,z in zip(\n",
    "        [x[0] for x in project_meta],\n",
    "        [x[1] for x in project_meta],\n",
    "        projects_clean)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load both files\n",
    "pio = read_rwjf_data('pioneer_grantees.txt')\n",
    "\n",
    "glob = read_rwjf_data('global_grantees.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_df = pd.DataFrame([x+['pioneers'] for x in pio]+[x+['global'] for x in glob],columns=['project',\n",
    "                                                                                'code','description','source'])\n",
    "rwjf_pio_global_descriptions = rw_df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_data + 'mti_labelled_rwfj_pioneers_and_globals.out') as f:\n",
    "    mesh_od_pio_global_labels = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_od_pio_global_labels = [m[1:] for m in mesh_od_pio_global_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mti_pio_global_output_dict, vocab_pio_global_mesh_mapping = parse_mti_batch_output(mesh_od_pio_global_labels, rwjf_pio_global_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_df['doc_id'] = rw_df.index.values\n",
    "for c in ['original_phrases', 'mesh_labels', 'pub_med_uids', 'mesh_labels', 'mesh_cuis', 'mesh_duis', 'mesh_term_token_start_idx', 'mesh_term_token_end_idx']:\n",
    "    rw_df[c] = rw_df['doc_id'].map(mti_pio_global_output_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_df.to_csv(proc_data + 'rwfj_pioneers_globals_{}.csv'.format(today_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some features of the labels look like for the processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(proc_data + 'gdb_5_25_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_labels = column_to_list(gdb_df, 'mesh_labels')\n",
    "mesh_label_counts = Counter(flatten(mesh_labels))\n",
    "\n",
    "original_phrases = column_to_list(gdb_df, 'original_phrases')\n",
    "original_phrases_counts = Counter(flatten(original_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label and Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common MeSH Labels and original terms:\\n')\n",
    "print('{}\\t{:30}\\t{}\\t{:20}\\t{}\\n'.format('Rank', 'MeSH Label', 'Count', 'Original Term', 'Count'))\n",
    "\n",
    "for i, (ml, op) in enumerate(zip(mesh_label_counts.most_common(20), original_phrases_counts.most_common(20))):\n",
    "    print('{}\\t{:30}\\t{}\\t{:20}\\t{}'.format(i, ml[0], ml[1], op[0], op[1]))\n",
    "\n",
    "print('\\nNumber of unique MeSH Labels:', len(set(flatten(mesh_labels))))\n",
    "print('Number of unique original terms:', len(set(flatten(original_phrases))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common MeSH label is _Humans_, however this is not the case for the original document terms. This is because the MeSH on Demand labelling parameters works on inference as well as exact matching. There can also be multiple MeSH labels for each term identified in the document. Overall however, there are a smaller set of MeSH labels compared to the original terms that generated them. There are a lot of shared terms, revealing some of the largest topics and actors to be covered in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "mesh_label_counts_low = Counter({k: v for k, v in mesh_label_counts.items() if v < n})\n",
    "original_phrases_counts_low = Counter({k: v for k, v in original_phrases_counts.items() if v < n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common MeSH Labels and original terms:\\n')\n",
    "print('{}\\t{:40}\\t{}\\t{:40}\\t{}\\n'.format('Rank', 'MeSH Label', 'Count', 'Original Term', 'Count'))\n",
    "\n",
    "for i, (ml, op) in enumerate(zip(mesh_label_counts_low.most_common(30), original_phrases_counts_low.most_common(30))):\n",
    "    print('{}\\t{:40}\\t{}\\t{:40}\\t{}'.format(i, ml[0], ml[1], op[0], op[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some of the labels and terms with low counts, we can see that there is much more variety, and some very specific concepts have been captured, from chemicals, to ailments, to body parts and some treatments.\n",
    "\n",
    "### Matching MeSH Labels to Groups\n",
    "\n",
    "A feature of the MeSH terms is that they are organised in a tree structure, with each term being nested underneath a larger grouping. Some of the tree branches are up to 13 nodes deep. We can take the MeSH ontology (pre-processed from the available XML file), and check how this matches up to the labelled terms in the documents. We do this using the Descriptor Unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mesh_codes_processed_DUI_5_21_2018.json', 'r') as f:\n",
    "    mesh_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:20}\\t{:20}\\t{}\\t{}\\t{}\\t{}\\n'.format('MeSH Label', 'Original', 'DUI', 'Start', 'End', 'MeSH Group'))\n",
    "for label, code, op, start, end in zip(mti_output_dict['mesh_labels'][0],\n",
    "                                       mti_output_dict['mesh_duis'][0],\n",
    "                                       mti_output_dict['original_phrases'][0],\n",
    "                                       mti_output_dict['mesh_term_token_start_idx'][0],\n",
    "                                       mti_output_dict['mesh_term_token_end_idx'][0]):\n",
    "    mesh_group = mesh_codes[code]['tree_DescriptorStringProcessed_1']\n",
    "    print('{:20}\\t{:20}\\t{}\\t{}\\t{}\\t{}'.format(label, op, code, start, end, mesh_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms above are printed from those labelled in the very first document in the database (shown in full above). The groups chosen are the 1st level, so not the most high level, and they seem to separate the terms into mostly sensible categories. There are mistakes due to the parsing method we have used for the MeSH XML file, however conersation is ongoing with NLM about how best to fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Topic Modelling\n",
    "\n",
    "To demonstrate a potential use of the MeSH labels, we will have a quick look at topic modelling.\n",
    "\n",
    "One issue with topic modelling on the entire dataset, is that only very coarse themes that are common across many documents are picked up. These often include very common health issues, or administrative jargon. Using the MeSH labels, we can create a subset of documents, and perform topic modelling on them instead, in a bid to find the topics that exist within any particular concept. We can imagine this as potentially useful as an innovation discovery tool for researchers wanting to find out more about a particular health domain.\n",
    "\n",
    "As an example, we can use the MeSH label \"Climate\" to generate our data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term = \"'Climate'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_key_term_df = gdb_df[gdb_df['mesh_labels'].str.contains(key_term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdb_key_term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data subset consists of about 1700 documents.\n",
    "\n",
    "First we parse the text, remove stop words and other non-useful tokens, and create ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/external/en_ranknl_long.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "stop_words = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop in stop_words:\n",
    "    nlp_sm.vocab[stop].is_stop = True\n",
    "    nlp_sm.vocab[stop.title()].is_stop = True\n",
    "    nlp_sm.vocab[stop.upper()].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_descriptions = [nlp_sm(d) for d in gdb_key_term_df['Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_processed_lemmas = []\n",
    "for ktd in key_term_descriptions:\n",
    "    lemmas = []\n",
    "    for t in ktd:\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        if t.is_bracket:\n",
    "            continue\n",
    "        if t.is_quote:\n",
    "            continue\n",
    "        if t.like_num:\n",
    "            continue\n",
    "        if t.is_digit:\n",
    "            continue\n",
    "        if t.like_url:\n",
    "            continue\n",
    "        if t.like_email:\n",
    "            continue\n",
    "        if t.is_space:\n",
    "            continue\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.text.title() == key_term:\n",
    "            continue\n",
    "            #         if t.lower_ in stop_words:\n",
    "#             continue\n",
    "        lemmas.append(t.lower_ + '/' + t.pos_)\n",
    "    key_term_description_processed_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_tools import GensimNGrammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer = GensimNGrammer(n=2, **{'min_count': 30, 'threshold': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_processed_lemma_trigrams = ngrammer.fit_transform(key_term_description_processed_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common term frequencies, we can see that even these contain fairly contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flatten(key_term_description_processed_lemma_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the topic model. As we don't have a large number of documents, the number of topics is kept fairly low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(key_term_description_processed_lemma_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow = [dictionary.doc2bow(kt) for kt in key_term_description_processed_lemma_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=doc_bow, id2word=dictionary, num_topics=100, chunksize=200, passes=10, minimum_probability=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda, doc_bow, dictionary)\n",
    "pyLDAvis.save_html(lda_vis, fig_path + '/gdb_climate_{date}_lda_viz.html'.format(date=today_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem to be fairly representative of groups of intuitively connected concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs = []\n",
    "for db in doc_bow:\n",
    "    lda_vecs.append(gensim.matutils.sparse2full(lda[db], length=100))\n",
    "\n",
    "lda_vecs = np.array(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne = tsne.fit_transform(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne_df = pd.DataFrame(lda_tsne)\n",
    "lda_tsne_df.rename(columns={0: 'tsne_0', 1: 'tsne_1'}, inplace=True)\n",
    "labels = [', '.join(list(set(ast.literal_eval(f)))) for f in gdb_key_term_df['mesh_labels'].values]\n",
    "lda_tsne_df['labels'] = labels\n",
    "lda_tsne_df['doc_id'] = gdb_key_term_df.index.values\n",
    "lda_tsne_cds = ColumnDataSource(lda_tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(p):\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # turn off x-axis tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # turn off y-axis tick labels\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool(tooltips=[\n",
    "    (\"ID\", \"@doc_id\"),\n",
    "    (\"Labels\", \"@labels\"),\n",
    "])\n",
    "\n",
    "p = figure(width=800, height=600,\n",
    "          x_axis_label='tsne_0', y_axis_label='tsne_1',\n",
    "           title='Documents containing the MeSH label \"Climate\" transformed by LDA and t-SNE')\n",
    "p.circle(x='tsne_0', y='tsne_1', source= lda_tsne_cds, size=7, alpha=0.5)\n",
    "p.add_tools(hover)\n",
    "p = clean_plot(p)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing one of the tight clusters at random, we can get a handful of document IDs, and fetch their details from the original GDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df.iloc[[9035, 13419, 21148, 11818, 15795]][['Source ID', 'Description', 'country', 'Start Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the topics and the clusterings that we have may be due to duplicate entries in the dataset. This is possibly dominating the topic models, and also the TSNE model. \n",
    "\n",
    "Let's try to do this again on a de-duplicated \"Climate\" data subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_key_term_dedupe_df = gdb_key_term_df.drop_duplicates(subset='Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdb_key_term_dedupe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_dedupe_descriptions = [nlp_sm(d) for d in gdb_key_term_dedupe_df['Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_dedupe_description_processed_lemmas = []\n",
    "for ktd in key_term_dedupe_descriptions:\n",
    "    lemmas = []\n",
    "    for t in ktd:\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        if t.is_bracket:\n",
    "            continue\n",
    "        if t.is_quote:\n",
    "            continue\n",
    "        if t.like_num:\n",
    "            continue\n",
    "        if t.is_digit:\n",
    "            continue\n",
    "        if t.like_url:\n",
    "            continue\n",
    "        if t.like_email:\n",
    "            continue\n",
    "        if t.is_space:\n",
    "            continue\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.text.title() == key_term:\n",
    "            continue\n",
    "            #         if t.lower_ in stop_words:\n",
    "#             continue\n",
    "        lemmas.append(t.lower_ + '/' + t.pos_)\n",
    "    key_term_dedupe_description_processed_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer = GensimNGrammer(n=2, **{'min_count': 15, 'threshold': 5})\n",
    "key_term_description_dedupe_processed_lemma_trigrams = ngrammer.fit_transform(key_term_dedupe_description_processed_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common term frequencies again, we can see a similar distribution, which is reassuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flatten(key_term_description_dedupe_processed_lemma_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_term_counter(counter, n, low_pass=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the topic model. As we don't have a large number of documents, the number of topics is kept fairly low. However, we should first remove some of the most commonly occuring words, as they will skew the topics significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_term_description_dedupe_processed_lemma_trigrams_low = [[t for t in terms if counts[t] < 1000] for terms in key_term_description_dedupe_processed_lemma_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dedupe = Dictionary(key_term_description_dedupe_processed_lemma_trigrams_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow_dedupe = [dictionary_dedupe.doc2bow(kt) for kt in key_term_description_dedupe_processed_lemma_trigrams_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dedupe = LdaModel(corpus=doc_bow_dedupe, id2word=dictionary_dedupe, num_topics=50, chunksize=1000, passes=10, minimum_probability=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem to be fairly representative of groups of intuitively connected concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dedupe.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs_dedupe = []\n",
    "for db in doc_bow_dedupe:\n",
    "    lda_vecs_dedupe.append(gensim.matutils.sparse2full(lda[db], length=100))\n",
    "\n",
    "lda_vecs_dedupe = np.array(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dedupe = TSNE(\n",
    "            random_state=42,\n",
    "            perplexity=30,\n",
    ")\n",
    "lda_tsne_dedupe = tsne.fit_transform(lda_vecs_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne_dedupe_df = pd.DataFrame(lda_tsne_dedupe)\n",
    "lda_tsne_dedupe_df.rename(columns={0: 'tsne_0', 1: 'tsne_1'}, inplace=True)\n",
    "labels_dedupe = [', '.join(list(set(ast.literal_eval(f)))) for f in gdb_key_term_dedupe_df['mesh_labels'].values]\n",
    "lda_tsne_dedupe_df['labels'] = labels\n",
    "lda_tsne_dedupe_df['doc_id'] = gdb_key_term_dedupe_df.index.values\n",
    "lda_tsne_dedupe_cds = ColumnDataSource(lda_tsne_dedupe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool(tooltips=[\n",
    "    (\"ID\", \"@doc_id\"),\n",
    "    (\"Labels\", \"@labels\"),\n",
    "])\n",
    "\n",
    "p = figure(width=800, height=600,\n",
    "           x_axis_label='tsne_0', y_axis_label='tsne_1',\n",
    "           title='De-duplicated documents containing the MeSH label \"Climate\" transformed by LDA and t-SNE')\n",
    "p.circle(x='tsne_0', y='tsne_1', source=lda_tsne_dedupe_cds, size=7, alpha=0.5)\n",
    "p.add_tools(hover)\n",
    "p = clean_plot(p)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda_dedupe, doc_bow_dedupe, dictionary_dedupe)\n",
    "pyLDAvis.save_html(lda_vis, fig_path + '/gdb_climate_dedupe_{date}_lda_viz.html'.format(date=today_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the very dense clusters have disappeared.\n",
    "\n",
    "From topic modelling within an individual topic, we can see that it is possible to uncover the more nuanced topics within the documents covered by a broad term such as \"Climate\". This could be useful for helping users to discover new areas to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
