{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks and Word Vectors with MeSH Labels\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "from analysis.src.data.readnwrite import get_data_dir\n",
    "from analysis.src.data.data_utilities import flatten, eval_column, grouper\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.centrality import eigenvector\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.draw import graph_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "# Get the top path\n",
    "data_path = get_data_dir()\n",
    "\n",
    "# Create the path for external data\n",
    "ext_data = os.path.join(data_path, 'external')\n",
    "# Raw data\n",
    "raw_data = os.path.join(data_path, 'raw')\n",
    "# And external data\n",
    "proc_data = os.path.join(data_path, 'processed')\n",
    "# And interim data\n",
    "inter_data = os.path.join(data_path, 'interim')\n",
    "# And figures\n",
    "fig_path = os.path.join(data_path, 'figures')\n",
    "\n",
    "# Get date for saving files\n",
    "today = datetime.utcnow()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.year,today.month,today.day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are going to load both the GDB and the RWJF Pioneer and Global projects, and join them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.read_csv(os.path.join(raw_data, 'gdb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjf_df = pd.read_csv(os.path.join(inter_data, 'rwjf_pioneer_and_global_projects.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join the other relevant data modules:\n",
    "\n",
    "Dates for GDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_dates_df = pd.read_csv(os.path.join(inter_data, 'gdb_dates.csv'))\n",
    "gdb_df = pd.concat([gdb_df, gdb_dates_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeSH labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_mesh_df = pd.read_csv(os.path.join(inter_data, 'gdb_mesh_labels.csv'))\n",
    "rwjf_mesh_df = pd.read_csv(os.path.join(inter_data, 'rwjf_mesh_labels.csv'))\n",
    "\n",
    "gdb_df = pd.concat([gdb_df, gdb_mesh_df], axis=1)\n",
    "rwjf_df = pd.concat([rwjf_df, rwjf_mesh_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove projects from GitHub as they don't play nicely with MeSH terms, and Crunchbase as they're very short. There are also some projects with null descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = gdb_df[gdb_df['source_id'] != 'GitHub']\n",
    "gdb_df = gdb_df[gdb_df['source_id'] != 'Crunchbase']\n",
    "gdb_df['description'][pd.isnull(gdb_df['description'])] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the two sets of projects and extract their descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df = pd.concat([gdb_df, rwjf_df], axis=0)\n",
    "gdb_df.set_index('doc_id', inplace=True)\n",
    "gdb_df = gdb_df.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(gdb_df['description'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a MeSH Label Corpus\n",
    "\n",
    "We need to build a corpus of MeSH label transformed documents that is appropriate for the network we want to build. This will require some filtering, however first we should build a vocabulary of all the terms that we have, so that we can reference any of them by a unique ID at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels = eval_column(gdb_df, 'mesh_labels')\n",
    "\n",
    "dictionary_mesh_labels = Dictionary(description_mesh_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For filtering later, we will calculate the counts of the MeSH labels. We know already that there are some labels which are highly over-represented, and many which occur only once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts = Counter(flatten(description_mesh_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sliding Window Coocurrence Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will want to create a new set of labelled descriptions where the terms with very high counts and little semantic value are removed, and also those that appear very few times in the corpus. We will also need to map the labels to token IDs which can then act as the vertex values in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_filter(docs, high_threshold=None, low_threshold=None, remove=[], counter=None):\n",
    "    \"\"\"freqency_filter\n",
    "    Filters words from a corpus that occur more frequently than high_threshold\n",
    "    and less frequently than low_threshold.\n",
    "    \n",
    "    Args:\n",
    "        docs (:obj:`list` of :obj:`list`): Corupus of tokenised documents.\n",
    "        high_threshold (int): Upper limit for token frequency\n",
    "        low_threshold (int): Lower limit for token frequency\n",
    "        remove (:obj:`list`): List of terms to remove\n",
    "    \n",
    "    Yields:\n",
    "        doc_filtered (:obj:`list`): Document with elements removed based\n",
    "            on frequency\n",
    "    \"\"\"\n",
    "    docs_filtered = []\n",
    "    if counter is None:\n",
    "        counter = Counter(flatten(docs))\n",
    "    for doc in docs:\n",
    "        doc_filtered = []\n",
    "        for t in doc:\n",
    "            if t in remove:\n",
    "                continue\n",
    "            if high_threshold is not None:\n",
    "                if counter[t] > high_threshold:\n",
    "                    continue\n",
    "            if low_threshold is not None:\n",
    "                if counter[t] < low_threshold:\n",
    "                    continue\n",
    "            doc_filtered.append(t)\n",
    "        docs_filtered.append(doc_filtered)\n",
    "    return docs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetitions(doc):\n",
    "    \"\"\"remove_repetitions\n",
    "    Returns a modified list where there are no sequential repetitions.\n",
    "    \n",
    "    Args:\n",
    "        doc (:obj:`list`): List of elements with sequential repetitions.\n",
    "        \n",
    "    Returns:\n",
    "        doc (:obj:`list`): List of elements with no sequential repetitions.\n",
    "    \n",
    "    Examples:\n",
    "        >>> doc = ['hi', 'hi', 'there', 'my', 'my', friend', 'friend', 'friend']\n",
    "        \n",
    "        >>> remove_repetitions(doc)\n",
    "        ['hi', 'there', 'my', 'friend']\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = [t[0] for t in itertools.groupby(doc)]\n",
    "    return doc\n",
    "\n",
    "print('remove_repetitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(seq, n=3):\n",
    "    \"\"\"window\n",
    "    Returns a sliding window (of width n) over data from the iterable\n",
    "       s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...\n",
    "    \"\"\"\n",
    "    it = iter(seq)\n",
    "    result = tuple(itertools.islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_coocurrences(docs, n=3):\n",
    "    coocurrences = []\n",
    "    for doc in docs:\n",
    "        doc_i = range(len(doc))\n",
    "        doc_i = window(doc_i, n=n)\n",
    "        doc_i = flatten([doc2coocurrences(t) for t in doc_i])\n",
    "        doc = [sorted((doc[a], doc[b])) for a, b in list(set(doc_i))]\n",
    "        doc = [tuple((a, b)) for a, b in doc if a !=b]\n",
    "        coocurrences.append(doc)\n",
    "    return coocurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2coocurrences(doc):\n",
    "    doc_combos = list(itertools.combinations(set(doc), r=2))\n",
    "    doc_coocurrences = list(set([tuple(sorted(dc)) for dc in doc_combos]))\n",
    "    return doc_coocurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2coocurrences(docs):\n",
    "    \"\"\"docs2coocurrences\n",
    "    Takes a set of documents and transforms each of them into a list\n",
    "    of tuples, representing every term-term coocurrence in the \n",
    "    document.\n",
    "    \n",
    "    Args:\n",
    "        docs (list of list of str): A list of tokenised documents.\n",
    "    \n",
    "    Returns:\n",
    "        doc_coocurrences (list of list of tuple): A list of all documents\n",
    "            transformed into term-term coocurrence tuples. Tuples are \n",
    "            sorted alphabetically.\n",
    "            \n",
    "    Examples:\n",
    "        >>> pie_docs = [\n",
    "                ['i', 'like', 'pie'],\n",
    "                ['me', 'too'],\n",
    "                ['i', 'like', 'it', 'more']\n",
    "            ]\n",
    "            \n",
    "        >>> docs2coocurrences(pie_docs)\n",
    "        [[('i', 'like'), ('i', 'pie'), ('like', 'pie')],\n",
    "         [('me', 'too')],\n",
    "         [('i', 'like'), ('i', 'it'), ('i', 'more'), ('like', 'it'), \n",
    "         ('like', 'more'), ('it', 'more')]\n",
    "        ]\n",
    "    \"\"\"\n",
    "    doc_coocurrences = []\n",
    "    for d in docs:\n",
    "        doc_combos = list(itertools.combinations(set(d), r=2))\n",
    "        doc_combos = [tuple(sorted(dc)) for dc in doc_combos]\n",
    "        doc_coocurrences.append(doc_combos)\n",
    "    return doc_coocurrences\n",
    "\n",
    "def edge_coocurrence_counts(doc_coocurrences):\n",
    "    \"\"\"coocurrence_counts\n",
    "    Takes a corpus of document coocurrence combinations and returns a\n",
    "    Counter object for them across the entire corpus.\n",
    "    \n",
    "    Args:\n",
    "        doc_coocurrences (:obj:`list` of :obj:`list` of :obj:`tuple`): \n",
    "            Corpus of documents expressed as their coocurrence pairs.\n",
    "            \n",
    "    Returns:\n",
    "        coocurrence_counts (:obj:`Counter`): Counter with keys as edges\n",
    "            and values as number of coocurrences between the two vertices.\n",
    "    \"\"\"\n",
    "    coocurrences = flatten(doc_coocurrences)\n",
    "    coocurrence_counts = Counter(coocurrences)\n",
    "    return coocurrence_counts\n",
    "\n",
    "def vertex_degree_centralities(coocurrence_counts):\n",
    "    \"\"\"vertex_degree_centrality\n",
    "    Takes a Counter of edge coocurrences and returns the degree centrality\n",
    "    for each vertex.\n",
    "    \n",
    "    Args:\n",
    "        coocurrence_counts (:obj:`Counter`): Counter with keys as edges\n",
    "            and values as number of coocurrences between the two vertices.\n",
    "            \n",
    "    Returns: \n",
    "        vertex_degrees (:obj:`Counter`): Counter with keys as vertices\n",
    "            and values as degree centralities.\n",
    "    \"\"\"\n",
    "    vertex_degrees = Counter()\n",
    "    for vertices, count in coocurrence_counts.items():\n",
    "        v_0 = vertices[0]\n",
    "        v_1 = vertices[1]\n",
    "        if v_0 in vertex_degrees:\n",
    "            vertex_degrees[v_0] += 1\n",
    "        else:\n",
    "            vertex_degrees[v_0] = 1\n",
    "            \n",
    "        if v_1 in vertex_degrees:\n",
    "            vertex_degrees[v_1] += 1\n",
    "        else:\n",
    "            vertex_degrees[v_1] = 1\n",
    "    return vertex_degrees\n",
    "\n",
    "def vertex_coocurrence_centrality(coocurrence_counts):\n",
    "    \"\"\"vertex_coocurrence_centrality\n",
    "    Takes a Counter of edge cooccurences and returns the coocurrence centrality\n",
    "    for each vertex. This is a summation of all coocurrences for each vertex.\n",
    "    \n",
    "    Args:\n",
    "        coocurrence_counts (:obj:`Counter`): Counter with keys as edges\n",
    "            and values as number of coocurrences between the two vertices.\n",
    "            \n",
    "    Returns:\n",
    "        vertex_coocurrences (:obj:`Counter`): Counter with keys as vertices\n",
    "            and values as number of coocurrences.\n",
    "    \"\"\"\n",
    "    \n",
    "    vertex_coocurrences = Counter()\n",
    "    for vertices, count in coocurrence_counts.items():\n",
    "        v_0 = vertices[0]\n",
    "        v_1 = vertices[1]\n",
    "        if v_0 in vertex_coocurrences:\n",
    "            vertex_coocurrences[v_0] += count\n",
    "        else:\n",
    "            vertex_coocurrences[v_0] = count\n",
    "        if v_1 in vertex_coocurrences:\n",
    "            vertex_coocurrences[v_1] += count\n",
    "        else:\n",
    "            vertex_coocurrences[v_1] = count\n",
    "    return vertex_coocurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(total_edges, n_coocurrences, coocurrence_count_0, coocurrence_count_1):\n",
    "    association_strength = (2 * total_edges * n_coocurrences) / (coocurrence_count_0 * coocurrence_count_1)\n",
    "    return association_strength\n",
    "\n",
    "def mesh_label_association_strength(term_0, term_1):\n",
    "    return association_strength(total_edges, coocurrence_counts[tuple(sorted([term_0, term_1]))],\n",
    "                     mesh_label_id_coocurrence_counts[term_0],\n",
    "                     mesh_label_id_coocurrence_counts[term_1])\n",
    "\n",
    "def normalised_coocurrence_count(n_coocurrences, count_term_0, count_term_1):\n",
    "    return n_coocurrences / (count_term_0 + count_term_1)\n",
    "\n",
    "def mesh_label_normalised_coocurrence_count(term_0, term_1):\n",
    "    n_cooc = coocurrence_counts[tuple(sorted([term_0, term_1]))]\n",
    "    return normalised_coocurrence_count(n_cooc,\n",
    "                                    mesh_label_deduped_id_counts[term_0],\n",
    "                                    mesh_label_deduped_id_counts[term_1])\n",
    "\n",
    "def jaccard_similarity(term_0, term_1, n):\n",
    "    return n / (mesh_label_unique_counts[term_0] + mesh_label_unique_counts[term_1] - n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_mesh_label_filtered = frequency_filter(description_mesh_labels, high_threshold=13000, low_threshold=5)\n",
    "dictionary_filtered = Dictionary(descriptions_mesh_label_filtered)\n",
    "\n",
    "descriptions_mesh_label_filtered_ids = [dictionary_filtered.doc2idx(dmlf) for dmlf in descriptions_mesh_label_filtered]\n",
    "\n",
    "description_mesh_label_sliwi_coocurrences = sliding_window_coocurrences(descriptions_mesh_label_filtered_ids, n=3)\n",
    "\n",
    "description_mesh_label_sliwi_edges = list(set(flatten(description_mesh_label_sliwi_coocurrences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliwi_edge_coocurrence_counts = edge_coocurrence_counts(description_mesh_label_sliwi_coocurrences)\n",
    "sliwi_vertex_degree_centralities = vertex_degree_centralities(sliwi_edge_coocurrence_counts)\n",
    "sliwi_vertex_coocurrences_centralities = vertex_coocurrence_centrality(sliwi_edge_coocurrence_counts)\n",
    "\n",
    "n_vertices = len(sliwi_vertex_degree_centralities)\n",
    "n_edges = len(sliwi_edge_coocurrence_counts)\n",
    "n_coocurrences = len(flatten(description_mesh_label_sliwi_coocurrences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total verticies: {:,}'.format(n_vertices))\n",
    "print('Total edges: {:,}'.format(n_edges))\n",
    "print('Total coocurrences: {:,}'.format(n_coocurrences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliwi_graph = gt.Graph(directed=False)\n",
    "sliwi_graph.add_vertex(n_vertices)\n",
    "sliwi_graph.add_edge_list(description_mesh_label_sliwi_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add association strengths\n",
    "a_strength_sliwi_graph = sliwi_graph.new_edge_property(\"float\")\n",
    "\n",
    "for s, t in description_mesh_label_sliwi_edges:\n",
    "    a_strength_sliwi_graph[sliwi_graph.edge(s, t)] = association_strength(\n",
    "        n_coocurrences,\n",
    "        sliwi_edge_coocurrence_counts[tuple(sorted([s, t]))],\n",
    "        sliwi_vertex_coocurrences_centralities[s],\n",
    "        sliwi_vertex_coocurrences_centralities[t])\n",
    "    \n",
    "sliwi_graph.edge_properties[\"association_strength\"] = a_strength_sliwi_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(inter_data, 'sliwi_mesh_label_coocurrences_100.edgelist'), 'w') as f:\n",
    "    for edge, assoc_strength in zip(sliwi_graph.edges(), a_strength_sliwi_graph.get_array()):\n",
    "        s = int(edge.source())\n",
    "        t = int(edge.target())\n",
    "        if (s < 100) & (t < 100):\n",
    "            w = '{} {} {}\\n'.format(s, t, assoc_strength)\n",
    "            f.write(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(inter_data, 'test_graph.edgelist'), 'w') as f:\n",
    "    for edge in G.edges:\n",
    "        s = '{} {}\\n'.format(edge[0], edge[1])\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliwi_graph_nx = nx.Graph()\n",
    "sliwi_graph_nx.add_nodes_from(list(set(flatten(descriptions_mesh_label_filtered_ids))))\n",
    "sliwi_graph_nx.add_edges_from(description_mesh_label_sliwi_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_sliwi = Node2Vec(sliwi_graph_nx, dimensions=128,\n",
    "                    walk_length=60, num_walks=16, workers=4,\n",
    "                    p=1, q=2)\n",
    "node2vec_model_sliwi = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Graph Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_deduped_counts = Counter(flatten(description_mesh_labels_deduped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_deduped = [[m.lower() for m in list(set(ml))] for ml in description_mesh_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_duis = eval_column(gdb_df, 'mesh_duis')\n",
    "description_mesh_duis_unique = [list(set(ml)) for ml in description_mesh_duis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_deduped_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_description_labels(description_labels, fn):\n",
    "    return [list(filter(fn, dl)) for dl in description_labels]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_deduped_filtered = filter_description_labels(description_mesh_labels_deduped,\n",
    "                                            lambda x: x not in ['humans', 'goals', 'students', 'animals', 'universities',\n",
    "                                            'research personnel', 'research', 'awards and prizes', 'faculty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_dictionary = Dictionary(description_mesh_labels_deduped_filtered)\n",
    "\n",
    "# create a version of the MeSH label descriptions with IDs replacing their tokens\n",
    "description_mesh_labels_deduped_ids = [mesh_label_dictionary.doc2idx(dmld) for dmld in description_mesh_labels_deduped_filtered]\n",
    "\n",
    "# count the number of occurrences of each ID\n",
    "mesh_label_deduped_id_counts = Counter(flatten(description_mesh_labels_deduped_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [d for d in description_mesh_labels if 'Machine Learning' in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_token_edges(tokenised_descriptions):\n",
    "    combinations = []\n",
    "    for td in tokenised_descriptions:\n",
    "        description_combos = list(itertools.combinations(set(td), r=2))\n",
    "        description_combos = [tuple(sorted(dc)) for dc in description_combos]\n",
    "        combinations.append(description_combos)\n",
    "    return combinations\n",
    "\n",
    "description_mesh_label_id_edge_list = document_token_edges(description_mesh_labels_deduped_ids)\n",
    "mesh_label_id_edge_list = list(set(flatten(description_mesh_label_id_edge_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocurrence_counts = Counter(flatten(description_mesh_label_id_edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_id_coocurrence_counts = {}\n",
    "\n",
    "for k, v in coocurrence_counts.items():\n",
    "    term_0 = k[0]\n",
    "    term_1 = k[1]\n",
    "    if term_0 in mesh_label_id_coocurrence_counts:\n",
    "        mesh_label_id_coocurrence_counts[term_0] += v\n",
    "    else:\n",
    "        mesh_label_id_coocurrence_counts[term_0] = v\n",
    "    if term_1 in mesh_label_id_coocurrence_counts:\n",
    "        mesh_label_id_coocurrence_counts[term_1] += v\n",
    "    else:\n",
    "        mesh_label_id_coocurrence_counts[term_1] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = len(mesh_label_deduped_id_counts)\n",
    "total_coocurrences = len(flatten(description_mesh_label_id_edge_list))\n",
    "total_edges = len(coocurrence_counts)\n",
    "\n",
    "print('Total number of terms: {:,}'.format(total_labels))\n",
    "print('Total number of coocurrences: {:,}'.format(total_coocurrences))\n",
    "print('Total number of edges: {:,}'.format(total_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(total_edges, n_coocurrences, edge_count_0, edge_count_1):\n",
    "    association_strength = (2 * total_edges * n_coocurrences) / (edge_count_0 * edge_count_1)\n",
    "    return association_strength\n",
    "\n",
    "def mesh_label_association_strength(term_0, term_1):\n",
    "    return association_strength(total_edges, coocurrence_counts[tuple(sorted([term_0, term_1]))],\n",
    "                     mesh_label_id_coocurrence_counts[term_0],\n",
    "                     mesh_label_id_coocurrence_counts[term_1])\n",
    "\n",
    "def normalised_coocurrence_count(n_coocurrences, count_term_0, count_term_1):\n",
    "    return n_coocurrences / (count_term_0 + count_term_1)\n",
    "\n",
    "def mesh_label_normalised_coocurrence_count(term_0, term_1):\n",
    "    n_cooc = coocurrence_counts[tuple(sorted([term_0, term_1]))]\n",
    "    return normalised_coocurrence_count(n_cooc,\n",
    "                                    mesh_label_deduped_id_counts[term_0],\n",
    "                                    mesh_label_deduped_id_counts[term_1])\n",
    "\n",
    "def jaccard_similarity(term_0, term_1, n):\n",
    "    return n / (mesh_label_unique_counts[term_0] + mesh_label_unique_counts[term_1] - n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Graph\n",
    "\n",
    "We now have a series of documents described in terms of their MeSH labels. From this we can build a network to explore the connections between concepts, and try to understand how we might be able to find \"innovative\" combinations or terms in an unsupervised manner. For this network, nodes will be the terms themselves, while an edge being drawn between two nodes will represent that term pair appearing in at least one document together - a coocurrence.\n",
    "\n",
    "- Node attributes:\n",
    "    - count: total number of times the term appeared in the corpus (int)\n",
    "    - count_normalised: count, normalised by the total number of terms in the corpus (float)\n",
    "    - mesh_tree_codes: array of MeSH Tree Numbers (list)\n",
    "    - edge_count: number of other nodes that this node is connected to (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_graph = gt.Graph(directed=False)\n",
    "term_graph.add_vertex(total_labels)\n",
    "term_graph.add_edge_list(mesh_label_id_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add association strengths\n",
    "association_strength_term_graph = term_graph.new_edge_property(\"double\")\n",
    "\n",
    "for s, t in mesh_label_id_edge_list:\n",
    "    association_strength_term_graph[term_graph.edge(s, t)] = association_strength(\n",
    "        total_edges,\n",
    "        coocurrence_counts[tuple(sorted([s, t]))],\n",
    "        mesh_label_id_coocurrence_counts[s],\n",
    "        mesh_label_id_coocurrence_counts[t])\n",
    "    \n",
    "term_graph.edge_properties[\"association_strength\"] = association_strength_term_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add term frequencies\n",
    "label_count_term_graph = term_graph.new_vertex_property(\"int\")\n",
    "\n",
    "for k, v in mesh_label_deduped_id_counts.items():\n",
    "    label_count_term_graph[term_graph.vertex(k)] = v\n",
    "    \n",
    "term_graph.vertex_properties[\"label_count\"] = label_count_term_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pass_filter_term_graph = term_graph.new_vertex_property(\"bool\")\n",
    "\n",
    "for k, v in mesh_label_deduped_id_counts.items():\n",
    "    if v > 10:\n",
    "        high_pass_filter_term_graph[term_graph.vertex(k)] = True\n",
    "    else:\n",
    "        high_pass_filter_term_graph[term_graph.vertex(k)] = False\n",
    "        \n",
    "term_graph.vertex_properties[\"high_pass_filter\"] = high_pass_filter_term_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(inter_data, 'full_mesh_label_coocurrences.edgelist'), 'w') as f:\n",
    "    for edge, assoc_strength in zip(term_graph.edges(), association_strength_term_graph.get_array()):\n",
    "        s = '{} {} {}\\n'.format(int(edge.source()), int(edge.target()), assoc_strength)\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.centrality import betweenness, closeness, katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_graph.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_graph_betweenness_nw = betweenness(term_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_graph_katz_nw = katz(term_graph, alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_graph_closeness_nw = closeness(term_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df = pd.DataFrame({'betweenness': bv_nw.get_array().astype('float32'),\n",
    "                              'closeness': term_graph_closeness_nw.get_array().astype('float32'),\n",
    "                              'katz': term_graph_katz_nw.get_array().astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df['closeness'][pd.isnull(centrality_df['closeness'])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_tsne = tsne.fit_transform(centrality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(centrality_tsne[:, 0], centrality_tsne[:, 1], alpha=0.1)\n",
    "plt.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Term Sub-Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doctokens2graph(docs, dictionary):\n",
    "    \n",
    "    n_vertices = len(dictionary.keys())\n",
    "\n",
    "    id_docs = [dictionary.doc2idx(d) for d in docs]\n",
    "    id_coocurrences = docs2coocurrences(id_docs)\n",
    "    id_edges = list(set(flatten(id_coocurrences)))\n",
    "\n",
    "    graph = gt.Graph(directed=False)\n",
    "    graph.add_vertex(n_vertices)\n",
    "    graph.add_edge_list(id_coocurrences)\n",
    "    \n",
    "    return graph, id_edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a graph for each subset of documents that contains a label. Calculate centrality and other measures for that label within its sub-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities = {}\n",
    "\n",
    "for label in mesh_label_dictionary.values():\n",
    "    subset_descriptions = [d for d in description_mesh_labels_deduped_filtered if label in d]\n",
    "    n_tokens = len(flatten(subset_descriptions))\n",
    "    n_docs = len(subset_descriptions)\n",
    "    dictionary = Dictionary(subset_descriptions)\n",
    "\n",
    "    n_terms = len(dictionary.keys())\n",
    "\n",
    "    label_id = dictionary.token2id[label]\n",
    "\n",
    "    subset_description_ids = [dictionary.doc2idx(sd) for sd in subset_descriptions]\n",
    "    subset_description_id_counts = Counter(flatten(subset_description_ids))\n",
    "\n",
    "    subset_id_coocurrences = docs2coocurrences(subset_description_ids)\n",
    "    subset_id_edge_list = list(set(flatten(subset_id_coocurrences)))\n",
    "    n_edges = len(subset_id_edge_list)\n",
    "    \n",
    "    coocurrence_counts = edge_coocurrence_counts(subset_id_coocurrences)\n",
    "    vertex_coocurrences = vertex_coocurrence_centrality(coocurrence_counts)\n",
    "    \n",
    "    sub_graph = gt.Graph(directed=False)\n",
    "    sub_graph.add_vertex(n_terms)\n",
    "    sub_graph.add_edge_list(subset_id_edge_list)\n",
    "    \n",
    "#     association_strength_sub_graph = sub_graph.new_edge_property(\"double\")\n",
    "\n",
    "#     for s, t in subset_id_edge_list:\n",
    "#         association_strength_sub_graph[sub_graph.edge(s, t)] = association_strength(\n",
    "#             n_edges,\n",
    "#             coocurrence_counts[tuple(sorted([s, t]))],\n",
    "#             vertex_coocurrences[s],\n",
    "#             vertex_coocurrences[t])\n",
    "    \n",
    "#     sub_graph.edge_properties[\"association_strength\"] = association_strength_sub_graph\n",
    "\n",
    "    label_betweenness = betweenness(sub_graph)[0].get_array().astype('float32')[label_id]\n",
    "    label_katz = katz(sub_graph, alpha=0.01).get_array().astype('float32')[label_id]\n",
    "    label_global_cluster_coef = global_clustering(sub_graph)\n",
    "#     label_betweenness = betweenness(sub_graph, weight=association_strength_sub_graph)[0].get_array().astype('float32')[label_id]\n",
    "#     label_katz = katz(sub_graph, alpha=0.001, weight=association_strength_sub_graph).get_array().astype('float32')[label_id]\n",
    "#     label_closeness = closeness(sub_graph).get_array().astype('float32')[label_id]\n",
    "\n",
    "    n_edges = sub_graph.vertex(label_id).out_degree()\n",
    "\n",
    "    sub_graph_centralities[label] = {'betweenness': label_betweenness,\n",
    "                                     'katz': label_katz,\n",
    "#                                      'closeness': label_closeness,\n",
    "                                     'global_cluster_coef': label_global_cluster_coef[0],\n",
    "                                     'global_cluster_std': label_global_cluster_coef[1],\n",
    "                                     'n_vertices': n_terms,\n",
    "                                     'n_edges': n_edges,\n",
    "                                     'n_docs': n_docs,\n",
    "                                     'n_tokens': n_tokens,\n",
    "                                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df_weighted = pd.DataFrame.from_records(sub_graph_centralities).T\n",
    "sub_graph_centralities_df_weighted = sub_graph_centralities_df_weighted[sub_graph_centralities_df_weighted['n_docs'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df_weighted['n_vertices_normed'] = sub_graph_centralities_df_weighted['n_vertices'] / sub_graph_centralities_df_weighted['n_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df_weighted['lexical_richness'] = sub_graph_centralities_df_weighted['n_vertices'] / sub_graph_centralities_df_weighted['n_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df_weighted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df = sub_graph_centralities_df[sub_graph_centralities_df['n_docs'] >= 5]\n",
    "sub_graph_centralities_df['n_vertices_normed'] = sub_graph_centralities_df['n_vertices'] / sub_graph_centralities_df['n_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_centralities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sub_graph_centralities_df['betweenness'],\n",
    "            sub_graph_centralities_df_weighted['betweenness'],\n",
    "            alpha=0.1, edgecolor='none', \n",
    "            c=sub_graph_centralities_df_weighted['n_edges'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mln in mesh_label_nodes:\n",
    "    count = mesh_label_unique_counts[mln]\n",
    "    graph_mesh_labels.add_node(\n",
    "        mln,\n",
    "        count=count,\n",
    "        count_normalised=count / n_mesh_labels_unique,\n",
    "        group_0=map_tree_group(\n",
    "            mln, \n",
    "            mesh_label_dui_map, \n",
    "            dui_tree_number_map, \n",
    "            tree_level_0_map,\n",
    "            level=0\n",
    "        ),\n",
    "        group_1=map_tree_group(\n",
    "            mln, \n",
    "            mesh_label_dui_map, \n",
    "            dui_tree_number_map, \n",
    "            tree_level_1_map\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node is connected to another node by an edge when they coocurr in at least one document. However, this provides no information about how often different labels coocurr with each other. To account for this, we will provide a choice of weights for the edges. The first is the raw number of coocurrences across the corpus. We then also calculate the Jaccard index and the association strength as defined in Waltman _et al_.  2009 and Noyons _et al_. 2010 respectively.\n",
    "\n",
    "- Edge attributes\n",
    "    - weight_absolute: number of documents in which terms coocur (int)\n",
    "    - jaccard_similarity: number between 0 and 1 representing the Jaccard Index of terms (float)\n",
    "    - association_strength: association strength score of terms (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_combinations = []\n",
    "for ml in description_mesh_labels_unique:\n",
    "    mesh_label_doc = list(itertools.combinations(set(ml), r=2))\n",
    "    mesh_label_doc = [tuple(sorted(mld)) for mld in mesh_label_doc]\n",
    "    description_mesh_label_combinations.append(mesh_label_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_coocurrence_counts = Counter(flatten(description_mesh_label_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_coocurrence_count_map = defaultdict(dict)\n",
    "\n",
    "for ml, c in edge_coocurrence_counts.items():\n",
    "    n0 = ml[0]\n",
    "    n1 = ml[1]\n",
    "    mesh_label_coocurrence_count_map[n0][n1] = c\n",
    "    mesh_label_coocurrence_count_map[n1][n0] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coocurrences = len(flatten(description_mesh_label_combinations))\n",
    "n_edges = len(edge_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Co-occurrences:', n_coocurrences)\n",
    "print('Number of Edges:', n_edges)\n",
    "print('Mean Co-occurences per Edge:', n_coocurrences / n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_coocurrence_counts = {}\n",
    "mesh_label_edge_counts = {}\n",
    "\n",
    "for ml, co in mesh_label_coocurrence_count_map.items():\n",
    "    coocurrence_count = sum([n for n in co.values()])\n",
    "    edge_count = len(co)\n",
    "    mesh_label_edge_counts[ml] = edge_count\n",
    "    mesh_label_coocurrence_counts[ml] = coocurrence_count\n",
    "\n",
    "mesh_label_coocurrence_counts = Counter(mesh_label_coocurrence_counts)\n",
    "mesh_label_edge_counts = Counter(mesh_label_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_counter_extremes(mesh_label_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_counter_extremes(mesh_label_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    node_0 = edge[0]\n",
    "    node_1 = edge[1]\n",
    "    graph_mesh_labels.add_edge(node_0, node_1,\n",
    "                               coocurrences_absolute=count,\n",
    "                               jaccard_similarity=jaccard_similarity(node_0,\n",
    "                                                                     node_1,\n",
    "                                                                     count),\n",
    "                               association_strength=coocurrence_association_strength(node_0,\n",
    "                                                                                     node_1,\n",
    "                                                                                     count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mesh_term_raw_graph.json', 'w') as f:\n",
    "    json.dump(nx.node_link_data(graph_mesh_labels), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the edge attributes for the most common and some of the least common coocurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "\n",
    "print('{:<30}\\t{:<8}\\t{:30}\\t{:<8}\\t{:<15}\\t{}\\t{}\\n'.format(\n",
    "    'Node 0', 'Count 0', 'Node 1', 'Count 1', 'Cooccurences',\n",
    "    'Jaccard Index', 'Association Strength'))\n",
    "\n",
    "for l, n in edge_coocurrence_counts.most_common(top_n):\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    c0 = mesh_label_unique_counts[term_0]\n",
    "    c1 = mesh_label_unique_counts[term_1]\n",
    "    j = jaccard_similarity(term_0, term_1, n)\n",
    "    a = coocurrence_association_strength(term_0, term_1, n)\n",
    "    print('{:<30}\\t{:<8}\\t{:30}\\t{:<8}\\t{:<15}\\t{:.2f}\\t{:12.2f}'.format(l[0], c0, l[1], c1, n, j, a))\n",
    "    \n",
    "print('...')\n",
    "\n",
    "low_counts = {k: v for k, v in edge_coocurrence_counts.items() if v <= 3}\n",
    "low_count_keys_n = list(itertools.islice(low_counts, top_n))\n",
    "low_counts_n = [(k, low_counts[k]) for k in low_count_keys_n]\n",
    "\n",
    "for l, n in low_counts_n:\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    c0 = mesh_label_unique_counts[term_0]\n",
    "    c1 = mesh_label_unique_counts[term_1]\n",
    "    j = jaccard_similarity(term_0, term_1, n)\n",
    "    a = coocurrence_association_strength(term_0, term_1, n)\n",
    "    print('{:<30}\\t{:8}\\t{:30}\\t{:<8}\\t{:<15}\\t{:.2f}\\t{:12.2f}'.format(l[0], c0, l[1], c1, n, j, a))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that edges representing high numbers of coocurrences are unsurprisingly drawn between terms that have a high count across the corpus. Interestingly, the terms with the highest number of coocurrences are not the two that have the highest frequency (_Humans_ and _Students_). We can see that for the top terms, the Jaccard similarity hovers at the lower end of the range. We can see how it is modulated by both the number of coocurrences, but also the individual term frequencies. At the lower end of the coocurrence counts, we can see connected terms that exhibit a Jaccard index of only 0.01 or less. The association strength however shows a large degree of variation, in part due to its inherent nature of not being bounded between finite limits. The range of values among the most highly occurring terms and the less frequent terms are more consistent too. However, it can be seen that terms with lower frequencies can exhibit much higher association strengths, as their low counts reduces the chances of them coocurring \"by chance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from the MeSH labels the their unique IDs\n",
    "mesh_label_dui_map = {}\n",
    "\n",
    "for labels, duis in zip(description_mesh_labels_unique, description_mesh_duis_unique):\n",
    "    for label, dui in zip(labels, duis):\n",
    "        if label not in mesh_label_dui_map:\n",
    "            mesh_label_dui_map[label] = dui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(inter_data, 'mesh_ontology', 'mesh_descriptions.json'), 'r') as f:\n",
    "    mesh_label_ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from the MeSH label unique IDs to their MeSH Tree number\n",
    "dui_tree_number_map = {}\n",
    "\n",
    "for descriptor in mesh_label_ontology['DescriptorRecordSet']['DescriptorRecord']:\n",
    "    tree_number = descriptor.get('TreeNumberList')\n",
    "    if tree_number is not None:\n",
    "        tree_number = tree_number.get('TreeNumber')\n",
    "    if isinstance(tree_number, list):\n",
    "        dui_tree_number_map[descriptor['DescriptorUI']] = tree_number\n",
    "    else:\n",
    "        dui_tree_number_map[descriptor['DescriptorUI']] = [tree_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of 0th level MeSH Tree codes to their semantic representations\n",
    "tree_level_0_map = {\n",
    "    'A': 'anatomy',\n",
    "    'B': 'organisms',\n",
    "    'C': 'diseases',\n",
    "    'D': 'chemicals and drugs',\n",
    "    'E': 'analytical, diagnostic, and therapeutic techniques, and equipment',\n",
    "    'F': 'psychiatry and psychology',\n",
    "    'G': 'phenomena and processes',\n",
    "    'H': 'disciplines and occupations',\n",
    "    'I': 'anthropology, education, sociology, and social phenomena',\n",
    "    'J': 'technology, industry, and agriculture',\n",
    "    'K': 'humanities',\n",
    "    'L': 'information science',\n",
    "    'M': 'named groups',\n",
    "    'N': 'health care',\n",
    "    'V': 'publication characteristics',\n",
    "    'Z': 'geographicals'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of 1st level MeSH Tree codes to their semantic representations\n",
    "tree_level_1_map = {}\n",
    "\n",
    "for descriptor in mesh_label_ontology['DescriptorRecordSet']['DescriptorRecord']:\n",
    "    tree_number = descriptor.get('TreeNumberList')\n",
    "    descriptor_name = descriptor.get('DescriptorName')\n",
    "    descriptor_name = descriptor_name['String']\n",
    "    if tree_number is not None:\n",
    "        tree_number = tree_number.get('TreeNumber')\n",
    "        if isinstance(tree_number, str):\n",
    "            if len(tree_number.split('.')) == 1:\n",
    "                tree_level_1_map[tree_number] = descriptor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_nodes = list(mesh_label_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how long our corpus of MeSH labels is\n",
    "n_mesh_labels = len(flatten(description_mesh_labels))\n",
    "n_mesh_labels_unique = len(flatten(description_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of MeSH labels in project descriptions: {}'.format(n_mesh_labels))\n",
    "print('Number of unique MeSH labels in project descriptions: {}'.format(n_mesh_labels_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tree_group(label, label_dui_map, dui_tree_number_map, tree_map, level=1):\n",
    "    dui = label_dui_map.get(label)\n",
    "    groups = []\n",
    "    if dui is not None:\n",
    "        tree_numbers = dui_tree_number_map.get(dui)\n",
    "        if tree_numbers is not None:\n",
    "            for tn in tree_numbers:\n",
    "                if tn is not None:\n",
    "                    tn = tn.split('.')\n",
    "                    if level == 0:\n",
    "                        tn = tn[0][0]\n",
    "                    elif level == 1:\n",
    "                        tn = tn[0]\n",
    "                    else:\n",
    "                        tn = '.'.join(tn[:level - 1])\n",
    "                    group = tree_map.get(tn)\n",
    "                    if group is not None:\n",
    "                        groups.append(group)\n",
    "    if len(groups) > 0:\n",
    "        return list(set(groups))\n",
    "    else:\n",
    "        return ['Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Documents\n",
    "\n",
    "To understand the nature of the terms in the network and their relationships with the documents, we need to extract quantitative information aobut the documents. Here we parse the corpus, using _spaCy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(raw_data, 'en_ranknl_long.txt'), 'r') as f:\n",
    "    text = f.read()\n",
    "stop_words = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop in stop_words:\n",
    "    nlp.vocab[stop].is_stop = True\n",
    "    nlp.vocab[stop.title()].is_stop = True\n",
    "    nlp.vocab[stop.upper()].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy allows us to build a corpus where each document has attributes, such as the document ID. This is useful if we want to match up documents with other fields in the DataFrame later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = gdb_df[['doc_id', 'mesh_labels', 'country', 'year']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions_tokenized = [tokenizer(d) for d in descriptions]\n",
    "%time descriptions_tokenized = textacy.Corpus(lang=nlp, texts=descriptions, metadatas=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Exploration\n",
    "\n",
    "In this section, we plot and describe characteristics of the MeSH labels and the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_cdf(ax, x, bins, normed=False, stats=True):\n",
    "    \n",
    "    v = plt.get_cmap('viridis')\n",
    "    pdf_c = v.colors[0]\n",
    "    cdf_c = v.colors[90]\n",
    "    mean_c = v.colors[180]\n",
    "    med_c = v.colors[230]\n",
    "    \n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "\n",
    "    pdf, bin_edges = np.histogram(x, bins)\n",
    "    cdf = np.cumsum(pdf / np.sum(pdf))\n",
    "    if normed:\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.bar(bin_edges[:-1], pdf / np.sum(pdf), np.diff(bin_edges), color=pdf_c)\n",
    "    else:\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.bar(bin_edges[:-1], pdf, np.diff(bin_edges), color=pdf_c)\n",
    "    if stats:\n",
    "        mean = np.mean(x)\n",
    "        median = np.median(x)\n",
    "        ax.axvline(mean, color=mean_c, linestyle='--', linewidth=3, label='Mean: {:.2f}'.format(mean), alpha=0.7)\n",
    "        ax.axvline(median, color=med_c, linestyle='--', linewidth=3, label='Median: {:.2f}'.format(median), alpha=0.7)\n",
    "        ax.legend()\n",
    "    ax.set_xlim((np.min(bin_edges), np.max(bin_edges)))\n",
    "    ax_cdf = ax.twinx()\n",
    "    ax_cdf.plot(bin_edges[:-1], cdf, color=cdf_c, linewidth=4, alpha=0.8)\n",
    "    ax_cdf.set_ylabel('Cumulative')\n",
    "    ax_cdf.set_ylim((0, 1))\n",
    "    ax_cdf.tick_params('y')\n",
    "    return ax, ax_cdf\n",
    "    \n",
    "def pdf_cdf_h(ax, y, bins, normed=False, stats=True):\n",
    "    v = plt.get_cmap('viridis')\n",
    "    pdf_c = v.colors[0]\n",
    "    cdf_c = v.colors[90]\n",
    "    mean_c = v.colors[180]\n",
    "    med_c = v.colors[230]\n",
    "    \n",
    "    xmax = np.max(y)\n",
    "    xmin = np.min(y)\n",
    "\n",
    "    pdf, bin_edges = np.histogram(y, bins)\n",
    "    cdf = np.cumsum(pdf / np.sum(pdf))\n",
    "    if normed:\n",
    "        ax.set_xlabel('Probability')\n",
    "        ax.barh(bin_edges[:-1], pdf / np.sum(pdf), np.diff(bin_edges), color=pdf_c)\n",
    "    else:\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.barh(bin_edges[:-1], pdf, np.diff(bin_edges), color=pdf_c)\n",
    "    if stats:\n",
    "        mean = np.mean(y)\n",
    "        median = np.median(y)\n",
    "        ax.axhline(mean, color=mean_c, linestyle='--', linewidth=3, label='Mean: {:.2f}'.format(mean), alpha=0.7)\n",
    "        ax.axhline(median, color=med_c, linestyle='--', linewidth=3, label='Median: {:.2f}'.format(median), alpha=0.7)\n",
    "        ax.legend()\n",
    "    ax.set_ylim((np.min(bin_edges), np.max(bin_edges)))\n",
    "    ax_cdf = ax.twiny()\n",
    "    ax_cdf.plot(cdf, bin_edges[:-1], color=cdf_c, linewidth=4, alpha=0.8)\n",
    "    ax_cdf.set_xlabel('Cumulative')\n",
    "    ax_cdf.set_xlim((0, 1))\n",
    "    ax_cdf.tick_params('y')\n",
    "    return ax, ax_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexbin_sidegrams(x, y, hb_bins=None, hb_gridsize=50, hb_mincnt=1, hb_xlabel='', \n",
    "                     title='', hb_ylabel='', cb_label='', \n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_hb = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "    cbaxes = plt.axes([0, 0.1, 0.02, 0.65])\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    hb = ax_hb.hexbin(x, y, bins=hb_bins, gridsize=hb_gridsize, cmap='viridis', mincnt=hb_mincnt)\n",
    "    ax_hb.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "    cb = plt.colorbar(hb, cax = cbaxes)\n",
    "    cb.set_label(cb_label)\n",
    "    cb.ax.yaxis.set_ticks_position('left')\n",
    "    cb.ax.yaxis.set_label_position('left')\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x[~np.isinf(x)])\n",
    "    xmin = np.min(x[~np.isinf(x)])\n",
    "    ymax = np.max(y[~np.isinf(y)])\n",
    "    ymin = np.min(y[~np.isinf(y)])\n",
    "\n",
    "    ax_hb.set_xlim((xmin, xmax))\n",
    "    ax_hb.set_ylim((ymin, ymax))\n",
    "    ax_hb.set_xlabel(hb_xlabel)\n",
    "    ax_hb.set_ylabel(hb_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_hb.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_hb.get_ylim())\n",
    "    \n",
    "#     labels = ax_hist_cum_x.get_yticklabels()\n",
    "#     labels[0] = ''\n",
    "#     ax_hist_cum_x.set_yticklabels(labels)\n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_sidegrams(x, y, title='', sc_ylabel='', sc_xlabel='',\n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True,\n",
    "                     **sc_kwargs):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_sc = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    sc = ax_sc.scatter(x, y, **sc_kwargs)\n",
    "    ax_sc.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "    ymax = np.max(y)\n",
    "    ymin = np.min(y)\n",
    "\n",
    "    ax_sc.set_xlim((xmin, xmax))\n",
    "    ax_sc.set_ylim((ymin, ymax))\n",
    "    ax_sc.set_xlabel(sc_xlabel)\n",
    "    ax_sc.set_ylabel(sc_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_sc.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_sc.get_ylim())\n",
    "    \n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_scatter_sidegrams(x, y, c, cmap='viridis', title='', sc_ylabel='', sc_xlabel='',\n",
    "                     th_bins=100, th_norm=False, th_stats=True,\n",
    "                     lh_bins=100, lh_norm=False, lh_stats=True,\n",
    "                     **sc_kwargs):\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(14, 8))\n",
    "\n",
    "    ax_sc = plt.axes(rect_scatter)\n",
    "    ax_hist_x = plt.axes(rect_histx)\n",
    "    ax_hist_y = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    ax_hist_x.xaxis.set_major_formatter(nullfmt)\n",
    "    ax_hist_y.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    sc = ax_sc.scatter(x, y, c, cmap=cmap, **sc_kwargs)\n",
    "    ax_sc.axis([np.min(x), np.max(x), np.min(y), np.max(y)])\n",
    "\n",
    "    # now determine nice limits\n",
    "    xmax = np.max(x)\n",
    "    xmin = np.min(x)\n",
    "    ymax = np.max(y)\n",
    "    ymin = np.min(y)\n",
    "\n",
    "    ax_sc.set_xlim((xmin, xmax))\n",
    "    ax_sc.set_ylim((ymin, ymax))\n",
    "    ax_sc.set_xlabel(sc_xlabel)\n",
    "    ax_sc.set_ylabel(sc_ylabel)\n",
    "    \n",
    "    ax_hist_x, ax_hist_cum_x = pdf_cdf(ax_hist_x, x, th_bins, normed=th_norm, stats=th_stats)\n",
    "\n",
    "    ax_hist_y, ax_hist_cum_y = pdf_cdf_h(ax_hist_y, y, lh_bins, normed=lh_norm, stats=lh_stats)\n",
    "\n",
    "    ax_hist_x.set_xlim(ax_sc.get_xlim())\n",
    "    ax_hist_y.set_ylim(ax_sc.get_ylim())\n",
    "    \n",
    "    plt.setp(ax_hist_cum_x.get_yticklabels()[0], visible=False)\n",
    "    plt.setp(ax_hist_cum_y.get_xticklabels()[0], visible=False)\n",
    "\n",
    "    ax_hist_x.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df = pd.DataFrame({'terms': list(mesh_label_unique_counts.keys())})\n",
    "\n",
    "terms_df['occurrences'] = terms_df['terms'].map(mesh_label_unique_counts)\n",
    "terms_df['cooccurrences'] = terms_df['terms'].map(mesh_label_coocurrence_counts)\n",
    "terms_df['edges'] = terms_df['terms'].map(mesh_label_edge_counts)\n",
    "terms_df['occurrences_log'] = np.log10(terms_df['occurrences'])\n",
    "terms_df['cooccurrences_log'] = np.log10(terms_df['cooccurrences'])\n",
    "terms_df['edges_log'] = np.log10(terms_df['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df['cooccurrences_per_occurrence'] = terms_df['cooccurrences'] / terms_df['occurrences']\n",
    "terms_df['edges_per_occurrence'] = terms_df['edges'] / terms_df['occurrences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_arrays(df, col_0, col_1):\n",
    "    sub_df = df[[col_0, col_1]]\n",
    "    sub_df = sub_df.replace([np.inf, -np.inf], np.nan)\n",
    "    sub_df = sub_df[((~pd.isnull(sub_df[col_0])) & (~pd.isnull(sub_df[col_1])))]\n",
    "    arr_0 = sub_df[col_0]\n",
    "    arr_1 = sub_df[col_1]\n",
    "    return arr_0, arr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_occurrences_arr, term_cooccurrences_arr = get_valid_arrays(terms_df, 'occurrences_log', 'cooccurrences_log')\n",
    "\n",
    "hexbin_sidegrams(term_occurrences_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Count',\n",
    "                 hb_ylabel='Log10 Term Co-occurrences Count',\n",
    "                 title='Term Occurrences vs. Term Co-ocurrences', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_occurrences_arr, term_edges_arr = get_valid_arrays(terms_df, 'occurrences_log', 'edges_log')\n",
    "\n",
    "hexbin_sidegrams(term_occurrences_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Count',\n",
    "                 hb_ylabel='Log10 Term Edge Count',\n",
    "                 title='Term Occurrences vs. Term Edges', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(term_edges_arr, term_cooccurrences_arr,\n",
    "                 hb_gridsize=70,\n",
    "                 hb_xlabel='Log10 Term Edge Count',\n",
    "                 hb_ylabel='Log10 Term Co-occurrence Count',\n",
    "                 title='Term Occurrences vs. Term Edges', \n",
    "                 cb_label='Frequency',\n",
    "                 lh_bins=50,\n",
    "                 th_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mesh_labels_unique = list(set(flatten(description_mesh_labels_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_association_strengths_dict = defaultdict(list)\n",
    "\n",
    "for l, n in edge_coocurrence_counts.items():\n",
    "    term_0 = l[0]\n",
    "    term_1 = l[1]\n",
    "    assoc_str = coocurrence_association_strength(term_0, term_1, n)\n",
    "    term_association_strengths_dict[term_0].append(assoc_str)\n",
    "    term_association_strengths_dict[term_1].append(assoc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mean_assoc_strengths = {}\n",
    "term_std_assoc_strengths = {}\n",
    "term_med_assoc_strengths = {}\n",
    "term_min_assoc_strengths = {}\n",
    "term_max_assoc_strengths = {}\n",
    "\n",
    "for t, a_s in term_association_strengths_dict.items():\n",
    "    term_mean_assoc_strengths[t] = np.mean(a_s)\n",
    "    term_std_assoc_strengths[t] = np.std(a_s)\n",
    "    term_med_assoc_strengths[t] = np.median(a_s)\n",
    "    term_min_assoc_strengths[t] = np.min(a_s)\n",
    "    term_max_assoc_strengths[t] = np.max(a_s)\n",
    "    \n",
    "terms_df['mean_assoc_strength'] = terms_df['terms'].map(term_mean_assoc_strengths)\n",
    "terms_df['median_assoc_strength'] = terms_df['terms'].map(term_med_assoc_strengths)\n",
    "terms_df['std_assoc_strength'] = terms_df['terms'].map(term_std_assoc_strengths)\n",
    "terms_df['min_assoc_strength'] = terms_df['terms'].map(term_min_assoc_strengths)\n",
    "terms_df['max_assoc_strength'] = terms_df['terms'].map(term_max_assoc_strengths)\n",
    "terms_df['mean_assoc_strength_log'] = np.log10(terms_df['mean_assoc_strength'])\n",
    "terms_df['median_assoc_strength_log'] = np.log10(terms_df['median_assoc_strength'])\n",
    "terms_df['std_assoc_strength_log'] = np.log10(terms_df['std_assoc_strength'])\n",
    "terms_df['min_assoc_strength_log'] = np.log(terms_df['min_assoc_strength'])\n",
    "terms_df['max_assoc_strength_log'] = np.log(terms_df['max_assoc_strength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences, mean_assoc_strengths = get_valid_arrays(terms_df, 'cooccurrences', 'mean_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(np.log(cooccurrences), mean_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_assoc_strengths, std_assoc_strengths = get_valid_arrays(terms_df, 'mean_assoc_strength_log', 'std_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(mean_assoc_strengths, std_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_assoc_strengths, med_assoc_strengths = get_valid_arrays(terms_df, 'mean_assoc_strength_log', 'median_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(mean_assoc_strengths, med_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_assoc_strengths, max_assoc_strengths = get_valid_arrays(terms_df, 'min_assoc_strength_log', 'max_assoc_strength_log')\n",
    "\n",
    "hexbin_sidegrams(min_assoc_strengths, max_assoc_strengths,\n",
    "                 hb_xlabel='Log10 Mean Term Association Strengths',\n",
    "                 hb_ylabel='Log10 Term Association Strength Std Devs',\n",
    "                 title='Mean Term Assoc. Strengths vs. Std Deviation Term Assoc. Strengths', \n",
    "                 cb_label='Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_df(df, cols):\n",
    "    sub_df = df[cols]\n",
    "    sub_df = sub_df.replace([np.inf, -np.inf], np.nan)\n",
    "    for col in cols:\n",
    "        sub_df = sub_df[~pd.isnull(sub_df[col])]\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df_tmp = get_valid_df(terms_df, ['terms', 'occurrences_log', 'cooccurrences_log', 'edges_log', 'mean_assoc_strength_log', 'median_assoc_strength_log',\n",
    "                             'std_assoc_strength_log', 'min_assoc_strength_log', 'max_assoc_strength_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scatter_matrix(terms_df_tmp.sample(frac=0.1, random_state=42), alpha=0.1, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold.t_sne import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_tsne = tsne.fit_transform(terms_df_tmp[['occurrences_log', 'cooccurrences_log', 'edges_log', 'mean_assoc_strength_log', 'median_assoc_strength_log',\n",
    "                                              'std_assoc_strength_log', 'min_assoc_strength_log', 'max_assoc_strength_log']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_df_tmp['tsne_0'] = terms_tsne[:, 0]\n",
    "terms_df_tmp['tsne_1'] = terms_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = ColumnDataSource(terms_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BoxZoomTool, ResetTool, WheelZoomTool, LinearColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(palette='Magma256', low=min(terms_df_tmp['occurrences_log']), high=max(terms_df_tmp['occurrences_log']))\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Term\", \"@terms\"),\n",
    "])\n",
    "box = BoxZoomTool()\n",
    "reset = ResetTool()\n",
    "zoom = WheelZoomTool()\n",
    "# color_mapper = CategoricalColorMapper(factors=list(w2v_df['label'].values.unique()))\n",
    "\n",
    "terms_tsne_scatter = figure(width=700, height=600, tools=[hover, box, reset, zoom],\n",
    "                     title='TSNE Plot of MeSH Term Network Characteristics')\n",
    "terms_tsne_scatter.circle(x='tsne_0', y='tsne_1', source=cds, alpha=0.2,\n",
    "                          color={'field': 'occurrences_log', 'transform': color_mapper}\n",
    "#                           color=factor_cmap('clusters_k', palette=Category20_20, factors=list(w2v_df['clusters_k'].unique())),\n",
    "#                           size=6\n",
    "                         )\n",
    "terms_tsne_scatter.xgrid.visible = False\n",
    "terms_tsne_scatter.ygrid.visible = False\n",
    "\n",
    "\n",
    "show(terms_tsne_scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df = pd.DataFrame({'edge': list(edge_coocurrence_counts.keys())})\n",
    "term_edges_df['cooccurrence_count'] = term_edges_df['edge'].map(edge_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = term_edges_df['edge'].values\n",
    "edge_terms_0 = [e[0] for e in edge_list]\n",
    "edge_terms_1 = [e[1] for e in edge_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['term_0'] = edge_terms_0\n",
    "term_edges_df['term_1'] = edge_terms_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['term_occurrence_0'] = term_edges_df['term_0'].map(mesh_label_unique_counts)\n",
    "term_edges_df['term_occurrence_1'] = term_edges_df['term_1'].map(mesh_label_unique_counts)\n",
    "term_edges_df['term_cooccurrence_0'] = term_edges_df['term_0'].map(mesh_label_coocurrence_counts)\n",
    "term_edges_df['term_cooccurrence_1'] = term_edges_df['term_1'].map(mesh_label_coocurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['cooccurrence_per_occurrence'] = term_edges_df['cooccurrence_count'] / (term_edges_df['term_occurrence_0'] + term_edges_df['term_occurrence_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_df['cooccurrence_assoc_strength'] = (2 * n_coocurrences * term_edges_df['cooccurrence_count']) / (term_edges_df['term_cooccurrence_0'] * term_edges_df['term_cooccurrence_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_association_strength(edge):\n",
    "    edge = ast.literal_eval(edge)\n",
    "    return coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge])\n",
    "\n",
    "term_edges_df['assoc_strengths'] = term_edges_df['edge'].apply(lambda x: map_association_strength(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edges_sample_df = term_edges_df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(np.log(term_edges_df['cooccurrence_count']), np.log(term_edges_df['assoc_strengths']),\n",
    "                 hb_xlabel='Log10 Term Edge Co-occurrence Count',\n",
    "                 hb_ylabel='Log10 Term Edge Association Strength',\n",
    "                 title='Term Edge Co-occurrence Count vs. Association Strengths', \n",
    "                 cb_label='Log Frequency',\n",
    "                 hb_gridsize=100,\n",
    "                 lh_bins=50,\n",
    "                 hb_bins='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_association_strengths = []\n",
    "term_edge_counts = []\n",
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    term_edge_association_strengths.append(coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge]))\n",
    "    term_edge_counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_association_strengths_log = np.log(term_edge_association_strengths)\n",
    "term_edge_counts_log = np.log(term_edge_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_jaccard_similarities = []\n",
    "for edge, count in edge_coocurrence_counts.items():\n",
    "    term_edge_jaccard_similarities.append(jaccard_similarity(edge[0], edge[1], count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_edge_jaccard_similarities_log = np.log(term_edge_jaccard_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(term_edge_counts_log, term_edge_jaccard_similarities_log,\n",
    "                 hb_bins='log', hb_gridsize=50,\n",
    "                 hb_xlabel='Number of Tokens',\n",
    "                 hb_ylabel='Number of MeSH Labels',\n",
    "                 title='Document Lengths vs. Number of Unique MeSH Labels Identified', \n",
    "                 cb_label='Log Document Count'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no new dataframe here, as we can just use the gdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['n_tokens'] = [len(d) for d in descriptions_tokenized]\n",
    "gdb_df['n_mesh_labels'] = [len(ml) for ml in description_mesh_labels]\n",
    "gdb_df['n_unique_mesh_labels'] = [len(ml) for ml in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexbin_sidegrams(gdb_df['n_tokens'], gdb_df['n_unique_mesh_labels'], \n",
    "                 hb_bins='log', hb_gridsize=50,\n",
    "                 hb_xlabel='Number of Tokens',\n",
    "                 hb_ylabel='Number of Unique MeSH Labels',\n",
    "                 title='Document Lengths vs. Number of Unique MeSH Labels Identified', \n",
    "                 cb_label='Log Document Count',\n",
    "                 th_bins=200,\n",
    "                 lh_bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a large number (around 30%) of documents with less than 50 tokens and less than 10 MeSH labels. There's a roughly linear relationship between the number of tokens and the number of MeSH labels, as would be reasonable to expect, however the spread of points is very large. There vast majority of documents have less than 800 tokens or 40 labels. The table below gives exact values for the summary statistics of these distributions.\n",
    "\n",
    "One other notable feature is that the histogram of document lengths shows a few humps at around 250, 350 and 500 tokens. Perhaps this reflects proposal or abstract word limits imposed by funding bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_percentiles(distributions, percentiles=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]):\n",
    "    percentile_distributions = []\n",
    "    for d in distributions:\n",
    "        if len(d) > 1:\n",
    "            percentile_distributions.append(np.percentile(d, percentiles))\n",
    "        else:\n",
    "            percentile_distributions.append([np.nan] * len(percentiles))\n",
    "    return np.matrix(percentile_distributions)\n",
    "\n",
    "def matrix_2_df(matrix, header_prefix):\n",
    "    headers = ['{}_{}'.format(header_prefix, i * 10) for i in range(0, matrix.shape[1])]\n",
    "    df = pd.DataFrame(matrix, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first document features we will calculate are the **fraction of shared labels**, the **fraction of unique labels**, and the **percentiles of the distribution of label frequencies**.\n",
    "\n",
    "We will put these in a new dataframe, which will contain only the features we plan to use for document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shareds = []\n",
    "n_uniques = []\n",
    "label_freq_norm_dists = []\n",
    "\n",
    "for dmlu in description_mesh_labels_unique:\n",
    "    label_freqs = [mesh_label_unique_counts[label] for label in dmlu]\n",
    "    label_freq_norm_dists.append([lf / n_mesh_labels_unique for lf in label_freqs])\n",
    "    n_shared = len([lf for lf in label_freqs if lf > 1])\n",
    "    n_shareds.append(n_shared)\n",
    "    n_uniques.append(len(dmlu) - n_shared)\n",
    "\n",
    "frequency_normed_percentiles = distribution_percentiles(label_freq_norm_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_label_frequency_norm_percentiles_df = matrix_2_df(frequency_normed_percentiles, 'label_frequency_percentile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = gdb_df[['doc_id', 'n_unique_mesh_labels']]\n",
    "doc_df = doc_df.merge(pd.DataFrame({'n_shared_labels': n_shareds, 'n_unique_labels': n_uniques, 'doc_id': gdb_df['doc_id'].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['n_shared_labels_norm'] = doc_df['n_shared_labels'] / doc_df['n_unique_mesh_labels']\n",
    "doc_df['n_unique_labels_norm'] = doc_df['n_unique_labels'] / doc_df['n_unique_mesh_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, doc_label_frequency_norm_percentiles_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add the **perctentile distributions of the assocation strengths** of the intra-document label co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_n_unique_mesh_labels = gdb_df['n_unique_mesh_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_association_strengths = []\n",
    "\n",
    "for dmlc in description_mesh_label_combinations:\n",
    "    doc_association_strengths.append([coocurrence_association_strength(edge[0], edge[1], edge_coocurrence_counts[edge]) for edge in dmlc])\n",
    "    \n",
    "doc_assoc_strength_percentiles = distribution_percentiles(doc_association_strengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_doc_association_strength_headers = ['assoc_strength_{}_pctile'.format(i * 10) for i in range(0, 11)]\n",
    "percentile_df = pd.DataFrame(doc_assoc_strength_percentiles, columns=percentile_doc_association_strength_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, percentile_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to know the overlap of the documents with each other so we will calculate the percentile distributions of **document jaccard similarities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(d0, d1):\n",
    "    intersection_cardinality = len(set.intersection(*[set(d0), set(d1)]))\n",
    "    union_cardinality = len(set.union(*[set(d0), set(d1)]))\n",
    "    if union_cardinality > 0:\n",
    "        return intersection_cardinality / float(union_cardinality)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_unique_joined = [' '.join([t.replace('-', '_').replace(' ', '_').replace(',', '').lower() for t in d]) for d in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = cv.fit_transform(description_mesh_label_unique_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow_size = docs_bow.shape[0]\n",
    "# number of random documents to sample from the corpus\n",
    "size = 2000\n",
    "\n",
    "mean_jaccard_indices = []\n",
    "\n",
    "for i in range(docs_bow_size):\n",
    "    d0 = description_mesh_labels_unique[i]\n",
    "    d_ids = np.random.randint(0, high=docs_bow_size, size=(size))\n",
    "    d_ids[~np.in1d(d_ids, i).reshape(d_ids.shape)]\n",
    "    random_docs = itemgetter(*d_ids)(description_mesh_labels_unique)\n",
    "    \n",
    "    mean_jaccard_indices.append([jaccard_index(d0, random_docs[i]) for i in range(size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_jaccard_percentiles = distribution_percentiles(mean_jaccard_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_jaccard_indices_overlap_only = [[d for d in doc if d != 0] for doc in mean_jaccard_indices]\n",
    "doc_jaccard_overlap_percentiles = distribution_percentiles(doc_jaccard_indices_overlap_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_indices_overlap_df = matrix_2_df(doc_jaccard_overlap_percentiles, 'jaccard_overlap_pctile')\n",
    "jaccard_indices_df = matrix_2_df(doc_jaccard_percentiles, 'jaccard_pctile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, jaccard_indices_overlap_df], axis=1, join_axes=[doc_df.index])\n",
    "doc_df = pd.concat([doc_df, jaccard_indices_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv(proc_data + 'gdb_description_network_features_{}.csv'.format(today_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intra-document word embedding similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_ends = column_to_list(gdb_df, 'mesh_term_token_end_idx')\n",
    "description_mesh_starts = column_to_list(gdb_df, 'mesh_term_token_start_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_phrases = column_to_list(gdb_df, 'original_phrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_label_joined = [[t.replace('-', '_').replace(' ', '_').replace(',', '').lower() for t in d] for d in description_mesh_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_mesh_label_subs = []\n",
    "\n",
    "for description, originals, labels in zip(descriptions, description_mesh_phrases, description_mesh_label_joined):\n",
    "#     description = description.replace('{', ' ').replace('}', ' ')\n",
    "    labels_used = []\n",
    "    originals_used = []\n",
    "    for label, original in zip(labels, originals):\n",
    "#         description = description[:start] + '{}' + description[end:]\n",
    "#         description = description.format(label)\n",
    "        if label not in labels_used:\n",
    "            description = description.replace(original, label.lower())\n",
    "            labels_used.append(label)\n",
    "            originals_used.append(original)\n",
    "            \n",
    "    descriptions_mesh_label_subs.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_labeled_tokenized = [tokenizer(d) for d in descriptions_mesh_label_subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_labeled_tokens = [[t.text for t in d] for d in descriptions_labeled_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(descriptions_labeled_tokens, size=300, window=7, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_word_vecs = w2v_model.wv\n",
    "mesh_label_word_vecs.save('../models/mesh_labels_word_vecs_{}'.format(today_str))\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def doc_word_vecs(doc_combinations, word_vecs):\n",
    "    description_cosine_similarities = []\n",
    "    for combos in doc_combinations:\n",
    "        doc_similarities = []\n",
    "        if len(combos) > 0:\n",
    "            for combo in combos:\n",
    "                term_0 = combo[0].replace('-', '_').replace(' ', '_').replace(',', '').lower()\n",
    "                term_1 = combo[1].replace('-', '_').replace(' ', '_').replace(',', '').lower()\n",
    "                if (term_0 in word_vecs) & (term_1 in word_vecs):\n",
    "                    doc_similarities.append(cosine(word_vecs[term_0], word_vecs[term_1]))\n",
    "        if len(doc_similarities) == 0:\n",
    "            doc_similarities.append(np.array([np.nan]))\n",
    "        description_cosine_similarities.append(doc_similarities)\n",
    "    return description_cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarities = doc_word_vecs(description_mesh_label_combinations, mesh_label_word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarity_percentiles = distribution_percentiles(description_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cosine_similarity_percentiles_df = matrix_2_df(description_cosine_similarity_percentiles, 'mesh_label_w2v_cosine_pctile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat([doc_df, description_cosine_similarity_percentiles_df], axis=1, join_axes=[doc_df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv(proc_data + 'gdb_description_network_features_{}.csv'.format(today_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering based on all non-network features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should consider **network features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = doc_df['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df_features = doc_df.drop(columns=['doc_id', 'n_unique_mesh_labels', 'n_shared_labels', 'n_unique_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=5, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sclr = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in doc_df_features.columns:\n",
    "    doc_df_features[c][pd.isnull(doc_df_features[c])] = np.mean(doc_df_features[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df_features = std_sclr.fit_transform(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.fit(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gm.predict(doc_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame({'cluster': labels, 'doc_id': doc_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = label_df[label_df['cluster'] == 3].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_mesh_labels_unique_arr = np.array(description_mesh_labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, l in zip(gdb_df.iloc[cluster_0[:20]]['Description'].values, description_mesh_labels_unique_arr[cluster_0[:20]]):\n",
    "    print('=============================================== \\n')\n",
    "    print(l, '\\n')\n",
    "    print(d, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(description_mesh_labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(d) for d in description_mesh_labels_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lsi = corpus2dense(corpus_lsi, num_terms=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lsi = matrix_lsi.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_labels = lof.fit_predict(matrix_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pioneer_doc_ids = gdb_df[gdb_df['Source ID'] == 'pioneers']['doc_id'].index.values\n",
    "global_doc_ids = gdb_df[gdb_df['Source ID'] == 'global']['doc_id'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_df = pd.DataFrame({'cluster': lof_labels, 'doc_id': doc_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lofs = lof_df[lof_df['cluster'] == -1]['doc_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_df['local_outlier_factor'] = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, l in zip(gdb_df.iloc[lofs[:20]]['Description'].values, gdb_df.iloc[lofs[:20]]['mesh_labels'].values):\n",
    "    print('=============================================== \\n')\n",
    "    print(l, '\\n')\n",
    "    print(d, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count = Counter(lof_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_node_counts_x = []\n",
    "edge_node_counts_y = []\n",
    "\n",
    "for k, v in edge_node_counts.items():\n",
    "    edge_node_counts_x.append(k)\n",
    "    edge_node_counts_y.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax0.scatter(np.log(edge_node_counts_x), edge_node_counts_y, color=plt.get_cmap('viridis').colors[0], alpha=0.2)\n",
    "ax0.set_xlabel('Log Number of Nodes')\n",
    "ax0.set_ylabel('Number of Edges')\n",
    "pdf_cdf(ax1, np.log(edge_nodes), 100, normed=False)\n",
    "ax1.set_xlabel('Log Number of Edges')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_generator = community.girvan_newman(graph_mesh_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge, count in mesh_label_edges.items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_doc_association_strength = [0 if pd.isnull(a) else a for a in mean_doc_association_strength ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_label_counts['Nutrition Policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_policy_neighbours = list(graph_mesh_labels.neighbors('Nutrition Policy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_processes_neighbours = list(graph_mesh_labels.neighbors('Mental Processes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nodes = list(set(nutrition_policy_neighbours + mental_processes_neighbours))\n",
    "sub_nodes = [sn for sn in sub_nodes if mesh_label_counts[sn] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proc_data + 'mental_processes_nutrition_policy_graph.json', 'w') as f:\n",
    "    json.dump(nx.node_link_data(sub_graph), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nx.to_pandas_adjacency(sub_graph, weight='association_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df)\n",
    "df_log[np.isinf(df_log)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df_log, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
